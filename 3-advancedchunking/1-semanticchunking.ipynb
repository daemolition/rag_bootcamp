{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cf797da",
   "metadata": {},
   "source": [
    "### Semenatic chunking\n",
    "- SemanticChunker is a dacoument splitter tha uses embedding similarity between sentences to decide chunk boundaries\n",
    "- it ensures the each chunk is semantically coherent and not cu off mid-thought like traditional character/token splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26260a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0db4dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chunk 1:\n",
      "LangChain is a framework for building applications with LLMs. Langchain provides modular abstractions to combine LLMs with tools like OpenAI and Pinecone.\n",
      "\n",
      "Chunk 2:\n",
      "You can create chains, agents, memory, and retrievers.\n",
      "\n",
      "Chunk 3:\n",
      "The Eiffel Tower is located in Paris.\n",
      "\n",
      "Chunk 4:\n",
      "France is a popular tourist destination.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "## Sample etxt\n",
    "text = \"\"\n",
    "with open (\"langchain_intro.txt\", \"r\") as file:\n",
    "    text = file.read()\n",
    "    \n",
    "## Step 1: Split into sentences\n",
    "sentences = [s.strip() for s in text.split(\"\\n\") if s.strip()]\n",
    "\n",
    "## Step 2: Embed each sentence\n",
    "embeddings = model.encode(sentences)\n",
    "\n",
    "## initialize the parameters\n",
    "threshold = 0.7\n",
    "chunks = []\n",
    "current_chunk=[sentences[0]]\n",
    "\n",
    "## Step 4: Semantic grouing based on threshold\n",
    "for i in range(1, len(sentences)):\n",
    "    sim = cosine_similarity(\n",
    "        [embeddings[i-1]],\n",
    "        [embeddings[i]]\n",
    "    )[0][0]\n",
    "\n",
    "    if sim > threshold:\n",
    "        current_chunk.append(sentences[i])\n",
    "    else:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "        current_chunk=[sentences[i]]\n",
    "        \n",
    "        \n",
    "# Append the last chunk\n",
    "chunks.append(\" \".join(current_chunk))\n",
    "\n",
    "## Output the chunks\n",
    "for idx, chunk in enumerate(chunks):\n",
    "    print(f\"\\nChunk {idx+1}:\\n{chunk}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818ef64c",
   "metadata": {},
   "source": [
    "## RAG Pipeline - Modular Coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0d828231",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from langchain.schema import Document\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEndpointEmbeddings\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain.schema.runnable import RunnableLambda, RunnableMap\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6264fc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Custom semantic chunker with threshold\n",
    "\n",
    "class ThresholdSemanticChunker:\n",
    "    def __init__(\n",
    "            self, embedding_model: str = \"http://localhost:8080\", threshold: float = 0.7):\n",
    "        self.threshold = threshold\n",
    "        self.embeddings = HuggingFaceEndpointEmbeddings(model=embedding_model)\n",
    "        \n",
    "    \n",
    "    def split(self, text: str):\n",
    "        sentences = [s.strip() for s in text.split(\"\\n\") if s.strip()]\n",
    "        embeddings = model.encode(sentences)\n",
    "        chunks = []\n",
    "        current_chunk=[sentences[0]]\n",
    "\n",
    "        for i in range(1, len(sentences)):\n",
    "            sim = cosine_similarity([embeddings[i-1]],[embeddings[i]])[0][0]\n",
    "\n",
    "            if sim > threshold:\n",
    "                current_chunk.append(sentences[i])\n",
    "            else:\n",
    "                chunks.append(\" \".join(current_chunk) + \".\")\n",
    "                current_chunk=[sentences[i]]     \n",
    "        \n",
    "        chunks.append(\". \".join(current_chunk) + \".\")\n",
    "        return chunks \n",
    "           \n",
    "    \n",
    "    def split_documents(self, docs):\n",
    "        result = []\n",
    "        for doc in docs:\n",
    "            for chunk in self.split(doc.page_content):\n",
    "                result.append(Document(page_content=chunk, metadata=doc.metadata))\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "76cdba9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={}, page_content='LangChain is a framework for building applications with LLMs.\\nLangchain provides modular abstractions to combine LLMs with tools like OpenAI and Pinecone.\\nYou can create chains, agents, memory, and retrievers.\\nThe Eiffel Tower is located in Paris.\\nFrance is a popular tourist destination.')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Sample etxt\n",
    "text = \"\"\n",
    "with open (\"langchain_intro.txt\", \"r\") as file:\n",
    "    text = file.read()\n",
    "    \n",
    "doc = Document(page_content=text)\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7839b15e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='LangChain is a framework for building applications with LLMs. Langchain provides modular abstractions to combine LLMs with tools like OpenAI and Pinecone..'),\n",
       " Document(metadata={}, page_content='You can create chains, agents, memory, and retrievers..'),\n",
       " Document(metadata={}, page_content='The Eiffel Tower is located in Paris..'),\n",
       " Document(metadata={}, page_content='France is a popular tourist destination..')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Chunking\n",
    "chunker = ThresholdSemanticChunker(threshold=0.7)\n",
    "chunks=chunker.split_documents([doc])\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "823fb285",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Vectorestore\n",
    "from langchain_huggingface import HuggingFaceEndpointEmbeddings\n",
    "vectorestore = FAISS.from_documents(chunks, HuggingFaceEndpointEmbeddings(model=\"http://localhost:8080\"))\n",
    "retriever = vectorestore.as_retriever(search_kwargs={\"k\": 3}, search_type=\"similarity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "812945b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template='\\n    Your are a helpful assistant, please provide the answers to the following questions based on the given context:\\n\\n    {context}\\n\\n    Question: {question}\\n    ')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Prompttemplate\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Your are a helpful assistant, please provide the answers to the following questions based on the given context:\n",
    "    \n",
    "    {context}\n",
    "    \n",
    "    Question: {question}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74be4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "## LLM Model\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=\"gemma2:9b-instruct-q4_K_M\",\n",
    "    temperature=0.4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a2505855",
   "metadata": {},
   "outputs": [],
   "source": [
    "### LCEL Chain\n",
    "rag_chain = (\n",
    "    RunnableMap({\n",
    "        \"context\": lambda x: retriever.invoke(x['question']),\n",
    "        \"question\": lambda x: x[\"question\"]\n",
    "    })\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ad8e2014",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"While the provided text mentions that France is a popular tourist destination and that the Eiffel Tower is located in Paris, it doesn't explicitly state what the capital of France is.  \\n\\nHowever, it can be inferred from the context that **Paris** is the capital of France. \\n\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = rag_chain.invoke({\"question\": \"What is the capital of France\"})\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cace58b",
   "metadata": {},
   "source": [
    "## Semantic chunker with Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5a0a07fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " chunk 1: \n",
      "LangChain is a framework for building applications with LLMs. Langchain provides modular abstractions to combine LLMs with tools like OpenAI and Pinecone. You can create chains, agents, memory, and retrievers.\n",
      "\n",
      " chunk 2: \n",
      "The Eiffel Tower is located in Paris. France is a popular tourist destination.\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpointEmbeddings\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain.document_loaders import TextLoader\n",
    " \n",
    "loader = TextLoader(\"langchain_intro.txt\")\n",
    "docs =loader.load()\n",
    "\n",
    "embeddings = HuggingFaceEndpointEmbeddings()\n",
    "\n",
    "semantic_chunker = SemanticChunker(embeddings=embeddings)\n",
    "chunks = semantic_chunker.split_documents(docs)\n",
    "\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"\\n chunk {i+1}: \\n{chunk.page_content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8734594c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
