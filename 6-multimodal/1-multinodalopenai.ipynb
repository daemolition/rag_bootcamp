{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9c953cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/crizz/python/rag_bootcamp/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import fitz\n",
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np\n",
    "import os \n",
    "import base64\n",
    "import io\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import pymupdf\n",
    "\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6edd8596",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "Fetching 1 files: 100%|██████████| 1/1 [00:00<00:00, 4076.10it/s]\n"
     ]
    }
   ],
   "source": [
    "## Clip Model for unified embeddings\n",
    "clip_model=CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_processor=CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9068fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Embedding functions\n",
    "def embed_image(image_data):\n",
    "    \"\"\"Embed image using CLIP\"\"\"\n",
    "    if isinstance(image_data, str):\n",
    "        image = Image.open(image_data).convert(\"RGB\")\n",
    "    else:\n",
    "        image = image_data\n",
    "        \n",
    "    inputs = clip_processor(images=image, return_tensors=\"pt\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        features = clip_model.get_image_features(**inputs)\n",
    "        # Normalize embeddings to unit vector\n",
    "        features = features / features.norm(dim=-1, keepdim=True)\n",
    "        return features.squeeze().numpy()\n",
    "    \n",
    "def embed_text(text):\n",
    "    \"\"\"Embed text using CLIP\"\"\"\n",
    "    inputs = clip_processor(\n",
    "        text=text,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=77 # CLIPS max token length\n",
    "    )\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        features = clip_model.get_text_features(**inputs)\n",
    "        # Normalize embeddings\n",
    "        features = features / features.norm(dim=-1, keepdim=True)\n",
    "        return features.squeeze().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "144e1d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Process PDF\n",
    "pdf_path=\"multimodal_sample.pdf\"\n",
    "doc=pymupdf.open(pdf_path)\n",
    "\n",
    "# Storage for all documents and embeddings\n",
    "all_docs = []\n",
    "all_embeddings = []\n",
    "image_data_store = {}\n",
    "\n",
    "# Text Splitter\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69148979",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, page in enumerate(doc):\n",
    "    # Process text\n",
    "    text=page.get_text()\n",
    "    if text.strip():\n",
    "        # create a temporary document for splitting\n",
    "        temp_doc = Document(page_content=text, metadata={'page': i, \"type\": \"text\"})\n",
    "        text_chunks = splitter.split_documents([temp_doc])\n",
    "        \n",
    "        # Embed each chunk using CLIP\n",
    "        for chunk in text_chunks:\n",
    "            embedding = embed_text(chunk.page_content)\n",
    "            all_embeddings.append(embedding)\n",
    "            all_docs.append(chunk)\n",
    "    \n",
    "    # Process images\n",
    "    ## Three Steps:\n",
    "    ### Convert PDF Image to PIL format\n",
    "    ### Store as base64 \n",
    "    ### Create CLIP embedding for retrieval\n",
    "    \n",
    "    for img_index, img in enumerate(page.get_images(full=True)):\n",
    "        try:\n",
    "            xref=img[0]\n",
    "            base_image=doc.extract_image(xref)\n",
    "            image_bytes=base_image['image']\n",
    "            \n",
    "            # Convert to PIL mage\n",
    "            pil_image = Image.open(io.BytesIO(image_bytes)).convert(\"RGB\")\n",
    "            \n",
    "            # Create unique identifier\n",
    "            image_id = f\"page_{i}_img_{img_index}\"\n",
    "            \n",
    "            # Store image as base64 for later use with Model\n",
    "            buffered = io.BytesIO()\n",
    "            pil_image.save(buffered, format=\"PNG\")\n",
    "            img_base64=base64.b64encode(buffered.getvalue()).decode()\n",
    "            image_data_store[image_id] = img_base64\n",
    "            \n",
    "            # Embed image using CLIP\n",
    "            embedding = embed_image(pil_image)\n",
    "            all_embeddings.append(embedding)\n",
    "            \n",
    "            # Create document for CLIP\n",
    "            image_doc = Document(\n",
    "                page_content=f\"[Image: {image_id}]\",\n",
    "                metadata={'page':i, \"type\": \"image\", \"image_id\": image_id}\n",
    "            )            \n",
    "            all_docs.append(image_doc)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image {img_index} on page {i}: {e}\")\n",
    "            continue\n",
    "        \n",
    "doc.close()            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be52dcc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`embedding_function` is expected to be an Embeddings object, support for passing in a function will soon be removed.\n"
     ]
    }
   ],
   "source": [
    "# Create unified FAISS vectorestore with CLIP embeddings\n",
    "embeddings_array = np.array(all_embeddings)\n",
    "\n",
    "# Create custum FAISS index since we have precomputed embeddings\n",
    "vectore_store = FAISS.from_embeddings(\n",
    "    text_embeddings=[(doc.page_content, emb) for doc, emb in zip(all_docs, embeddings_array)],\n",
    "    embedding=None, # Using precomputed embeddings\n",
    "    metadatas=[doc.metadata for doc in all_docs]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19559244",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOllama(model='gemma3:4b-it-q4_K_M')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = ChatOllama(model=\"gemma3:4b-it-q4_K_M\")\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2069e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve\n",
    "def retrieve_multimodal(query, k=5):\n",
    "    \"\"\"Unified retrieval using CLIP embeddings for both text and images\"\"\"\n",
    "    # Embed query using CLIP\n",
    "    query_embedding = embed_text(query)\n",
    "    \n",
    "    # Search in unified vectore_store based on the query embeddings\n",
    "    results = vectore_store.similarity_search_by_vector(\n",
    "        embedding=query_embedding,\n",
    "        k=k\n",
    "    )\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8605795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Message\n",
    "def create_multimodal_message(query, retrieved_docs):\n",
    "    \"\"\"Create a message with both text and images\"\"\"\n",
    "    content = []\n",
    "    \n",
    "    # Add the query\n",
    "    content.append({\n",
    "        \"type\":\"text\",\n",
    "        \"text\": f\"Question: {query}\\n\\nContext:\\n\"\n",
    "    })\n",
    "    \n",
    "    # Seperate text and image documents\n",
    "    text_docs = [doc for doc in retrieved_docs if doc.metadata.get(\"type\") == \"text\"]\n",
    "    image_docs = [doc for doc in retrieved_docs if doc.metadata.get(\"type\") == \"image\"]\n",
    "    \n",
    "    if text_docs:\n",
    "        for doc in text_docs:\n",
    "            text_content = \"\\n\\n\".join([\n",
    "                f\"[Page {doc.metadata['page']}]: {doc.page_content}\"\n",
    "            ])\n",
    "            content.append({\n",
    "                \"type\":\"text\",\n",
    "                \"text\": f\"Text excerpts:\\n{text_content}\\n\"\n",
    "            })\n",
    "        \n",
    "    for doc in image_docs:\n",
    "        image_id = doc.metadata.get(\"image_id\")    \n",
    "        if image_id and image_id in image_data_store:\n",
    "            content.append({\n",
    "                \"type\":\"text\",\n",
    "                \"text\":f\"\\n[Image from page {doc.metadata['page']}]:\\n\"\n",
    "            })\n",
    "            content.append({\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\n",
    "                    \"url\": f\"data:image/png;base64,{image_data_store[image_id]}\"\n",
    "                }\n",
    "            })\n",
    "            \n",
    "    # Add instruction\n",
    "    content.append({\n",
    "        \"type\": \"text\",\n",
    "        \"text\": \"\\n\\nPlease answer the question based on the provided text and images.\"\n",
    "    })\n",
    "    \n",
    "    return HumanMessage(content=content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "53f80c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG Pipeline\n",
    "def multimodal_pdf_rag_pipeline(query):\n",
    "    \"\"\"Main pipeline for multimodal RAG\"\"\"\n",
    "    \n",
    "    # Retrieve relevant documents\n",
    "    context_docs = retrieve_multimodal(query, k=5)\n",
    "    \n",
    "    # Create multimodal message\n",
    "    message = create_multimodal_message(query, context_docs)\n",
    "    \n",
    "    # Get response from LLM\n",
    "    response = llm.invoke([message])\n",
    "    \n",
    "    # Print retrieved context info\n",
    "    print(f\"\\nRetrieved {len(context_docs)} documents\")\n",
    "    for doc in context_docs:\n",
    "        doc_type = doc.metadata.get(\"type\", \"unknown\")\n",
    "        page = doc.metadata.get(\"page\", \"?\")\n",
    "        if doc_type == \"text\":\n",
    "            preview = doc.page_content[:100] + \"...\" if len(doc.page_content) > 100 else doc.page_content\n",
    "            print(f\"  - Text from page {page}: {preview}\")\n",
    "        else:\n",
    "            print(f\"  - Image from page {page}\")\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    return response.content\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9785d759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: What does the chart on page 1 show about revenue trends?\n",
      "==================================================\n",
      "\n",
      "Retrieved 2 documents\n",
      "  - Text from page 0: Annual Revenue Overview\n",
      "This document summarizes the revenue trends across Q1, Q2, and Q3. As illust...\n",
      "  - Image from page 0\n",
      "\n",
      "\n",
      "Answer: Here's the answer to your question, based on the text and image:\n",
      "\n",
      "The chart shows that revenue grew steadily, with the highest growth occurring in Q3. Q1 showed a moderate increase due to new product lines, Q2 outperformed Q1 due to marketing campaigns, and Q3 had exponential growth due to global expansion.\n",
      "==================================================\n",
      "\n",
      "Query: Summarize the main findigs from the document\n",
      "==================================================\n",
      "\n",
      "Retrieved 2 documents\n",
      "  - Text from page 0: Annual Revenue Overview\n",
      "This document summarizes the revenue trends across Q1, Q2, and Q3. As illust...\n",
      "  - Image from page 0\n",
      "\n",
      "\n",
      "Answer: Here’s a summary of the findings from the document:\n",
      "\n",
      "The document reveals a positive trend in revenue growth over the first three quarters (Q1, Q2, and Q3). \n",
      "\n",
      "*   **Q1:** Revenue increased moderately due to the introduction of new product lines.\n",
      "*   **Q2:** Revenue exceeded Q1 due to successful marketing campaigns.\n",
      "*   **Q3:** Revenue experienced exponential growth, primarily driven by global expansion. \n",
      "\n",
      "Overall, the document indicates a strong upward trend in revenue, culminating in significant growth in the final quarter.\n",
      "==================================================\n",
      "\n",
      "Query: What visual elements are present in the codument?\n",
      "==================================================\n",
      "\n",
      "Retrieved 2 documents\n",
      "  - Text from page 0: Annual Revenue Overview\n",
      "This document summarizes the revenue trends across Q1, Q2, and Q3. As illust...\n",
      "  - Image from page 0\n",
      "\n",
      "\n",
      "Answer: Here's a breakdown of the visual elements present in the document:\n",
      "\n",
      "*   **Chart:** The document contains a bar chart illustrating revenue trends across Q1, Q2, and Q3.\n",
      "*   **Bars:** There are three bars representing the revenue for each quarter (Q1, Q2, and Q3).\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "queries = [\n",
    "    \"What does the chart on page 1 show about revenue trends?\",\n",
    "    \"Summarize the main findigs from the document\",\n",
    "    \"What visual elements are present in the codument?\"\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    print(\"=\" * 50)\n",
    "    answer = multimodal_pdf_rag_pipeline([query])\n",
    "    print(f\"Answer: {answer}\")\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ad32db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
