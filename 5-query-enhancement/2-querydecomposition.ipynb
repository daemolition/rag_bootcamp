{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e2055dd",
   "metadata": {},
   "source": [
    "# Query Enhancement - Query Expansion Techniques\n",
    "\n",
    "In a RAG Pipeline, the quality of the query sent to the retriever determines how good the retrieved context is - and therefore, how accurate the final llm answer will be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e88424c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89da4410",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 1: Load and Split\n",
    "loader = TextLoader(\"langchain_crewai_dataset.txt\")\n",
    "text = loader.load()\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
    "chunks = splitter.split_documents(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "98906347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Embedding\n",
    "embedding = HuggingFaceEmbeddings(model=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Step 3: Vectorstror\n",
    "vectorstore = FAISS.from_documents(\n",
    "    embedding=embedding,\n",
    "    documents=chunks\n",
    ")\n",
    "\n",
    "# Step 4: Retriever\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={'k': 5, 'lambda_mult': 0.7}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2aedea09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: LLM\n",
    "llm = ChatOllama(\n",
    "    model=\"gemma2:9b-instruct-q4_K_M\",\n",
    "    num_ctx=32768,\n",
    "    reasoning=False\n",
    ")\n",
    "\n",
    "# Step 6: Query Expansion\n",
    "query_expansion_prompt = PromptTemplate.from_template(\"\"\"\n",
    "You are a helpful assistant. Expand the following query to improve document retrieval by adding relevant synonyms, technical terms and useful context.\n",
    "\n",
    "\n",
    "Original query: \"{query}\n",
    "\n",
    "Expanded query:                                                      \n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "query_expension_chain = query_expansion_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c3157204",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Expanded Queries for \"Langchain Memory\": \n",
       "\n",
       "Here are a few expanded queries, each focusing on a different aspect of LangChain memory:\n",
       "\n",
       "**1. General Understanding:**\n",
       "\n",
       "* **\"LangChain memory management techniques\"** (Includes technical terms and focuses on the methods used)\n",
       "* **\"How does LangChain store and access memory?\"** (Clarifies the action and purpose of memory)\n",
       "* **\"LangChain persistent memory vs. in-context memory\"** (Differentiates between types of memory)\n",
       "\n",
       "**2. Specific Use Cases:**\n",
       "\n",
       "* **\"LangChain memory for chatbot applications\"** (Focuses on a specific application)\n",
       "* **\"Using LangChain memory for dialogue history tracking\"** (Provides a concrete use case)\n",
       "* **\"Improving LangChain agent performance with memory\"** (Highlights the benefits of memory)\n",
       "\n",
       "**3. Implementation Details:**\n",
       "\n",
       "* **\"LangChain memory integration with vector databases\"** (Mentions a specific technology)\n",
       "* **\"LangChain memory examples and code snippets\"** (Seeks practical implementation guidance)\n",
       "* **\"Best practices for designing LangChain memory systems\"** (Focuses on optimization and design)\n",
       "\n",
       "\n",
       "When choosing the best expanded query, consider your specific needs and what information you're looking for. \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "display(Markdown(query_expension_chain.invoke({\"query\": \"Langchain memory\"})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "da39dfd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG asnwering prompt\n",
    "answer_prompt = PromptTemplate.from_template(\"\"\"\n",
    "Answer the question based on the context below.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {input}                                             \n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c366f2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chains\n",
    "document_chain = create_stuff_documents_chain(\n",
    "    llm,\n",
    "    answer_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "683927ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run query\n",
    "def full_query_decomposition_rag_pipeline(user_query):\n",
    "    # Decompose the query\n",
    "    sub_qs_text = query_expension_chain.invoke(user_query)\n",
    "    sub_questions = [q.strip(\"-*1234567890. \").strip() for q in sub_qs_text.split(\"\\n\") if q.strip()]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for subq in sub_questions:\n",
    "        docs = retriever.invoke(subq)\n",
    "        result = document_chain.invoke({'input': subq, 'context': docs})\n",
    "        results.append(f\"Q: {subq}\\nA: {result}\")\n",
    "        \n",
    "    return \"\\n\\n\".join(results)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3f2785b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Answer:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Q: ## Expanded Query Options:\n",
       "A: The provided text focuses on LangChain's integration with vector databases for semantic search and its role in Retrieval-Augmented Generation (RAG). \n",
       "\n",
       "It doesn't directly mention \"Expanded Query Options.\"  To understand what that refers to, we need more context. \n",
       "\n",
       "**Here are some possibilities based on the given information:**\n",
       "\n",
       "* **Query Expansion Techniques:** LangChain could leverage vector databases to expand user queries by suggesting related terms or concepts, improving the accuracy of search results within large document corpora.\n",
       "* **Multi-Modal Queries:**  The text mentions LangChain's integration with various tools like code execution environments and APIs. \"Expanded Query Options\" might refer to the ability to construct queries that combine text with other data types (e.g., code snippets, images) for richer search capabilities.\n",
       "* **Advanced Search Operators:** LangChain could offer advanced search operators beyond basic keyword matching, allowing users to refine their queries based on semantic relationships or other criteria.\n",
       "\n",
       "\n",
       "Let me know if you have more context about \"Expanded Query Options,\" and I can give you a more precise answer. \n",
       "\n",
       "\n",
       "Q: Here are a few expanded query options, each focusing on different aspects:\n",
       "A: The provided context focuses on the capabilities of LangChain, particularly its ability to integrate with various tools like web search, calculators, code execution environments, and custom APIs. \n",
       "\n",
       "It also highlights LangChain's support for hybrid retrieval, combining keyword-based and embedding-based methods for more comprehensive content retrieval.\n",
       "\n",
       "\n",
       "However,  the question you posed is incomplete. It starts by listing \"expanded query options\" but doesn't provide the actual options themselves. \n",
       "\n",
       "Please provide the complete list of expanded query options so I can answer your question accurately. \n",
       "\n",
       "\n",
       "Q: Option 1 (Focus on Technical Details):\n",
       "A: The provided context repeatedly lists \"like web search, calculators, code execution environments, and custom APIs\" across various versions (v1, v2, v6, v9).  \n",
       "\n",
       "Based on this repetition, it seems likely that these are examples of the types of functionalities or tools that CrewAI offers or integrates with. \n",
       "\n",
       "\n",
       "Let me know if you'd like me to elaborate on any of these examples or explore potential uses for CrewAI based on this information. \n",
       "\n",
       "\n",
       "Q: \"**Comparison of LangChain and CrewAI in terms of memory management and agent architectures. What specific memory mechanisms (e.g., vector stores, RNNs) are used in each framework? How do their agent design patterns (e.g., prompt engineering, tool integration) differ?**\"\n",
       "A: The provided context highlights the compatibility between LangChain and CrewAI, emphasizing that they work best as complementary tools in a hybrid system. \n",
       "\n",
       "Here's a breakdown based on the information given and common knowledge about each framework:\n",
       "\n",
       "**LangChain:**\n",
       "\n",
       "* **Memory Management:** LangChain leverages various memory mechanisms depending on the use case.\n",
       "    *  **Vector Stores:** Commonly used for storing and retrieving contextual information as embeddings. Popular choices include FAISS, Pinecone, Chroma, etc.\n",
       "    *  **RNNs (Recurrent Neural Networks):** Can be integrated for maintaining sequential memory within an agent's execution flow. However, RNNs can be resource-intensive and may not scale as well as vector stores for large amounts of context.\n",
       "* **Agent Architecture:** LangChain agents utilize a planner-executor model:\n",
       "    * **Planner:** Determines the sequence of tool invocations needed to achieve the desired goal. This often involves decision-making based on the current context and potential outcomes.\n",
       "    * **Executor:** Executes the planned steps, interacting with tools and potentially incorporating feedback from previous steps.\n",
       "\n",
       "**CrewAI:**\n",
       "\n",
       "*  **Memory Management:** The context doesn't specify the exact memory mechanisms used by CrewAI. However, given its focus on role-based collaboration, it likely employs some form of persistent memory to track individual roles, their tasks, and ongoing conversations within a collaborative workflow.\n",
       "* **Agent Architecture:** While not explicitly detailed, CrewAI likely uses a more structured agent design pattern tailored for its role-based collaboration approach. This could involve:\n",
       "    *  **Role Definitions:** Clearly defining the responsibilities and capabilities of each role within the system.\n",
       "    *  **Task Assignment:** Mechanisms for assigning tasks to specific roles based on their expertise or current workload.\n",
       "    *  **Communication Channels:** Facilitating communication and information sharing between different roles.\n",
       "\n",
       "**Key Differences:**\n",
       "\n",
       "* **Focus:** LangChain emphasizes general-purpose agent development with a flexible tool integration framework, while CrewAI specializes in structured collaboration among multiple agents.\n",
       "* **Memory Emphasis:** LangChain offers more explicit control over memory mechanisms through vector stores and RNNs, whereas CrewAI's memory management is likely geared towards maintaining role-specific context within collaborative workflows.\n",
       "\n",
       "\n",
       "Let me know if you have any other questions!\n",
       "\n",
       "Q: Option 2 (Focus on Use Cases):\n",
       "A: Based on the context provided, CrewAI is particularly beneficial for complex tasks involving multiple steps that can be broken down into specialized components.  \n",
       "\n",
       "Here are some examples of use cases highlighted in the text:\n",
       "\n",
       "* **Market research:** Gathering and analyzing data from various sources, potentially involving different agents specializing in data collection, analysis, and interpretation.\n",
       "* **Legal document analysis:** Breaking down the process into stages like document extraction, keyword identification, legal term recognition, and summarization, each handled by specialized agents.\n",
       "* **Product development:**  Managing different stages of the product lifecycle, from ideation and design to testing and launch, with agents collaborating on specific aspects.\n",
       "* **Coding assistants:** Assisting developers with code generation, debugging, documentation, and other tasks through the combined expertise of multiple agents. \n",
       "\n",
       "\n",
       "Essentially, CrewAI shines when tasks require collaboration, specialization, and the ability to manage complex workflows involving multiple interconnected steps. \n",
       "\n",
       "\n",
       "Q: \"**How do LangChain and CrewAI handle long-term context and conversational memory in applications like chatbots or interactive storytelling? Which framework is more suitable for building agents that can learn from past interactions and perform complex multi-step tasks?**\"\n",
       "A: Here's a breakdown of how LangChain and CrewAI address context and conversational memory, along with which might be better suited for your needs:\n",
       "\n",
       "**LangChain:**\n",
       "\n",
       "* **Context Handling:**  LangChain leverages its memory modules (ConversationBufferMemory and ConversationSummaryMemory) to manage context. \n",
       "    * `ConversationBufferMemory` stores a history of previous turns, ideal for remembering recent details in a conversation.\n",
       "    * `ConversationSummaryMemory` summarizes past interactions, useful for maintaining a broader understanding of the conversation flow, especially when dealing with lengthy exchanges.\n",
       "\n",
       "* **Agent Capabilities:** LangChain agents excel at complex multi-step tasks thanks to their planner-executor model. They can:\n",
       "    * Plan a sequence of actions (tool invocations) to achieve a goal.\n",
       "    * Make dynamic decisions based on the current context and past interactions.\n",
       "    * Employ branching logic to handle different scenarios.\n",
       "    * Utilize memory effectively across multiple steps.\n",
       "\n",
       "**CrewAI:**\n",
       "\n",
       "* **Context Handling:** While CrewAI's documentation doesn't explicitly detail specific memory mechanisms, its focus on role-based collaboration implies an implicit form of contextual awareness within each role's interactions. \n",
       "* **Agent Capabilities:** CrewAI shines in scenarios requiring coordinated teamwork and diverse perspectives. Its strength lies in managing multiple agents (roles) that collaborate to solve problems.\n",
       "\n",
       "**Which is More Suitable?**\n",
       "\n",
       "* **For chatbots or interactive storytelling with a focus on remembering specific details and navigating complex conversations:** LangChain's memory modules and agent capabilities are highly suitable.\n",
       "* **For applications requiring teamwork, coordination, and diverse perspectives:** CrewAI's role-based collaboration framework might be a better fit. \n",
       "\n",
       "\n",
       "Let me know if you have any other questions!\n",
       "\n",
       "Q: Option 3 (Focus on Development Workflow):\n",
       "A: Please provide the options for \"Option 3 (Focus on Development Workflow)\".  I need the choices to answer the question accurately. \n",
       "\n",
       "\n",
       "For example, your options might look something like this:\n",
       "\n",
       "* **Option 3:** LangChain is best suited for building static websites.\n",
       "* **Option 3:** LangChain's modularity makes it easy to build and maintain complex LLM applications.\n",
       "* **Option 3:**  LangChain is primarily used for creating chatbot interfaces. \n",
       "\n",
       "\n",
       "Let me know the options, and I'll be happy to help! \n",
       "\n",
       "\n",
       "Q: \"**What are the key differences in developing and deploying agents using LangChain and CrewAI?  How do their respective SDKs and tooling support memory management, agent training, and integration with external APIs?**\"\n",
       "A: While the provided context highlights the compatibility between LangChain and CrewAI, it doesn't delve into specific details about their SDKs, tooling, or how they handle memory management, agent training, and API integration. \n",
       "\n",
       "Here's a breakdown based on general knowledge about each platform:\n",
       "\n",
       "**LangChain:**\n",
       "\n",
       "* **Focus:**  LangChain is primarily focused on providing the infrastructure for building applications powered by language models. It excels at agent development, offering tools for defining agents, managing their memory, selecting tools, and orchestrating their actions.\n",
       "* **SDK and Tooling:** LangChain provides a Python SDK with a modular design. This allows developers to easily build agents by chaining together different components like LLMs, prompts, tools, and memory management systems. \n",
       "* **Memory Management:** LangChain supports various memory mechanisms like conversation memory, vector stores, and external databases, allowing agents to retain context across interactions.\n",
       "* **Agent Training:** LangChain doesn't inherently provide a training framework for agents. However, its flexibility allows integration with other training libraries or techniques.\n",
       "* **API Integration:** LangChain makes it straightforward to integrate with external APIs through its tool wrapper system. Developers can define custom tools that interact with APIs and use them within their agents.\n",
       "\n",
       "**CrewAI:**\n",
       "\n",
       "* **Focus:** CrewAI specializes in building collaborative AI systems, particularly focusing on role-based interactions.\n",
       "* **SDK and Tooling:** CrewAI likely provides a SDK tailored for creating agents with defined roles and responsibilities. The tooling may focus on defining these roles, managing their interactions, and potentially visualizing the collaboration flow.\n",
       "* **Memory Management:** CrewAI's memory management is probably designed to support role-based context. Each role might have its own memory space or mechanisms for sharing information between roles.\n",
       "* **Agent Training:**  CrewAI could offer specialized training methods for role-based agents, perhaps incorporating techniques like reinforcement learning to optimize collaboration.\n",
       "* **API Integration:** CrewAI's API integration capabilities would likely be geared towards supporting the needs of collaborative agents, potentially enabling seamless communication with external tools or APIs relevant to each role.\n",
       "\n",
       "\n",
       "**In Summary:**\n",
       "\n",
       "To get a precise answer about the differences in developing and deploying agents using LangChain and CrewAI, you'd need to consult their respective documentation, tutorials, and community resources. \n",
       "\n",
       "\n",
       "Q: Tips for further expansion:\n",
       "A: Based on the context provided, here are some tips for further expanding LangChain's capabilities:\n",
       "\n",
       "* **Enhance Planner Capabilities:** \n",
       "\n",
       "    * Explore more sophisticated planning algorithms to enable agents to handle complex, multi-step tasks with greater accuracy and efficiency.\n",
       "    * Incorporate machine learning techniques into the planner to learn from past interactions and improve future plan generation.\n",
       "\n",
       "* **Expand Tool Integration:**  \n",
       "    * Develop a standardized interface for integrating new tools and APIs, making it easier for developers to connect LangChain agents with a wider range of external resources.\n",
       "    * Build specialized tool libraries tailored for specific domains (e.g., finance, healthcare) to simplify agent development in those areas.\n",
       "\n",
       "* **Improve Context Management:**\n",
       "\n",
       "    * Research advanced memory mechanisms that can effectively handle long-term context and relationships between steps in an agent's execution.\n",
       "    * Explore techniques for summarizing and highlighting key information from past interactions to aid decision-making in subsequent steps.\n",
       "\n",
       "* **Promote Reusability and Collaboration:** \n",
       "\n",
       "    * Create a platform or registry where developers can share pre-built agent templates, tool integrations, and planning strategies.\n",
       "    * Develop standardized formats for representing agent knowledge and workflows, enabling easier sharing and modification.\n",
       "\n",
       "\n",
       "By focusing on these areas, LangChain can become even more powerful and versatile, empowering developers to build sophisticated AI agents that solve real-world problems across diverse domains. \n",
       "\n",
       "\n",
       "Q: Specify your level of expertise:** Are you a beginner or an experienced developer? This can help tailor the query to your needs\n",
       "A: Based on the context provided, it seems like I am designed to be helpful for developers of all levels. \n",
       "\n",
       "The information mentions features like:\n",
       "\n",
       "* **LangChain's ability to connect LLMs with tools:** This suggests a focus on helping developers build applications using large language models.\n",
       "* **Support for various tools and APIs:**  This indicates a broad range of potential use cases, catering to both beginners exploring basic integrations and experienced developers building complex systems.\n",
       "\n",
       "\n",
       "Therefore, I can't definitively say whether I am geared towards beginners or experienced developers. My capabilities seem versatile enough to assist both. \n",
       "\n",
       "\n",
       "Q: Mention specific use cases:** If you have a particular application in mind, include it in the query (e.g., \"customer service chatbot,\" \"research assistant\")\n",
       "A: According to the text, CrewAI is especially useful in these multi-step workflows:\n",
       "\n",
       "* **Market research:** Agents could collaborate to gather data from various sources, analyze trends, and generate reports. \n",
       "* **Legal document analysis:** Agents could specialize in different aspects of legal documents, such as identifying key clauses, extracting relevant information, and summarizing findings.\n",
       "* **Product development:** Agents could assist with tasks like brainstorming ideas, researching competitors, prototyping designs, and managing project timelines.\n",
       "* **Coding assistants:** Agents could help with code generation, debugging, documentation, and testing. \n",
       "\n",
       "\n",
       "Let me know if you'd like to explore any of these use cases in more detail! \n",
       "\n",
       "\n",
       "Q: Identify key features:** Are there any specific memory mechanisms or agent capabilities that are particularly important to you?\n",
       "A: Based on the context provided, these stand out as particularly important features:\n",
       "\n",
       "* **ConversationBufferMemory and ConversationSummaryMemory:** The ability for an LLM to remember previous conversation turns is crucial for creating natural and coherent conversations.  `ConversationBufferMemory` stores the entire history, while `ConversationSummaryMemory` provides a condensed version, helpful for managing token limits in longer interactions.\n",
       "* **Agent capabilities with planner-executor model:** LangChain's agents are powerful because they can:\n",
       "    * **Reason and plan:** LLMs within agents can determine the best tools and steps to achieve a goal.\n",
       "    * **Dynamic decision-making:** Agents can adapt their approach based on the evolving situation and tool outputs.\n",
       "    * **Branching logic:**  Agents can handle complex workflows with multiple possible paths.\n",
       "    * **Context-aware memory:**  Agents leverage memory effectively, using past information to make informed decisions in each step. \n",
       "\n",
       "\n",
       "These features combined allow for building truly interactive and intelligent applications that go beyond simple question-answering.\n",
       "\n",
       "Q: Remember, the more specific and detailed your query is, the better the chances of retrieving relevant and useful information\n",
       "A: This statement emphasizes the importance of crafting precise and specific queries when searching for information. \n",
       "\n",
       "Here's why:\n",
       "\n",
       "* **Specificity filters noise:**  Broad queries often return a large volume of results, many of which may be irrelevant. A specific query narrows the search focus, leading to more targeted and useful results.\n",
       "* **Understanding intent:**  A well-defined query clearly conveys your information needs. It helps search engines or systems understand exactly what you're looking for.\n",
       "\n",
       "**Example:**\n",
       "\n",
       "Instead of searching for \"dogs,\" a more specific query like \"golden retriever puppies for sale in San Francisco\" will yield much more relevant results. \n",
       "\n",
       "\n",
       "Let me know if you have any other questions!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 6\n",
    "query = \"How does LangChain use memory and agents compared to CrewAI?\"\n",
    "final_answer = full_query_decomposition_rag_pipeline(query)\n",
    "print(\"Final Answer:\\n\")\n",
    "display(Markdown(final_answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf6bcdc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
