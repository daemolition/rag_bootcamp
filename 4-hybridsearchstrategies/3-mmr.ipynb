{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0b37202",
   "metadata": {},
   "source": [
    "## Maximal Marginal Relevance\n",
    "\n",
    "MMR (Maximal Marginal Relevance) is a powerful diversity-aware retrieval technique used in information retrieval and RAG pipelines to balance relevance and novelty when selecting documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65c3a57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains.retrieval import create_retrieval_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "31ca0c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TextLoader\n",
    "loader = TextLoader(\"langchain_rag_dataset.txt\")\n",
    "raw_documents = loader.load()\n",
    "\n",
    "# TextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "\n",
    "documents = text_splitter.split_documents(raw_documents)\n",
    "\n",
    "# Adding page numbers to metadata\n",
    "for i, doc in enumerate(documents):\n",
    "    doc.metadata['page'] = i + 1\n",
    "\n",
    "# Embeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# FAISS\n",
    "vectorstore = FAISS.from_documents(\n",
    "    documents,\n",
    "    embeddings\n",
    ")\n",
    "\n",
    "# Retrieval\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_kwargs={\n",
    "        \"k\": 3\n",
    "    },\n",
    "    search_type=\"mmr\"\n",
    ")\n",
    "\n",
    "# Model \n",
    "llm = ChatOllama(model=\"gemma3:4b-it-q4_K_M\", temperature=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "fcbb4ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# So werden einzelne Dokumente mit Metadaten formatiert\n",
    "document_prompt = PromptTemplate.from_template(\"\"\"  \n",
    "Page number: {page}                                                                                         \n",
    "Source: {source}\n",
    "Content: {page_content}\n",
    "\"\"\")\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"\"\"\n",
    "You are a helpful assistant. You will be given a question and you will answer it based on the context provided in the documents.\n",
    "If the question is not answered by the context, you can say \"I don't knwow\" or \"I'm sorry, I don't know that.\n",
    "\n",
    "Each document has metadata. Use them to provide a better answer.\n",
    "\n",
    "Return your answer in markdown format.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Return the metadata as to your answer as follows:\n",
    "\n",
    "Metadata:\n",
    "*page\n",
    "*source\n",
    "\n",
    "Question: {input}\n",
    "\"\"\")\n",
    "\n",
    "# Document Chain\n",
    "document_chain = create_stuff_documents_chain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    document_prompt=document_prompt\n",
    ")\n",
    "\n",
    "# Retrieval Chain\n",
    "retrieval_chain = create_retrieval_chain(\n",
    "    retriever=retriever,\n",
    "    combine_docs_chain=document_chain\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "145a2cea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Metadata:\n",
       "*page: 1\n",
       "*source: langchain_rag_dataset.txt\n",
       "\n",
       "LangChain provides abstractions for working with prompts, chains, memory, and agents, making it easier to build complex LLM-based systems. Memory in LangChain helps models retain previous interactions, making multi-turn conversations more coherent. Agents in LangChain can use tools like calculators, search APIs, or custom functions based on the instructions they receive."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "query = {\"input\": \"How does LangChain support agents and memory?\"}\n",
    "response = retrieval_chain.invoke(query)\n",
    "\n",
    "display(Markdown(response['answer']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "8bdebeed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'How does LangChain support agents and memory?',\n",
       " 'context': [Document(id='e88bb1b9-51dd-4186-a70b-4c3610a42bde', metadata={'source': 'langchain_rag_dataset.txt', 'page': 3}, page_content='Memory in LangChain helps models retain previous interactions, making multi-turn conversations more coherent.\\nAgents in LangChain can use tools like calculators, search APIs, or custom functions based on the instructions they receive.'),\n",
       "  Document(id='6897db18-4e79-40a8-a0b2-9584e060ed0d', metadata={'source': 'langchain_rag_dataset.txt', 'page': 5}, page_content='Chroma is a lightweight vector store often used in LangChain for embedding-based document storage and retrieval.\\nPrompt templates in LangChain support Jinja-style formatting and variable injection to customize model inputs.'),\n",
       "  Document(id='15214e69-f4a6-4599-8f6f-2e6c0d3e9550', metadata={'source': 'langchain_rag_dataset.txt', 'page': 1}, page_content='LangChain is an open-source framework designed to simplify the development of applications using large language models (LLMs).\\nLangChain provides abstractions for working with prompts, chains, memory, and agents, making it easier to build complex LLM-based systems.')],\n",
       " 'answer': 'Metadata:\\n*page: 1\\n*source: langchain_rag_dataset.txt\\n\\nLangChain provides abstractions for working with prompts, chains, memory, and agents, making it easier to build complex LLM-based systems. Memory in LangChain helps models retain previous interactions, making multi-turn conversations more coherent. Agents in LangChain can use tools like calculators, search APIs, or custom functions based on the instructions they receive.'}"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbedfb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
