{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "54121f6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from typing import List\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.document_loaders import TextLoader, WebBaseLoader\n",
    "from langchain_community.document_loaders.youtube import YoutubeLoader\n",
    "from langchain_community.document_loaders import ArxivLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "from langgraph.graph import StateGraph, END\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a9a309b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "urls = [\n",
    "    \"https://docs.langchain.com/oss/python/langgraph/overview\",\n",
    "    \"https://docs.langchain.com/oss/python/langgraph/workflows-agents\",\n",
    "    \"https://docs.langchain.com/oss/python/langgraph/graph-api#map-reduce-and-the-send-api\"\n",
    "]\n",
    "\n",
    "# Load the Docs\n",
    "docs = [WebBaseLoader(url) for url in urls]\n",
    "\n",
    "# Split the Docs into Chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=60)\n",
    "chunks = text_splitter.split_documents([doc for loader in docs for doc in loader.load()])\n",
    "\n",
    "# Create the Vector Store\n",
    "vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "retriever=vectorstore.as_retriever(search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a282fb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"Qwen/Qwen3-Coder-30B-A3B-Instruct\", \n",
    "    temperature=0, \n",
    "    base_url=\"http://192.168.1.20:10000/v1\", \n",
    "    api_key=\"123\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "686adfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Router\n",
    "from typing import Literal\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class RouteQuery(BaseModel):\n",
    "    \"\"\"Route a user query to the most relevant datasource.\"\"\"\n",
    "    datasource: Literal[\"vectorestore\", \"web_search\"] = Field(\n",
    "        ...,\n",
    "        description=\"Given a user question choost to route it to web search or a vectorestore.\"\n",
    "    )\n",
    "    \n",
    "# LLM With function calling\n",
    "structured_llm_router = llm.with_structured_output(RouteQuery)\n",
    "\n",
    "# Prompt\n",
    "system = \"\"\"Your are an expert at routing a user question to a vectorestore or web search.\n",
    "The vectorstore contains documents related to agents, prompt engineering, and adversarial attacks on LLMs.\n",
    "Use the vectorstore for questions on these topics. Otherwise, use web-search.\n",
    "\"\"\"\n",
    "\n",
    "route_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system),\n",
    "    (\"user\", \"Question: {question}\")\n",
    "])\n",
    "\n",
    "question_router = route_prompt | structured_llm_router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ef148204",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RouteQuery(datasource='vectorestore')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_router.invoke({\"question\": \"What are the types of agent memory?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "51ca207b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradeDocument(BaseModel):\n",
    "   \"\"\"Binary Score for relevance check on retrieved documents\"\"\"\n",
    "   binary_score: str = Field(\n",
    "       description=\"A binary score indicating whether the retrieved documents are relevant to the user's query. Return 'yes' if relevant, 'no' if not.\",\n",
    "   )\n",
    "   \n",
    "structured_llm_grader = llm.with_structured_output(GradeDocument)\n",
    "\n",
    "system = \"\"\"\n",
    "You are a grader assessing relevance of a retrieved document to a usser question. \\n\n",
    "If the document contains keyword(s) or semantci meaning related to the question, grade it as relevant. \\n\n",
    "Give binary score 'yes' ord 'no' score to indicate whether the document is relevant to the question.\n",
    "\"\"\"\n",
    "\n",
    "grade_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system),\n",
    "    (\"user\", \"User Question: {question}\\n\\n Retrieved Document: {document}\")\n",
    "])\n",
    "\n",
    "retrieval_grader = grade_prompt | structured_llm_grader\n",
    "question = \"agent memory\"\n",
    "docs = retriever.invoke(question)\n",
    "doc_txt = docs[1].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "66f3f0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "          (\"system\", \"\"\"You are an expert at question answering. Use the following context to answer the question.\n",
    "            If you don't know the answer, just say \"I don't know\", don't try to make up an answer.\"\"\"),\n",
    "])\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([f\"Document {i+1}:\\n{doc.page_content}\" for i, doc in enumerate(docs)])\n",
    "\n",
    "rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "generation = rag_chain.invoke({\"context\": format_docs(docs), \"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6d4490b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "system = \"\"\"\n",
    "Ypur are an expert question rewriter that converts an input question into a better version that is optimizesd \\n\n",
    "for web search. Look at the input try to reason about the underlying semantic intent / meaning.\n",
    "\"\"\"\n",
    "\n",
    "rewrite_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system),\n",
    "    (\"user\", \"here is the initial question: \\n\\n {question} \\n Formulate an improved question.\")\n",
    "])\n",
    "\n",
    "question_rewriter = rewrite_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2643c3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_tavily import TavilySearch\n",
    "tavily = TavilySearch(max_results=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f89f1d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, TypedDict\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"Represents the state of our graph.\n",
    "    \n",
    "    Attributes:\n",
    "        question: question\n",
    "        generation: LLM generation\n",
    "        web_search: wheter to add search\n",
    "        documents: retrieved documents\n",
    "    \"\"\"\n",
    "    question: str\n",
    "    generation: str\n",
    "    web_search: bool\n",
    "    documents: List[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3a6a66a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "def retrieve(state):\n",
    "    \"\"\"Retrieve relevant documents based on the question in the state.\"\"\"\n",
    "    \n",
    "    print(\"---RETRIEVE---\")\n",
    "    question = state[\"question\"]\n",
    "    \n",
    "    # Retrieval\n",
    "    documents = retriever.invoke(question)\n",
    "    return {'documents': documents, 'question': question}\n",
    "\n",
    "\n",
    "def generate(state):\n",
    "    \"\"\"Generate an answer based on the question and retrieved documents in the state.\"\"\"\n",
    "    \n",
    "    print(\"---GENERATE---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    \n",
    "    # Format documents for context\n",
    "    context = format_docs(documents)\n",
    "    \n",
    "    # Generation\n",
    "    generation = rag_chain.invoke({\"context\": context, \"question\": question})\n",
    "    return {'generation': generation, 'question': question}\n",
    "\n",
    "def grade_documents(state):\n",
    "    \"\"\"Grade the relevance of retrieved documents in the state.\"\"\"\n",
    "    \n",
    "    print(\"---GRADE DOCUMENTS---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    \n",
    "    graded_docs = []\n",
    "    web_search = \"No\"\n",
    "    for doc in documents:\n",
    "        score = retrieval_grader.invoke({\"question\": question, \"document\": doc.page_content})\n",
    "        grade = score.binary_score\n",
    "        \n",
    "        if grade == \"yes\":\n",
    "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
    "            graded_docs.append(doc)\n",
    "        else:\n",
    "            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
    "            web_search = \"Yes\"\n",
    "            continue            \n",
    "        \n",
    "    \n",
    "    return {'documents': graded_docs, 'question': question, \"web_search\": web_search}\n",
    "\n",
    "\n",
    "def transform_query(state):\n",
    "    \"\"\"Transform the question in the state using web search if needed.\"\"\"\n",
    "    print(\"---TRANSFORM QUERY---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    \n",
    "    better_question = question_rewriter.invoke({\"question\": question})\n",
    "    return {\"documents\": documents, \"question\": better_question}\n",
    "\n",
    "def web_search(state):\n",
    "    \"\"\"Perform web search to retrieve additional documents if needed.\"\"\"\n",
    "    print(\"---WEB SEARCH---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    \n",
    "    # Get the raw response from Tavily\n",
    "    raw_response = tavily.invoke(question)\n",
    "    \n",
    "    # Handle different response formats\n",
    "    if isinstance(raw_response, str):\n",
    "        # If it's already a string, use it directly\n",
    "        web_results = raw_response\n",
    "    elif isinstance(raw_response, dict):\n",
    "        # If it's a dict, extract content\n",
    "        if \"results\" in raw_response:\n",
    "            # Extract content from results\n",
    "            content_list = [r.get(\"content\", \"\") for r in raw_response[\"results\"]]\n",
    "            web_results = \"\\n\".join(content_list)\n",
    "        else:\n",
    "            web_results = raw_response.get(\"content\", str(raw_response))\n",
    "    elif isinstance(raw_response, list):\n",
    "        # If it's a list, extract content\n",
    "        content_list = [r.get(\"content\", \"\") if isinstance(r, dict) else str(r) for r in raw_response]\n",
    "        web_results = \"\\n\".join(content_list)\n",
    "    else:\n",
    "        web_results = str(raw_response)\n",
    "    \n",
    "    web_results_doc = Document(page_content=web_results)\n",
    "    documents.append(web_results_doc)\n",
    "    return {\"documents\": documents, \"question\": question}\n",
    "\n",
    "def route_question(state):\n",
    "    \"\"\"Route qestion to vectorestore or web search based on content.\"\"\"\n",
    "    print(\"---ROUTE QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    \n",
    "    route = question_router.invoke({\"question\": question})\n",
    "    if route.datasource == \"vectorestore\":\n",
    "        print(\"---ROUTE TO VECTORESTORE---\")\n",
    "        return \"retrieve\"\n",
    "    else:\n",
    "        print(\"---ROUTE TO WEB SEARCH---\")\n",
    "        return \"transform_query\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "36fba0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decide_to_generate(state):\n",
    "    \"\"\"Determine whether to generate an answer based on the relevance of documents.\"\"\"\n",
    "    print(\"---DECIDE TO GENERATE---\")\n",
    "    web_search_needed = state[\"web_search\"]\n",
    "    \n",
    "    if web_search_needed == \"Yes\":\n",
    "        print(\"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTIN, TRANSFORM QUERY---\")\n",
    "        return \"transform_query\"\n",
    "    else:\n",
    "        print(\"---DECISION: GENERATE---\")\n",
    "        return \"generate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "409ce1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END, START\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "workflow.add_node(\"retrieve\", retrieve)\n",
    "workflow.add_node(\"grade_documents\", grade_documents)\n",
    "workflow.add_node(\"generate\", generate)\n",
    "workflow.add_node(\"transform_query\", transform_query)\n",
    "workflow.add_node(\"web_search_node\", web_search)\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    START,\n",
    "    route_question,\n",
    "    {\n",
    "        \"web_search_node\": \"generate\",\n",
    "        \"vectorstore\": \"retrieve\"\n",
    "    }\n",
    ")\n",
    "workflow.add_edge(\"web_search_node\", \"generate\")\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    decide_to_generate,\n",
    "    {\n",
    "        \"transform_query\": \"transform_query\",\n",
    "        \"generate\": \"generate\"\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"transform_query\", \"retrieve\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"generate\",\n",
    "    grade_documents,\n",
    "    {\n",
    "     \"not supported\": \"generate\",\n",
    "     \"useful\": END,\n",
    "     \"not usefule\": \"transform_query\",        \n",
    "    },\n",
    ")\n",
    "\n",
    "app = workflow.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33241afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = app.invoke({\"question\": \"What are the types of agent memory?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c8e41c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
