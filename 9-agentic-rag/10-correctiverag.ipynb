{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1e5dabc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List\n",
    "from pydantic import BaseModel\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.document_loaders import TextLoader, WebBaseLoader\n",
    "from langchain_community.document_loaders.youtube import YoutubeLoader\n",
    "from langchain_community.document_loaders import ArxivLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "from langgraph.graph import StateGraph, END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9bfa8788",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "urls = [\n",
    "    \"https://docs.langchain.com/oss/python/langgraph/overview\",\n",
    "    \"https://docs.langchain.com/oss/python/langgraph/workflows-agents\",\n",
    "    \"https://docs.langchain.com/oss/python/langgraph/graph-api#map-reduce-and-the-send-api\"\n",
    "]\n",
    "\n",
    "# Load the Docs\n",
    "docs = [WebBaseLoader(url) for url in urls]\n",
    "\n",
    "# Split the Docs into Chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=60)\n",
    "chunks = text_splitter.split_documents([doc for loader in docs for doc in loader.load()])\n",
    "\n",
    "# Create the Vector Store\n",
    "vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "retriever=vectorstore.as_retriever(search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "22238600",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"Qwen/Qwen3-Coder-30B-A3B-Instruct\", \n",
    "    temperature=0, \n",
    "    base_url=\"http://192.168.1.20:10000/v1\", \n",
    "    api_key=\"123\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cce22ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradeDocument(BaseModel):\n",
    "   \"\"\"Binary Score for relevance check on retrieved documents\"\"\"\n",
    "   binary_score: str = Field(\n",
    "       description=\"A binary score indicating whether the retrieved documents are relevant to the user's query. Return 'yes' if relevant, 'no' if not.\",\n",
    "   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fd7da593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary_score='no'\n"
     ]
    }
   ],
   "source": [
    "structured_llm_grader = llm.with_structured_output(GradeDocument)\n",
    "\n",
    "system = \"\"\"\n",
    "You are a grader assessing relevance of a retrieved document to a usser question. \\n\n",
    "If the document contains keyword(s) or semantci meaning related to the question, grade it as relevant. \\n\n",
    "Give binary score 'yes' ord 'no' score to indicate whether the document is relevant to the question.\n",
    "\"\"\"\n",
    "\n",
    "grade_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system),\n",
    "    (\"user\", \"User Question: {question}\\n\\n Retrieved Document: {document}\")\n",
    "])\n",
    "\n",
    "retrieval_grader = grade_prompt | structured_llm_grader\n",
    "question = \"agent memory\"\n",
    "docs = retriever.invoke(question)\n",
    "doc_txt = docs[1].page_content\n",
    "print(retrieval_grader.invoke({\"question\": question, \"document\": doc_txt}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5986046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent memory refers to the stateful capabilities that allow agents to maintain both short-term working memory for current tasks and long-term memory across sessions. This comprehensive memory system enables durable execution where agents can persist through failures and resume operations from their previous state. Agents leverage this memory to make autonomous decisions and solve problems in unpredictable environments while maintaining continuity across interactions.\n"
     ]
    }
   ],
   "source": [
    "from langchain_classic import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([f\"Document {i+1}:\\n{doc.page_content}\" for i, doc in enumerate(docs)])\n",
    "\n",
    "rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "generation = rag_chain.invoke({\"context\": format_docs(docs), \"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5ae302a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'**Improved question:** \"What is agent memory in AI systems and how does it work?\"\\n\\n**Reasoning behind the improvement:**\\n\\n1. **Adds specificity**: The original \"agent memory\" is too vague and could refer to multiple concepts (memory in AI agents, memory in robotics, memory in software agents, etc.)\\n\\n2. **Clarifies the core intent**: The improved question clearly asks about the definition and functioning of agent memory, which is likely what the user wants to understand\\n\\n3. **Optimizes for web search**: \\n   - Includes key terms: \"agent memory,\" \"AI systems,\" \"how it works\"\\n   - Uses natural language phrasing that search engines recognize\\n   - Avoids ambiguity that might lead to irrelevant results\\n\\n4. **Addresses underlying semantic intent**: The user appears to be seeking educational information about agent memory as a concept in artificial intelligence or computer science, rather than just a technical term.\\n\\n5. **Search-friendly structure**: The question follows a common informational query pattern that yields relevant results from academic sources, technical documentation, and educational content.'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system = \"\"\"\n",
    "Ypur are an expert question rewriter that converts an input question into a better version that is optimizesd \\n\n",
    "for web search. Look at the input try to reason about the underlying semantic intent / meaning.\n",
    "\"\"\"\n",
    "\n",
    "rewrite_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system),\n",
    "    (\"user\", \"here is the initial question: \\n\\n {question} \\n Formulate an improved question.\")\n",
    "])\n",
    "\n",
    "question_rewriter = rewrite_prompt | llm | StrOutputParser()\n",
    "question_rewriter.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c1811f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_tavily import TavilySearch\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "tavily = TavilySearch(max_results=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1832e9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, TypedDict\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"Represents the state of our graph.\n",
    "    \n",
    "    Attributes:\n",
    "        question: question\n",
    "        generation: LLM generation\n",
    "        web_search: wheter to add search\n",
    "        documents: retrieved documents\n",
    "    \"\"\"\n",
    "    question: str\n",
    "    generation: str\n",
    "    web_search: bool\n",
    "    documents: List[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "491f206c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "def retrieve(state):\n",
    "    \"\"\"Retrieve relevant documents based on the question in the state.\"\"\"\n",
    "    \n",
    "    print(\"---RETRIEVE---\")\n",
    "    question = state[\"question\"]\n",
    "    \n",
    "    # Retrieval\n",
    "    documents = retriever.invoke(question)\n",
    "    return {'documents': documents, 'question': question}\n",
    "\n",
    "\n",
    "def generate(state):\n",
    "    \"\"\"Generate an answer based on the question and retrieved documents in the state.\"\"\"\n",
    "    \n",
    "    print(\"---GENERATE---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    \n",
    "    # Format documents for context\n",
    "    context = format_docs(documents)\n",
    "    \n",
    "    # Generation\n",
    "    generation = rag_chain.invoke({\"context\": context, \"question\": question})\n",
    "    return {'generation': generation, 'question': question}\n",
    "\n",
    "def grade_documents(state):\n",
    "    \"\"\"Grade the relevance of retrieved documents in the state.\"\"\"\n",
    "    \n",
    "    print(\"---GRADE DOCUMENTS---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    \n",
    "    graded_docs = []\n",
    "    web_search = \"No\"\n",
    "    for doc in documents:\n",
    "        score = retrieval_grader.invoke({\"question\": question, \"document\": doc.page_content})\n",
    "        grade = score.binary_score\n",
    "        \n",
    "        if grade == \"yes\":\n",
    "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
    "            graded_docs.append(doc)\n",
    "        else:\n",
    "            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
    "            web_search = \"Yes\"\n",
    "            continue            \n",
    "        \n",
    "    \n",
    "    return {'documents': graded_docs, 'question': question, \"web_search\": web_search}\n",
    "\n",
    "\n",
    "def transform_query(state):\n",
    "    \"\"\"Transform the question in the state using web search if needed.\"\"\"\n",
    "    print(\"---TRANSFORM QUERY---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    \n",
    "    better_question = question_rewriter.invoke({\"question\": question})\n",
    "    return {\"documents\": documents, \"question\": better_question}\n",
    "\n",
    "def web_search(state):\n",
    "    \"\"\"Perform web search to retrieve additional documents if needed.\"\"\"\n",
    "    print(\"---WEB SEARCH---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    \n",
    "    # Get the raw response from Tavily\n",
    "    raw_response = tavily.invoke(question)\n",
    "    \n",
    "    # Handle different response formats\n",
    "    if isinstance(raw_response, str):\n",
    "        # If it's already a string, use it directly\n",
    "        web_results = raw_response\n",
    "    elif isinstance(raw_response, dict):\n",
    "        # If it's a dict, extract content\n",
    "        if \"results\" in raw_response:\n",
    "            # Extract content from results\n",
    "            content_list = [r.get(\"content\", \"\") for r in raw_response[\"results\"]]\n",
    "            web_results = \"\\n\".join(content_list)\n",
    "        else:\n",
    "            web_results = raw_response.get(\"content\", str(raw_response))\n",
    "    elif isinstance(raw_response, list):\n",
    "        # If it's a list, extract content\n",
    "        content_list = [r.get(\"content\", \"\") if isinstance(r, dict) else str(r) for r in raw_response]\n",
    "        web_results = \"\\n\".join(content_list)\n",
    "    else:\n",
    "        web_results = str(raw_response)\n",
    "    \n",
    "    web_results_doc = Document(page_content=web_results)\n",
    "    documents.append(web_results_doc)\n",
    "    return {\"documents\": documents, \"question\": question}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "965709e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decide_to_generate(state):\n",
    "    \"\"\"Determine whether to generate an answer based on the relevance of documents.\"\"\"\n",
    "    print(\"---DECIDE TO GENERATE---\")\n",
    "    web_search_needed = state[\"web_search\"]\n",
    "    \n",
    "    if web_search_needed == \"Yes\":\n",
    "        print(\"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTIN, TRANSFORM QUERY---\")\n",
    "        return \"transform_query\"\n",
    "    else:\n",
    "        print(\"---DECISION: GENERATE---\")\n",
    "        return \"generate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "63cc5dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END, START\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "workflow.add_node(\"retrieve\", retrieve)\n",
    "workflow.add_node(\"grade_documents\", grade_documents)\n",
    "workflow.add_node(\"generate\", generate)\n",
    "workflow.add_node(\"transform_query\", transform_query)\n",
    "workflow.add_node(\"web_search_node\", web_search)\n",
    "\n",
    "workflow.add_edge(START, \"retrieve\")\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    decide_to_generate,\n",
    "    {\n",
    "        \"transform_query\": \"transform_query\",\n",
    "        \"generate\": \"generate\"\n",
    "    }\n",
    ")\n",
    "workflow.add_edge(\"transform_query\", \"web_search_node\")\n",
    "workflow.add_edge(\"web_search_node\", \"generate\")\n",
    "workflow.add_edge(\"generate\", END)\n",
    "\n",
    "app = workflow.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "367e465c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---RETRIEVE---\n",
      "---GRADE DOCUMENTS---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---DECIDE TO GENERATE---\n",
      "---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTIN, TRANSFORM QUERY---\n",
      "---TRANSFORM QUERY---\n",
      "---WEB SEARCH---\n",
      "---GENERATE---\n"
     ]
    }
   ],
   "source": [
    "result = app.invoke({\"question\": \"What are the types of agent memory?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7769fd26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Durable execution: Build agents that persist through failures and can run for extended periods, resuming from where they left off.\n",
       "Human-in-the-loop: Incorporate human oversight by inspecting and modifying agent state at any point.\n",
       "Comprehensive memory: Create stateful agents with both short-term working memory for ongoing reasoning and long-term memory across sessions."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "Markdown(result['documents'][0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccc4d19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
