{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2fd6c5f",
   "metadata": {},
   "source": [
    "## Agentic RAG with workflow to rewrite the prompt to get better results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "a00bd59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader  \n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_core.tools import Tool, tool, StructuredTool\n",
    "from langchain.agents import create_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "56e641e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\n",
    "    \"https://docs.langchain.com/oss/python/langgraph/overview\",\n",
    "    \"https://docs.langchain.com/oss/python/langgraph/workflows-agents\",\n",
    "    \"https://docs.langchain.com/oss/python/langgraph/graph-api#map-reduce-and-the-send-api\"\n",
    "]\n",
    "\n",
    "# Load the Docs\n",
    "docs = [WebBaseLoader(url).load() for url in urls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "2f8bcd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text Splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500, \n",
    "    chunk_overlap=50,\n",
    "    separators=[\".\", \",\"],\n",
    "    keep_separator=True\n",
    ")\n",
    "docs_list = [doc for sublist in docs for doc in sublist]\n",
    "split_docs = text_splitter.split_documents(docs_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "7d11667d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embeddings\n",
    "embeddings = HuggingFaceEmbeddings(model=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Vectorstore\n",
    "vectorstore = FAISS.from_documents(documents=split_docs, embedding=embeddings)\n",
    "\n",
    "# Retriever\n",
    "retriever = vectorstore.as_retriever(search_kwargs={'k': 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "30c14a46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tool(name='Retriever_Tool', description='Use this tool to retrieve relevant documents', func=<function create_retriever_tool at 0x7327f1abee80>)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Retriever to retriever tools\n",
    "\n",
    "def create_retriever_tool(query):\n",
    "    \"\"\"Use this tool to retrieve relevant documents\"\"\"\n",
    "    print(\"Calling retriever tool\")\n",
    "    result = retriever.invoke(query)\n",
    "    return \"\\n\\n\".join([docs.page_content for docs in result])\n",
    "\n",
    "\n",
    "retriever_tool = Tool(\n",
    "    name=\"Retriever_Tool\",\n",
    "    description=\"Use this tool to retrieve relevant documents\",\n",
    "    func=create_retriever_tool\n",
    ")\n",
    "\n",
    "retriever_tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35bb598b",
   "metadata": {},
   "source": [
    "## Langchain - Seperate Vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "fe09e811",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Document(metadata={'source': 'https://docs.langchain.com/oss/python/langchain/rag', 'title': 'Build a RAG agent with LangChain - Docs by LangChain', 'language': 'en'}, page_content='Build a RAG agent with LangChain - Docs by LangChainSkip to main contentDocs by LangChain home pageLangChain + LangGraphSearch...âŒ˜KGitHubTry LangSmithTry LangSmithSearch...NavigationLangChainBuild a RAG agent with LangChainLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonLearnTutorialsLangChainSemantic searchRAG agentSQL agentSupervisor agentLangGraphConceptual overviewsMemoryContextGraph APIFunctional APIAdditional resourcesLangChain AcademyCase studiesOn this pageOverviewConceptsPreviewSetupInstallationLangSmithComponents1. IndexingLoading documentsSplitting documentsStoring documents2. Retrieval and GenerationRAG agentsRAG chainsNext stepsTutorialsLangChainBuild a RAG agent with LangChainCopy pageCopy page\\u200bOverview\\nOne of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.\\nThis tutorial will show how to build a simple Q&A application over an unstructured text data source. We will demonstrate:\\n\\nA RAG agent that executes searches with a simple tool. This is a good general-purpose implementation.\\nA two-step RAG chain that uses just a single LLM call per query. This is a fast and effective method for simple queries.\\n\\n\\u200bConcepts\\nWe will cover the following concepts:\\n\\n\\nIndexing: a pipeline for ingesting data from a source and indexing it. This usually happens in a separate process.\\n\\n\\nRetrieval and generation: the actual RAG process, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model.\\n\\n\\nOnce weâ€™ve indexed our data, we will use an agent as our orchestration framework to implement the retrieval and generation steps.\\nThe indexing portion of this tutorial will largely follow the semantic search tutorial.If your data is already available for search (i.e., you have a function to execute a search), or youâ€™re comfortable with the content from that tutorial, feel free to skip to the section on retrieval and generation\\n\\u200bPreview\\nIn this guide weâ€™ll build an app that answers questions about the websiteâ€™s content. The specific website we will use is the LLM Powered Autonomous Agents blog post by Lilian Weng, which allows us to ask questions about the contents of the post.\\nWe can create a simple indexing pipeline and RAG chain to do this in ~40 lines of code. See below for the full code snippet:\\nExpand for full code snippetCopyAsk AIimport bs4\\nfrom langchain.agents import AgentState, create_agent\\nfrom langchain_community.document_loaders import WebBaseLoader\\nfrom langchain.messages import MessageLikeRepresentation\\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\\n\\n# Load and chunk contents of the blog\\nloader = WebBaseLoader(\\n    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\\n    bs_kwargs=dict(\\n        parse_only=bs4.SoupStrainer(\\n            class_=(\"post-content\", \"post-title\", \"post-header\")\\n        )\\n    ),\\n)\\ndocs = loader.load()\\n\\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\\nall_splits = text_splitter.split_documents(docs)\\n\\n# Index chunks\\n_ = vector_store.add_documents(documents=all_splits)\\n\\n# Construct a tool for retrieving context\\n@tool(response_format=\"content_and_artifact\")\\ndef retrieve_context(query: str):\\n    \"\"\"Retrieve information to help answer a query.\"\"\"\\n    retrieved_docs = vector_store.similarity_search(query, k=2)\\n    serialized = \"\\\\n\\\\n\".join(\\n        (f\"Source: {doc.metadata}\\\\nContent: {doc.page_content}\")\\n        for doc in retrieved_docs\\n    )\\n    return serialized, retrieved_docs\\n\\ntools = [retrieve_context]\\n# If desired, specify custom instructions\\nprompt = (\\n    \"You have access to a tool that retrieves context from a blog post. \"\\n    \"Use the tool to help answer user queries.\"\\n)\\nagent = create_agent(model, tools, system_prompt=prompt)\\nCopyAsk AIquery = \"What is task decomposition?\"\\nfor step in agent.stream(\\n    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\\n    stream_mode=\"values\",\\n):\\n    step[\"messages\"][-1].pretty_print()\\nCopyAsk AI================================ Human Message =================================\\n\\nWhat is task decomposition?\\n================================== Ai Message ==================================\\nTool Calls:\\n  retrieve_context (call_xTkJr8njRY0geNz43ZvGkX0R)\\n Call ID: call_xTkJr8njRY0geNz43ZvGkX0R\\n  Args:\\n    query: task decomposition\\n================================= Tool Message =================================\\nName: retrieve_context\\n\\nSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}\\nContent: Task decomposition can be done by...\\n\\nSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}\\nContent: Component One: Planning...\\n================================== Ai Message ==================================\\n\\nTask decomposition refers to...\\nCheck out the LangSmith trace.\\n\\u200bSetup\\n\\u200bInstallation\\nThis tutorial requires these langchain dependencies:\\npipuvcondaCopyAsk AIpip install langchain langchain-text-splitters langchain-community bs4\\n\\nFor more details, see our Installation guide.\\n\\u200bLangSmith\\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with LangSmith.\\nAfter you sign up at the link above, make sure to set your environment variables to start logging traces:\\nCopyAsk AIexport LANGSMITH_TRACING=\"true\"\\nexport LANGSMITH_API_KEY=\"...\"\\n\\nOr, set them in Python:\\nCopyAsk AIimport getpass\\nimport os\\n\\nos.environ[\"LANGSMITH_TRACING\"] = \"true\"\\nos.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\\n\\n\\u200bComponents\\nWe will need to select three components from LangChainâ€™s suite of integrations.\\nSelect a chat model:\\n OpenAI Anthropic Azure Google Gemini AWS BedrockðŸ‘‰ Read the OpenAI chat model integration docsCopyAsk AIpip install -U \"langchain[openai]\"\\ninit_chat_modelModel ClassCopyAsk AIimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\\n\\nmodel = init_chat_model(\"gpt-4.1\")\\n\\nSelect an embeddings model:\\n OpenAI Azure Google Gemini Google Vertex AWS HuggingFace Ollama Cohere MistralAI Nomic NVIDIA Voyage AI IBM watsonx FakeCopyAsk AIpip install -U \"langchain-openai\"\\nCopyAsk AIimport getpass\\nimport os\\n\\nif not os.environ.get(\"OPENAI_API_KEY\"):\\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\\n\\nfrom langchain_openai import OpenAIEmbeddings\\n\\nembeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\\n\\nSelect a vector store:\\n In-memory AstraDB Chroma FAISS Milvus MongoDB PGVector PGVectorStore Pinecone QdrantCopyAsk AIpip install -U \"langchain-core\"\\nCopyAsk AIfrom langchain_core.vectorstores import InMemoryVectorStore\\n\\nvector_store = InMemoryVectorStore(embeddings)\\n\\n\\u200b1. Indexing\\nThis section is an abbreviated version of the content in the semantic search tutorial.If your data is already indexed and available for search (i.e., you have a function to execute a search), or if youâ€™re comfortable with document loaders, embeddings, and vector stores, feel free to skip to the next section on retrieval and generation.\\nIndexing commonly works as follows:\\n\\nLoad: First we need to load our data. This is done with Document Loaders.\\nSplit: Text splitters break large Documents into smaller chunks. This is useful both for indexing data and passing it into a model, as large chunks are harder to search over and wonâ€™t fit in a modelâ€™s finite context window.\\nStore: We need somewhere to store and index our splits, so that they can be searched over later. This is often done using a VectorStore and Embeddings model.\\n\\n\\n\\u200bLoading documents\\nWe need to first load the blog post contents. We can use DocumentLoaders for this, which are objects that load in data from a source and return a list of Document objects.\\nIn this case weâ€™ll use the WebBaseLoader, which uses urllib to load HTML from web URLs and BeautifulSoup to parse it to text. We can customize the HTML -> text parsing by passing in parameters into the BeautifulSoup parser via bs_kwargs (see BeautifulSoup docs). In this case only HTML tags with class â€œpost-contentâ€, â€œpost-titleâ€, or â€œpost-headerâ€ are relevant, so weâ€™ll remove all others.\\nCopyAsk AIimport bs4\\nfrom langchain_community.document_loaders import WebBaseLoader\\n\\n# Only keep post title, headers, and content from the full HTML.\\nbs4_strainer = bs4.SoupStrainer(class_=(\"post-title\", \"post-header\", \"post-content\"))\\nloader = WebBaseLoader(\\n    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\\n    bs_kwargs={\"parse_only\": bs4_strainer},\\n)\\ndocs = loader.load()\\n\\nassert len(docs) == 1\\nprint(f\"Total characters: {len(docs[0].page_content)}\")\\n\\nCopyAsk AITotal characters: 43131\\n\\nCopyAsk AIprint(docs[0].page_content[:500])\\n\\nCopyAsk AI      LLM Powered Autonomous Agents\\n\\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview#\\nIn\\n\\nGo deeper\\nDocumentLoader: Object that loads data from a source as list of Documents.\\n\\nIntegrations: 160+ integrations to choose from.\\nBaseLoader: API reference for the base interface.\\n\\n\\u200bSplitting documents\\nOur loaded document is over 42k characters which is too long to fit into the context window of many models. Even for those models that could fit the full post in their context window, models can struggle to find information in very long inputs.\\nTo handle this weâ€™ll split the Document into chunks for embedding and vector storage. This should help us retrieve only the most relevant parts of the blog post at run time.\\nAs in the semantic search tutorial, we use a RecursiveCharacterTextSplitter, which will recursively split the document using common separators like new lines until each chunk is the appropriate size. This is the recommended text splitter for generic text use cases.\\nCopyAsk AIfrom langchain_text_splitters import RecursiveCharacterTextSplitter\\n\\ntext_splitter = RecursiveCharacterTextSplitter(\\n    chunk_size=1000,  # chunk size (characters)\\n    chunk_overlap=200,  # chunk overlap (characters)\\n    add_start_index=True,  # track index in original document\\n)\\nall_splits = text_splitter.split_documents(docs)\\n\\nprint(f\"Split blog post into {len(all_splits)} sub-documents.\")\\n\\nCopyAsk AISplit blog post into 66 sub-documents.\\n\\nGo deeper\\nTextSplitter: Object that splits a list of Document objects into smaller\\nchunks for storage and retrieval.\\n\\nIntegrations\\nInterface: API reference for the base interface.\\n\\n\\u200bStoring documents\\nNow we need to index our 66 text chunks so that we can search over them at runtime. Following the semantic search tutorial, our approach is to embed the contents of each document split and insert these embeddings into a vector store. Given an input query, we can then use vector search to retrieve relevant documents.\\nWe can embed and store all of our document splits in a single command using the vector store and embeddings model selected at the start of the tutorial.\\nCopyAsk AIdocument_ids = vector_store.add_documents(documents=all_splits)\\n\\nprint(document_ids[:3])\\n\\nCopyAsk AI[\\'07c18af6-ad58-479a-bfb1-d508033f9c64\\', \\'9000bf8e-1993-446f-8d4d-f4e507ba4b8f\\', \\'ba3b5d14-bed9-4f5f-88be-44c88aedc2e6\\']\\n\\nGo deeper\\nEmbeddings: Wrapper around a text embedding model, used for converting text to embeddings.\\n\\nIntegrations: 30+ integrations to choose from.\\nInterface: API reference for the base interface.\\n\\nVectorStore: Wrapper around a vector database, used for storing and querying embeddings.\\n\\nIntegrations: 40+ integrations to choose from.\\nInterface: API reference for the base interface.\\n\\nThis completes the Indexing portion of the pipeline. At this point we have a query-able vector store containing the chunked contents of our blog post. Given a user question, we should ideally be able to return the snippets of the blog post that answer the question.\\n\\u200b2. Retrieval and Generation\\nRAG applications commonly work as follows:\\n\\nRetrieve: Given a user input, relevant splits are retrieved from storage using a Retriever.\\nGenerate: A model produces an answer using a prompt that includes both the question with the retrieved data\\n\\n\\nNow letâ€™s write the actual application logic. We want to create a simple application that takes a user question, searches for documents relevant to that question, passes the retrieved documents and initial question to a model, and returns an answer.\\nWe will demonstrate:\\n\\nA RAG agent that executes searches with a simple tool. This is a good general-purpose implementation.\\nA two-step RAG chain that uses just a single LLM call per query. This is a fast and effective method for simple queries.\\n\\n\\u200bRAG agents\\nOne formulation of a RAG application is as a simple agent with a tool that retrieves information. We can assemble a minimal RAG agent by implementing a tool that wraps our vector store:\\nCopyAsk AIfrom langchain.tools import tool\\n\\n@tool(response_format=\"content_and_artifact\")\\ndef retrieve_context(query: str):\\n    \"\"\"Retrieve information to help answer a query.\"\"\"\\n    retrieved_docs = vector_store.similarity_search(query, k=2)\\n    serialized = \"\\\\n\\\\n\".join(\\n        (f\"Source: {doc.metadata}\\\\nContent: {doc.page_content}\")\\n        for doc in retrieved_docs\\n    )\\n    return serialized, retrieved_docs\\n\\nHere we use the tool decorator to configure the tool to attach raw documents as artifacts to each ToolMessage. This will let us access document metadata in our application, separate from the stringified representation that is sent to the model.\\nRetrieval tools are not limited to a single string query argument, as in the above example. You can\\nforce the LLM to specify additional search parameters by adding argumentsâ€” for example, a category:CopyAsk AIfrom typing import Literal\\n\\ndef retrieve_context(query: str, section: Literal[\"beginning\", \"middle\", \"end\"]):\\n\\nGiven our tool, we can construct the agent:\\nCopyAsk AIfrom langchain.agents import create_agent\\n\\n\\ntools = [retrieve_context]\\n# If desired, specify custom instructions\\nprompt = (\\n    \"You have access to a tool that retrieves context from a blog post. \"\\n    \"Use the tool to help answer user queries.\"\\n)\\nagent = create_agent(model, tools, system_prompt=prompt)\\n\\nLetâ€™s test this out. We construct a question that would typically require an iterative sequence of retrieval steps to answer:\\nCopyAsk AIquery = (\\n    \"What is the standard method for Task Decomposition?\\\\n\\\\n\"\\n    \"Once you get the answer, look up common extensions of that method.\"\\n)\\n\\nfor event in agent.stream(\\n    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\\n    stream_mode=\"values\",\\n):\\n    event[\"messages\"][-1].pretty_print()\\n\\nCopyAsk AI================================ Human Message =================================\\n\\nWhat is the standard method for Task Decomposition?\\n\\nOnce you get the answer, look up common extensions of that method.\\n================================== Ai Message ==================================\\nTool Calls:\\n  retrieve_context (call_d6AVxICMPQYwAKj9lgH4E337)\\n Call ID: call_d6AVxICMPQYwAKj9lgH4E337\\n  Args:\\n    query: standard method for Task Decomposition\\n================================= Tool Message =================================\\nName: retrieve_context\\n\\nSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}\\nContent: Task decomposition can be done...\\n\\nSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}\\nContent: Component One: Planning...\\n================================== Ai Message ==================================\\nTool Calls:\\n  retrieve_context (call_0dbMOw7266jvETbXWn4JqWpR)\\n Call ID: call_0dbMOw7266jvETbXWn4JqWpR\\n  Args:\\n    query: common extensions of the standard method for Task Decomposition\\n================================= Tool Message =================================\\nName: retrieve_context\\n\\nSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}\\nContent: Task decomposition can be done...\\n\\nSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}\\nContent: Component One: Planning...\\n================================== Ai Message ==================================\\n\\nThe standard method for Task Decomposition often used is the Chain of Thought (CoT)...\\n\\nNote that the agent:\\n\\nGenerates a query to search for a standard method for task decomposition;\\nReceiving the answer, generates a second query to search for common extensions of it;\\nHaving received all necessary context, answers the question.\\n\\nWe can see the full sequence of steps, along with latency and other metadata, in the LangSmith trace.\\nYou can add a deeper level of control and customization using the LangGraph framework directlyâ€” for example, you can add steps to grade document relevance and rewrite search queries. Check out LangGraphâ€™s Agentic RAG tutorial for more advanced formulations.\\n\\u200bRAG chains\\nIn the above agentic RAG formulation we allow the LLM to use its discretion in generating a tool call to help answer user queries. This is a good general-purpose solution, but comes with some trade-offs:\\n\\nâœ… Benefitsâš ï¸ Drawbacks\\nSearch only when needed â€“ The LLM can handle greetings, follow-ups, and simple queries without triggering unnecessary searches.Two inference calls â€“ When a search is performed, it requires one call to generate the query and another to produce the final response.Contextual search queries â€“ By treating search as a tool with a query input, the LLM crafts its own queries that incorporate conversational context.Reduced control â€“ The LLM may skip searches when they are actually needed, or issue extra searches when unnecessary.Multiple searches allowed â€“ The LLM can execute several searches in support of a single user query.\\n\\nAnother common approach is a two-step chain, in which we always run a search (potentially using the raw user query) and incorporate the result as context for a single LLM query. This results in a single inference call per query, buying reduced latency at the expense of flexibility.\\nIn this approach we no longer call the model in a loop, but instead make a single pass.\\nWe can implement this chain by removing tools from the agent and instead incorporating the retrieval step into a custom prompt:\\nCopyAsk AIfrom langchain.agents.middleware import dynamic_prompt, ModelRequest\\n\\n@dynamic_prompt\\ndef prompt_with_context(request: ModelRequest) -> str:\\n    \"\"\"Inject context into state messages.\"\"\"\\n    last_query = request.state[\"messages\"][-1].text\\n    retrieved_docs = vector_store.similarity_search(last_query)\\n\\n    docs_content = \"\\\\n\\\\n\".join(doc.page_content for doc in retrieved_docs)\\n\\n    system_message = (\\n        \"You are a helpful assistant. Use the following context in your response:\"\\n        f\"\\\\n\\\\n{docs_content}\"\\n    )\\n\\n    return system_message\\n\\n\\nagent = create_agent(model, tools=[], middleware=[prompt_with_context])\\n\\nLetâ€™s try this out:\\nCopyAsk AIquery = \"What is task decomposition?\"\\nfor step in agent.stream(\\n    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\\n    stream_mode=\"values\",\\n):\\n    step[\"messages\"][-1].pretty_print()\\n\\nCopyAsk AI================================ Human Message =================================\\n\\nWhat is task decomposition?\\n================================== Ai Message ==================================\\n\\nTask decomposition is...\\n\\nIn the LangSmith trace we can see the retrieved context incorporated into the model prompt.\\nThis is a fast and effective method for simple queries in constrained settings, when we typically do want to run user queries through semantic search to pull additional context.\\nReturning source documentsThe above RAG chain incorporates retrieved context into a single system message for that run.As in the agentic RAG formulation, we sometimes want to include raw source documents in the application state to have access to document metadata. We can do this for the two-step chain case by:\\nAdding a key to the state to store the retrieved documents\\nAdding a new node via a pre-model hook to populate that key (as well as inject the context).\\nCopyAsk AIfrom typing import Any\\nfrom langchain_core.documents import Document\\nfrom langchain.agents.middleware import AgentMiddleware, AgentState\\n\\n\\nclass State(AgentState):\\n    context: list[Document]\\n\\n\\nclass RetrieveDocumentsMiddleware(AgentMiddleware[State]):\\n    state_schema = State\\n\\n    def before_model(self, state: AgentState) -> dict[str, Any] | None:\\n        last_message = state[\"messages\"][-1]\\n        retrieved_docs = vector_store.similarity_search(last_message.text)\\n\\n        docs_content = \"\\\\n\\\\n\".join(doc.page_content for doc in retrieved_docs)\\n\\n        augmented_message_content = (\\n            f\"{last_message.text}\\\\n\\\\n\"\\n            \"Use the following context to answer the query:\\\\n\"\\n            f\"{docs_content}\"\\n        )\\n        return {\\n            \"messages\": [last_message.model_copy(update={\"content\": augmented_message_content})],\\n            \"context\": retrieved_docs,\\n        }\\n\\n\\nagent = create_agent(\\n    model,\\n    tools=[],\\n    middleware=[RetrieveDocumentsMiddleware()],\\n)\\n\\n\\u200bNext steps\\nNow that weâ€™ve implemented a simple RAG application via create_agent, we can easily incorporate new features and go deeper:\\n\\nStream tokens and other information for responsive user experiences\\nAdd conversational memory to support multi-turn interactions\\nAdd long-term memory to support memory across conversational threads\\nAdd structured responses\\nDeploy your application with LangSmith Deployments\\n\\n\\nEdit the source of this page on GitHub.\\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.Was this page helpful?YesNoBuild a semantic search engine with LangChainPreviousBuild a SQL agentNextâŒ˜IDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify')],\n",
       " [Document(metadata={'source': 'https://docs.langchain.com/oss/python/langchain/knowledge-base', 'title': 'Build a semantic search engine with LangChain - Docs by LangChain', 'language': 'en'}, page_content='Build a semantic search engine with LangChain - Docs by LangChainSkip to main contentDocs by LangChain home pageLangChain + LangGraphSearch...âŒ˜KGitHubTry LangSmithTry LangSmithSearch...NavigationLangChainBuild a semantic search engine with LangChainLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonLearnTutorialsLangChainSemantic searchRAG agentSQL agentSupervisor agentLangGraphConceptual overviewsMemoryContextGraph APIFunctional APIAdditional resourcesLangChain AcademyCase studiesOn this pageOverviewConceptsSetupInstallationLangSmith1. Documents and Document LoadersLoading documentsSplitting2. Embeddings3. Vector stores4. RetrieversNext stepsTutorialsLangChainBuild a semantic search engine with LangChainCopy pageCopy page\\u200bOverview\\nThis tutorial will familiarize you with LangChainâ€™s document loader, embedding, and vector store abstractions. These abstractions are designed to support retrieval of dataâ€”  from (vector) databases and other sources â€” for integration with LLM workflows. They are important for applications that fetch data to be reasoned over as part of model inference, as in the case of retrieval-augmented generation, or RAG.\\nHere we will build a search engine over a PDF document. This will allow us to retrieve passages in the PDF that are similar to an input query. The guide also includes a minimal RAG implementation on top of the search engine.\\n\\u200bConcepts\\nThis guide focuses on retrieval of text data. We will cover the following concepts:\\n\\nDocuments and document loaders;\\nText splitters;\\nEmbeddings;\\nVector stores and retrievers.\\n\\n\\u200bSetup\\n\\u200bInstallation\\nThis tutorial requires the langchain-community and pypdf packages:\\npipcondaCopyAsk AIpip install langchain-community pypdf\\n\\nFor more details, see our Installation guide.\\n\\u200bLangSmith\\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls.\\nAs these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent.\\nThe best way to do this is with LangSmith.\\nAfter you sign up at the link above, make sure to set your environment variables to start logging traces:\\nCopyAsk AIexport LANGSMITH_TRACING=\"true\"\\nexport LANGSMITH_API_KEY=\"...\"\\n\\nOr, if in a notebook, you can set them with:\\nCopyAsk AIimport getpass\\nimport os\\n\\nos.environ[\"LANGSMITH_TRACING\"] = \"true\"\\nos.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\\n\\n\\u200b1. Documents and Document Loaders\\nLangChain implements a Document abstraction, which is intended to represent a unit of text and associated metadata. It has three attributes:\\n\\npage_content: a string representing the content;\\nmetadata: a dict containing arbitrary metadata;\\nid: (optional) a string identifier for the document.\\n\\nThe metadata attribute can capture information about the source of the document, its relationship to other documents, and other information. Note that an individual Document object often represents a chunk of a larger document.\\nWe can generate sample documents when desired:\\nCopyAsk AIfrom langchain_core.documents import Document\\n\\ndocuments = [\\n    Document(\\n        page_content=\"Dogs are great companions, known for their loyalty and friendliness.\",\\n        metadata={\"source\": \"mammal-pets-doc\"},\\n    ),\\n    Document(\\n        page_content=\"Cats are independent pets that often enjoy their own space.\",\\n        metadata={\"source\": \"mammal-pets-doc\"},\\n    ),\\n]\\n\\nHowever, the LangChain ecosystem implements document loaders that integrate with hundreds of common sources. This makes it easy to incorporate data from these sources into your AI application.\\n\\u200bLoading documents\\nLetâ€™s load a PDF into a sequence of Document objects. Here is a sample PDF â€” a 10-k filing for Nike from 2023. We can consult the LangChain documentation for available PDF document loaders.\\nCopyAsk AIfrom langchain_community.document_loaders import PyPDFLoader\\n\\nfile_path = \"../example_data/nke-10k-2023.pdf\"\\nloader = PyPDFLoader(file_path)\\n\\ndocs = loader.load()\\n\\nprint(len(docs))\\n\\nCopyAsk AI107\\n\\nPyPDFLoader loads one Document object per PDF page. For each, we can easily access:\\n\\nThe string content of the page;\\nMetadata containing the file name and page number.\\n\\nCopyAsk AIprint(f\"{docs[0].page_content[:200]}\\\\n\")\\nprint(docs[0].metadata)\\n\\nCopyAsk AITable of Contents\\nUNITED STATES\\nSECURITIES AND EXCHANGE COMMISSION\\nWashington, D.C. 20549\\nFORM 10-K\\n(Mark One)\\nâ˜‘ ANNUAL REPORT PURSUANT TO SECTION 13 OR 15(D) OF THE SECURITIES EXCHANGE ACT OF 1934\\nFO\\n\\n{\\'source\\': \\'../example_data/nke-10k-2023.pdf\\', \\'page\\': 0}\\n\\n\\u200bSplitting\\nFor both information retrieval and downstream question-answering purposes, a page may be too coarse a representation. Our goal in the end will be to retrieve Document objects that answer an input query, and further splitting our PDF will help ensure that the meanings of relevant portions of the document are not â€œwashed outâ€ by surrounding text.\\nWe can use text splitters for this purpose. Here we will use a simple text splitter that partitions based on characters. We will split our documents into chunks of 1000 characters\\nwith 200 characters of overlap between chunks. The overlap helps\\nmitigate the possibility of separating a statement from important\\ncontext related to it. We use the\\nRecursiveCharacterTextSplitter,\\nwhich will recursively split the document using common separators like\\nnew lines until each chunk is the appropriate size. This is the\\nrecommended text splitter for generic text use cases.\\nWe set add_start_index=True so that the character index where each\\nsplit Document starts within the initial Document is preserved as\\nmetadata attribute â€œstart_indexâ€.\\nCopyAsk AIfrom langchain_text_splitters import RecursiveCharacterTextSplitter\\n\\ntext_splitter = RecursiveCharacterTextSplitter(\\n    chunk_size=1000, chunk_overlap=200, add_start_index=True\\n)\\nall_splits = text_splitter.split_documents(docs)\\n\\nprint(len(all_splits))\\n\\nCopyAsk AI514\\n\\n\\u200b2. Embeddings\\nVector search is a common way to store and search over unstructured data (such as unstructured text). The idea is to store numeric vectors that are associated with the text. Given a query, we can embed it as a vector of the same dimension and use vector similarity metrics (such as cosine similarity) to identify related text.\\nLangChain supports embeddings from dozens of providers. These models specify how text should be converted into a numeric vector. Letâ€™s select a model:\\n OpenAI Azure Google Gemini Google Vertex AWS HuggingFace Ollama Cohere MistralAI Nomic NVIDIA Voyage AI IBM watsonx FakeCopyAsk AIpip install -U \"langchain-openai\"\\nCopyAsk AIimport getpass\\nimport os\\n\\nif not os.environ.get(\"OPENAI_API_KEY\"):\\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\\n\\nfrom langchain_openai import OpenAIEmbeddings\\n\\nembeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\\n\\nCopyAsk AIvector_1 = embeddings.embed_query(all_splits[0].page_content)\\nvector_2 = embeddings.embed_query(all_splits[1].page_content)\\n\\nassert len(vector_1) == len(vector_2)\\nprint(f\"Generated vectors of length {len(vector_1)}\\\\n\")\\nprint(vector_1[:10])\\n\\nCopyAsk AIGenerated vectors of length 1536\\n\\n[-0.008586574345827103, -0.03341241180896759, -0.008936782367527485, -0.0036674530711025, 0.010564599186182022, 0.009598285891115665, -0.028587326407432556, -0.015824200585484505, 0.0030416189692914486, -0.012899317778646946]\\n\\nArmed with a model for generating text embeddings, we can next store them in a special data structure that supports efficient similarity search.\\n\\u200b3. Vector stores\\nLangChain VectorStore objects contain methods for adding text and Document objects to the store, and querying them using various similarity metrics. They are often initialized with embedding models, which determine how text data is translated to numeric vectors.\\nLangChain includes a suite of integrations with different vector store technologies. Some vector stores are hosted by a provider (e.g., various cloud providers) and require specific credentials to use; some (such as Postgres) run in separate infrastructure that can be run locally or via a third-party; others can run in-memory for lightweight workloads. Letâ€™s select a vector store:\\n In-memory AstraDB Chroma FAISS Milvus MongoDB PGVector PGVectorStore Pinecone QdrantCopyAsk AIpip install -U \"langchain-core\"\\nCopyAsk AIfrom langchain_core.vectorstores import InMemoryVectorStore\\n\\nvector_store = InMemoryVectorStore(embeddings)\\n\\nHaving instantiated our vector store, we can now index the documents.\\nCopyAsk AIids = vector_store.add_documents(documents=all_splits)\\n\\nNote that most vector store implementations will allow you to connect to an existing vector storeâ€”  e.g., by providing a client, index name, or other information. See the documentation for a specific integration for more detail.\\nOnce weâ€™ve instantiated a VectorStore that contains documents, we can query it. VectorStore includes methods for querying:\\n\\nSynchronously and asynchronously;\\nBy string query and by vector;\\nWith and without returning similarity scores;\\nBy similarity and @[maximum marginal relevance][VectorStore.max_marginal_relevance_search] (to balance similarity with query to diversity in retrieved results).\\n\\nThe methods will generally include a list of Document objects in their outputs.\\nUsage\\nEmbeddings typically represent text as a â€œdenseâ€ vector such that texts with similar meanings are geometrically close. This lets us retrieve relevant information just by passing in a question, without knowledge of any specific key-terms used in the document.\\nReturn documents based on similarity to a string query:\\nCopyAsk AIresults = vector_store.similarity_search(\\n    \"How many distribution centers does Nike have in the US?\"\\n)\\n\\nprint(results[0])\\n\\nCopyAsk AIpage_content=\\'direct to consumer operations sell products through the following number of retail stores in the United States:\\nU.S. RETAIL STORES NUMBER\\nNIKE Brand factory stores 213\\nNIKE Brand in-line stores (including employee-only stores) 74\\nConverse stores (including factory stores) 82\\nTOTAL 369\\nIn the United States, NIKE has eight significant distribution centers. Refer to Item 2. Properties for further information.\\n2023 FORM 10-K 2\\' metadata={\\'page\\': 4, \\'source\\': \\'../example_data/nke-10k-2023.pdf\\', \\'start_index\\': 3125}\\n\\nAsync query:\\nCopyAsk AIresults = await vector_store.asimilarity_search(\"When was Nike incorporated?\")\\n\\nprint(results[0])\\n\\nCopyAsk AIpage_content=\\'Table of Contents\\nPART I\\nITEM 1. BUSINESS\\nGENERAL\\nNIKE, Inc. was incorporated in 1967 under the laws of the State of Oregon. As used in this Annual Report on Form 10-K (this \"Annual Report\"), the terms \"we,\" \"us,\" \"our,\"\\n\"NIKE\" and the \"Company\" refer to NIKE, Inc. and its predecessors, subsidiaries and affiliates, collectively, unless the context indicates otherwise.\\nOur principal business activity is the design, development and worldwide marketing and selling of athletic footwear, apparel, equipment, accessories and services. NIKE is\\nthe largest seller of athletic footwear and apparel in the world. We sell our products through NIKE Direct operations, which are comprised of both NIKE-owned retail stores\\nand sales through our digital platforms (also referred to as \"NIKE Brand Digital\"), to retail accounts and to a mix of independent distributors, licensees and sales\\' metadata={\\'page\\': 3, \\'source\\': \\'../example_data/nke-10k-2023.pdf\\', \\'start_index\\': 0}\\n\\nReturn scores:\\nCopyAsk AI# Note that providers implement different scores; the score here\\n# is a distance metric that varies inversely with similarity.\\n\\nresults = vector_store.similarity_search_with_score(\"What was Nike\\'s revenue in 2023?\")\\ndoc, score = results[0]\\nprint(f\"Score: {score}\\\\n\")\\nprint(doc)\\n\\nCopyAsk AIScore: 0.23699893057346344\\n\\npage_content=\\'Table of Contents\\nFISCAL 2023 NIKE BRAND REVENUE HIGHLIGHTS\\nThe following tables present NIKE Brand revenues disaggregated by reportable operating segment, distribution channel and major product line:\\nFISCAL 2023 COMPARED TO FISCAL 2022\\nâ€¢NIKE, Inc. Revenues were $51.2 billion in fiscal 2023, which increased 10% and 16% compared to fiscal 2022 on a reported and currency-neutral basis, respectively.\\nThe increase was due to higher revenues in North America, Europe, Middle East & Africa (\"EMEA\"), APLA and Greater China, which contributed approximately 7, 6,\\n2 and 1 percentage points to NIKE, Inc. Revenues, respectively.\\nâ€¢NIKE Brand revenues, which represented over 90% of NIKE, Inc. Revenues, increased 10% and 16% on a reported and currency-neutral basis, respectively. This\\nincrease was primarily due to higher revenues in Men\\'s, the Jordan Brand, Women\\'s and Kids\\' which grew 17%, 35%,11% and 10%, respectively, on a wholesale\\nequivalent basis.\\' metadata={\\'page\\': 35, \\'source\\': \\'../example_data/nke-10k-2023.pdf\\', \\'start_index\\': 0}\\n\\nReturn documents based on similarity to an embedded query:\\nCopyAsk AIembedding = embeddings.embed_query(\"How were Nike\\'s margins impacted in 2023?\")\\n\\nresults = vector_store.similarity_search_by_vector(embedding)\\nprint(results[0])\\n\\nCopyAsk AIpage_content=\\'Table of Contents\\nGROSS MARGIN\\nFISCAL 2023 COMPARED TO FISCAL 2022\\nFor fiscal 2023, our consolidated gross profit increased 4% to $22,292 million compared to $21,479 million for fiscal 2022. Gross margin decreased 250 basis points to\\n43.5% for fiscal 2023 compared to 46.0% for fiscal 2022 due to the following:\\n*Wholesale equivalent\\nThe decrease in gross margin for fiscal 2023 was primarily due to:\\nâ€¢Higher NIKE Brand product costs, on a wholesale equivalent basis, primarily due to higher input costs and elevated inbound freight and logistics costs as well as\\nproduct mix;\\nâ€¢Lower margin in our NIKE Direct business, driven by higher promotional activity to liquidate inventory in the current period compared to lower promotional activity in\\nthe prior period resulting from lower available inventory supply;\\nâ€¢Unfavorable changes in net foreign currency exchange rates, including hedges; and\\nâ€¢Lower off-price margin, on a wholesale equivalent basis.\\nThis was partially offset by:\\' metadata={\\'page\\': 36, \\'source\\': \\'../example_data/nke-10k-2023.pdf\\', \\'start_index\\': 0}\\n\\nLearn more:\\n\\nAPI Reference\\nIntegration-specific docs\\n\\n\\u200b4. Retrievers\\nLangChain VectorStore objects do not subclass @[Runnable]. LangChain @[Retrievers] are Runnables, so they implement a standard set of methods (e.g., synchronous and asynchronous invoke and batch operations). Although we can construct retrievers from vector stores, retrievers can interface with non-vector store sources of data, as well (such as external APIs).\\nWe can create a simple version of this ourselves, without subclassing Retriever. If we choose what method we wish to use to retrieve documents, we can create a runnable easily. Below we will build one around the similarity_search method:\\nCopyAsk AIfrom typing import List\\n\\nfrom langchain_core.documents import Document\\nfrom langchain_core.runnables import chain\\n\\n\\n@chain\\ndef retriever(query: str) -> List[Document]:\\n    return vector_store.similarity_search(query, k=1)\\n\\n\\nretriever.batch(\\n    [\\n        \"How many distribution centers does Nike have in the US?\",\\n        \"When was Nike incorporated?\",\\n    ],\\n)\\n\\nCopyAsk AI[[Document(metadata={\\'page\\': 4, \\'source\\': \\'../example_data/nke-10k-2023.pdf\\', \\'start_index\\': 3125}, page_content=\\'direct to consumer operations sell products through the following number of retail stores in the United States:\\\\nU.S. RETAIL STORES NUMBER\\\\nNIKE Brand factory stores 213 \\\\nNIKE Brand in-line stores (including employee-only stores) 74 \\\\nConverse stores (including factory stores) 82 \\\\nTOTAL 369 \\\\nIn the United States, NIKE has eight significant distribution centers. Refer to Item 2. Properties for further information.\\\\n2023 FORM 10-K 2\\')],\\n [Document(metadata={\\'page\\': 3, \\'source\\': \\'../example_data/nke-10k-2023.pdf\\', \\'start_index\\': 0}, page_content=\\'Table of Contents\\\\nPART I\\\\nITEM 1. BUSINESS\\\\nGENERAL\\\\nNIKE, Inc. was incorporated in 1967 under the laws of the State of Oregon. As used in this Annual Report on Form 10-K (this \"Annual Report\"), the terms \"we,\" \"us,\" \"our,\"\\\\n\"NIKE\" and the \"Company\" refer to NIKE, Inc. and its predecessors, subsidiaries and affiliates, collectively, unless the context indicates otherwise.\\\\nOur principal business activity is the design, development and worldwide marketing and selling of athletic footwear, apparel, equipment, accessories and services. NIKE is\\\\nthe largest seller of athletic footwear and apparel in the world. We sell our products through NIKE Direct operations, which are comprised of both NIKE-owned retail stores\\\\nand sales through our digital platforms (also referred to as \"NIKE Brand Digital\"), to retail accounts and to a mix of independent distributors, licensees and sales\\')]]\\n\\nVectorstores implement an as_retriever method that will generate a Retriever, specifically a VectorStoreRetriever. These retrievers include specific search_type and search_kwargs attributes that identify what methods of the underlying vector store to call, and how to parameterize them. For instance, we can replicate the above with the following:\\nCopyAsk AIretriever = vector_store.as_retriever(\\n    search_type=\"similarity\",\\n    search_kwargs={\"k\": 1},\\n)\\n\\nretriever.batch(\\n    [\\n        \"How many distribution centers does Nike have in the US?\",\\n        \"When was Nike incorporated?\",\\n    ],\\n)\\n\\nCopyAsk AI[[Document(metadata={\\'page\\': 4, \\'source\\': \\'../example_data/nke-10k-2023.pdf\\', \\'start_index\\': 3125}, page_content=\\'direct to consumer operations sell products through the following number of retail stores in the United States:\\\\nU.S. RETAIL STORES NUMBER\\\\nNIKE Brand factory stores 213 \\\\nNIKE Brand in-line stores (including employee-only stores) 74 \\\\nConverse stores (including factory stores) 82 \\\\nTOTAL 369 \\\\nIn the United States, NIKE has eight significant distribution centers. Refer to Item 2. Properties for further information.\\\\n2023 FORM 10-K 2\\')],\\n [Document(metadata={\\'page\\': 3, \\'source\\': \\'../example_data/nke-10k-2023.pdf\\', \\'start_index\\': 0}, page_content=\\'Table of Contents\\\\nPART I\\\\nITEM 1. BUSINESS\\\\nGENERAL\\\\nNIKE, Inc. was incorporated in 1967 under the laws of the State of Oregon. As used in this Annual Report on Form 10-K (this \"Annual Report\"), the terms \"we,\" \"us,\" \"our,\"\\\\n\"NIKE\" and the \"Company\" refer to NIKE, Inc. and its predecessors, subsidiaries and affiliates, collectively, unless the context indicates otherwise.\\\\nOur principal business activity is the design, development and worldwide marketing and selling of athletic footwear, apparel, equipment, accessories and services. NIKE is\\\\nthe largest seller of athletic footwear and apparel in the world. We sell our products through NIKE Direct operations, which are comprised of both NIKE-owned retail stores\\\\nand sales through our digital platforms (also referred to as \"NIKE Brand Digital\"), to retail accounts and to a mix of independent distributors, licensees and sales\\')]]\\n\\nVectorStoreRetriever supports search types of \"similarity\" (default), \"mmr\" (maximum marginal relevance, described above), and \"similarity_score_threshold\". We can use the latter to threshold documents output by the retriever by similarity score.\\nRetrievers can easily be incorporated into more complex applications, such as retrieval-augmented generation (RAG) applications that combine a given question with retrieved context into a prompt for a LLM. To learn more about building such an application, check out the RAG tutorial tutorial.\\n\\u200bNext steps\\nYouâ€™ve now seen how to build a semantic search engine over a PDF document.\\nFor more on document loaders:\\n\\nOverview\\nAvailable integrations\\n\\nFor more on embeddings:\\n\\nOverview\\nAvailable integrations\\n\\nFor more on vector stores:\\n\\nOverview\\nAvailable integrations\\n\\nFor more on RAG, see:\\n\\nBuild a Retrieval Augmented Generation (RAG) App\\n\\n\\nEdit the source of this page on GitHub.\\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.Was this page helpful?YesNoLearnPreviousBuild a RAG agent with LangChainNextâŒ˜IDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify')],\n",
       " [Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/sql-agent', 'title': 'Build a custom SQL agent - Docs by LangChain', 'language': 'en'}, page_content='Build a custom SQL agent - Docs by LangChainSkip to main contentDocs by LangChain home pageLangChain + LangGraphSearch...âŒ˜KGitHubTry LangSmithTry LangSmithSearch...NavigationLangGraphBuild a custom SQL agentLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonLearnTutorialsLangChainLangGraphCustom RAG agentCustom SQL agentConceptual overviewsMemoryContextGraph APIFunctional APIAdditional resourcesLangChain AcademyCase studiesOn this pageConceptsSetupInstallationLangSmith1. Select an LLM2. Configure the database3. Add tools for database interactions4. Define application steps5. Implement the agent6. Implement human-in-the-loop reviewNext stepsTutorialsLangGraphBuild a custom SQL agentCopy pageCopy pageIn this tutorial we will build a custom agent that can answer questions about a SQL database using LangGraph.\\nLangChain offers built-in agent implementations, implemented using LangGraph primitives. If deeper customization is required, agents can be implemented directly in LangGraph. This guide demonstrates an example implementation of a SQL agent. You can find a tutorial building a SQL agent using higher-level LangChain abstractions here.\\nBuilding Q&A systems of SQL databases requires executing model-generated SQL queries. There are inherent risks in doing this. Make sure that your database connection permissions are always scoped as narrowly as possible for your agentâ€™s needs. This will mitigate, though not eliminate, the risks of building a model-driven system.\\nThe prebuilt agent lets us get started quickly, but we relied on the system prompt to constrain its behaviorâ€” for example, we instructed the agent to always start with the â€œlist tablesâ€ tool, and to always run a query-checker tool before executing the query.\\nWe can enforce a higher degree of control in LangGraph by customizing the agent. Here, we implement a simple ReAct-agent setup, with dedicated nodes for specific tool-calls. We will use the same [state] as the pre-built agent.\\n\\u200bConcepts\\nWe will cover the following concepts:\\n\\nTools for reading from SQL databases\\nThe LangGraph Graph API, including state, nodes, edges, and conditional edges.\\nHuman-in-the-loop processes\\n\\n\\u200bSetup\\n\\u200bInstallation\\npipCopyAsk AIpip install langchain  langgraph  langchain-community\\n\\n\\u200bLangSmith\\nSet up LangSmith to inspect what is happening inside your chain or agent. Then set the following environment variables:\\nCopyAsk AIexport LANGSMITH_TRACING=\"true\"\\nexport LANGSMITH_API_KEY=\"...\"\\n\\n\\u200b1. Select an LLM\\nSelect a model that supports tool-calling:\\n OpenAI Anthropic Azure Google Gemini AWS BedrockðŸ‘‰ Read the OpenAI chat model integration docsCopyAsk AIpip install -U \"langchain[openai]\"\\ninit_chat_modelModel ClassCopyAsk AIimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\\n\\nmodel = init_chat_model(\"gpt-4.1\")\\n\\nThe output shown in the examples below used OpenAI.\\n\\u200b2. Configure the database\\nYou will be creating a SQLite database for this tutorial. SQLite is a lightweight database that is easy to set up and use. We will be loading the chinook database, which is a sample database that represents a digital media store.\\nFor convenience, we have hosted the database (Chinook.db) on a public GCS bucket.\\nCopyAsk AIimport requests, pathlib\\n\\nurl = \"https://storage.googleapis.com/benchmarks-artifacts/chinook/Chinook.db\"\\nlocal_path = pathlib.Path(\"Chinook.db\")\\n\\nif local_path.exists():\\n    print(f\"{local_path} already exists, skipping download.\")\\nelse:\\n    response = requests.get(url)\\n    if response.status_code == 200:\\n        local_path.write_bytes(response.content)\\n        print(f\"File downloaded and saved as {local_path}\")\\n    else:\\n        print(f\"Failed to download the file. Status code: {response.status_code}\")\\n\\nWe will use a handy SQL database wrapper available in the langchain_community package to interact with the database. The wrapper provides a simple interface to execute SQL queries and fetch results:\\nCopyAsk AIfrom langchain_community.utilities import SQLDatabase\\n\\ndb = SQLDatabase.from_uri(\"sqlite:///Chinook.db\")\\n\\nprint(f\"Dialect: {db.dialect}\")\\nprint(f\"Available tables: {db.get_usable_table_names()}\")\\nprint(f\\'Sample output: {db.run(\"SELECT * FROM Artist LIMIT 5;\")}\\')\\n\\nCopyAsk AIDialect: sqlite\\nAvailable tables: [\\'Album\\', \\'Artist\\', \\'Customer\\', \\'Employee\\', \\'Genre\\', \\'Invoice\\', \\'InvoiceLine\\', \\'MediaType\\', \\'Playlist\\', \\'PlaylistTrack\\', \\'Track\\']\\nSample output: [(1, \\'AC/DC\\'), (2, \\'Accept\\'), (3, \\'Aerosmith\\'), (4, \\'Alanis Morissette\\'), (5, \\'Alice In Chains\\')]\\n\\n\\u200b3. Add tools for database interactions\\nUse the SQLDatabase wrapper available in the langchain_community package to interact with the database. The wrapper provides a simple interface to execute SQL queries and fetch results:\\nCopyAsk AIfrom langchain_community.agent_toolkits import SQLDatabaseToolkit\\n\\ntoolkit = SQLDatabaseToolkit(db=db, llm=llm)\\n\\ntools = toolkit.get_tools()\\n\\nfor tool in tools:\\n    print(f\"{tool.name}: {tool.description}\\\\n\")\\n\\nCopyAsk AIsql_db_query: Input to this tool is a detailed and correct SQL query, output is a result from the database. If the query is not correct, an error message will be returned. If an error is returned, rewrite the query, check the query, and try again. If you encounter an issue with Unknown column \\'xxxx\\' in \\'field list\\', use sql_db_schema to query the correct table fields.\\n\\nsql_db_schema: Input to this tool is a comma-separated list of tables, output is the schema and sample rows for those tables. Be sure that the tables actually exist by calling sql_db_list_tables first! Example Input: table1, table2, table3\\n\\nsql_db_list_tables: Input is an empty string, output is a comma-separated list of tables in the database.\\n\\nsql_db_query_checker: Use this tool to double check if your query is correct before executing it. Always use this tool before executing a query with sql_db_query!\\n\\n\\u200b4. Define application steps\\nWe construct dedicated nodes for the following steps:\\n\\nListing DB tables\\nCalling the â€œget schemaâ€ tool\\nGenerating a query\\nChecking the query\\n\\nPutting these steps in dedicated nodes lets us (1) force tool-calls when needed, and (2) customize the prompts associated with each step.\\nCopyAsk AIfrom typing import Literal\\n\\nfrom langchain.messages import AIMessage\\nfrom langchain_core.runnables import RunnableConfig\\nfrom langgraph.graph import END, START, MessagesState, StateGraph\\nfrom langgraph.prebuilt import ToolNode\\n\\n\\nget_schema_tool = next(tool for tool in tools if tool.name == \"sql_db_schema\")\\nget_schema_node = ToolNode([get_schema_tool], name=\"get_schema\")\\n\\nrun_query_tool = next(tool for tool in tools if tool.name == \"sql_db_query\")\\nrun_query_node = ToolNode([run_query_tool], name=\"run_query\")\\n\\n\\n# Example: create a predetermined tool call\\ndef list_tables(state: MessagesState):\\n    tool_call = {\\n        \"name\": \"sql_db_list_tables\",\\n        \"args\": {},\\n        \"id\": \"abc123\",\\n        \"type\": \"tool_call\",\\n    }\\n    tool_call_message = AIMessage(content=\"\", tool_calls=[tool_call])\\n\\n    list_tables_tool = next(tool for tool in tools if tool.name == \"sql_db_list_tables\")\\n    tool_message = list_tables_tool.invoke(tool_call)\\n    response = AIMessage(f\"Available tables: {tool_message.content}\")\\n\\n    return {\"messages\": [tool_call_message, tool_message, response]}\\n\\n\\n# Example: force a model to create a tool call\\ndef call_get_schema(state: MessagesState):\\n    # Note that LangChain enforces that all models accept `tool_choice=\"any\"`\\n    # as well as `tool_choice=<string name of tool>`.\\n    llm_with_tools = llm.bind_tools([get_schema_tool], tool_choice=\"any\")\\n    response = llm_with_tools.invoke(state[\"messages\"])\\n\\n    return {\"messages\": [response]}\\n\\n\\ngenerate_query_system_prompt = \"\"\"\\nYou are an agent designed to interact with a SQL database.\\nGiven an input question, create a syntactically correct {dialect} query to run,\\nthen look at the results of the query and return the answer. Unless the user\\nspecifies a specific number of examples they wish to obtain, always limit your\\nquery to at most {top_k} results.\\n\\nYou can order the results by a relevant column to return the most interesting\\nexamples in the database. Never query for all the columns from a specific table,\\nonly ask for the relevant columns given the question.\\n\\nDO NOT make any DML statements (INSERT, UPDATE, DELETE, DROP etc.) to the database.\\n\"\"\".format(\\n    dialect=db.dialect,\\n    top_k=5,\\n)\\n\\n\\ndef generate_query(state: MessagesState):\\n    system_message = {\\n        \"role\": \"system\",\\n        \"content\": generate_query_system_prompt,\\n    }\\n    # We do not force a tool call here, to allow the model to\\n    # respond naturally when it obtains the solution.\\n    llm_with_tools = llm.bind_tools([run_query_tool])\\n    response = llm_with_tools.invoke([system_message] + state[\"messages\"])\\n\\n    return {\"messages\": [response]}\\n\\n\\ncheck_query_system_prompt = \"\"\"\\nYou are a SQL expert with a strong attention to detail.\\nDouble check the {dialect} query for common mistakes, including:\\n- Using NOT IN with NULL values\\n- Using UNION when UNION ALL should have been used\\n- Using BETWEEN for exclusive ranges\\n- Data type mismatch in predicates\\n- Properly quoting identifiers\\n- Using the correct number of arguments for functions\\n- Casting to the correct data type\\n- Using the proper columns for joins\\n\\nIf there are any of the above mistakes, rewrite the query. If there are no mistakes,\\njust reproduce the original query.\\n\\nYou will call the appropriate tool to execute the query after running this check.\\n\"\"\".format(dialect=db.dialect)\\n\\n\\ndef check_query(state: MessagesState):\\n    system_message = {\\n        \"role\": \"system\",\\n        \"content\": check_query_system_prompt,\\n    }\\n\\n    # Generate an artificial user message to check\\n    tool_call = state[\"messages\"][-1].tool_calls[0]\\n    user_message = {\"role\": \"user\", \"content\": tool_call[\"args\"][\"query\"]}\\n    llm_with_tools = llm.bind_tools([run_query_tool], tool_choice=\"any\")\\n    response = llm_with_tools.invoke([system_message, user_message])\\n    response.id = state[\"messages\"][-1].id\\n\\n    return {\"messages\": [response]}\\n\\n\\u200b5. Implement the agent\\nWe can now assemble these steps into a workflow using the Graph API. We define a conditional edge at the query generation step that will route to the query checker if a query is generated, or end if there are no tool calls present, such that the LLM has delivered a response to the query.\\nCopyAsk AIdef should_continue(state: MessagesState) -> Literal[END, \"check_query\"]:\\n    messages = state[\"messages\"]\\n    last_message = messages[-1]\\n    if not last_message.tool_calls:\\n        return END\\n    else:\\n        return \"check_query\"\\n\\n\\nbuilder = StateGraph(MessagesState)\\nbuilder.add_node(list_tables)\\nbuilder.add_node(call_get_schema)\\nbuilder.add_node(get_schema_node, \"get_schema\")\\nbuilder.add_node(generate_query)\\nbuilder.add_node(check_query)\\nbuilder.add_node(run_query_node, \"run_query\")\\n\\nbuilder.add_edge(START, \"list_tables\")\\nbuilder.add_edge(\"list_tables\", \"call_get_schema\")\\nbuilder.add_edge(\"call_get_schema\", \"get_schema\")\\nbuilder.add_edge(\"get_schema\", \"generate_query\")\\nbuilder.add_conditional_edges(\\n    \"generate_query\",\\n    should_continue,\\n)\\nbuilder.add_edge(\"check_query\", \"run_query\")\\nbuilder.add_edge(\"run_query\", \"generate_query\")\\n\\nagent = builder.compile()\\n\\nWe visualize the application below:\\nCopyAsk AIfrom IPython.display import Image, display\\nfrom langchain_core.runnables.graph import CurveStyle, MermaidDrawMethod, NodeStyles\\n\\ndisplay(Image(agent.get_graph().draw_mermaid_png()))\\n\\n\\nWe can now invoke the graph:\\nCopyAsk AIquestion = \"Which genre on average has the longest tracks?\"\\n\\nfor step in agent.stream(\\n    {\"messages\": [{\"role\": \"user\", \"content\": question}]},\\n    stream_mode=\"values\",\\n):\\n    step[\"messages\"][-1].pretty_print()\\n\\nCopyAsk AI================================ Human Message =================================\\n\\nWhich genre on average has the longest tracks?\\n================================== Ai Message ==================================\\n\\nAvailable tables: Album, Artist, Customer, Employee, Genre, Invoice, InvoiceLine, MediaType, Playlist, PlaylistTrack, Track\\n================================== Ai Message ==================================\\nTool Calls:\\n  sql_db_schema (call_yzje0tj7JK3TEzDx4QnRR3lL)\\n Call ID: call_yzje0tj7JK3TEzDx4QnRR3lL\\n  Args:\\n    table_names: Genre, Track\\n================================= Tool Message =================================\\nName: sql_db_schema\\n\\n\\nCREATE TABLE \"Genre\" (\\n\\t\"GenreId\" INTEGER NOT NULL,\\n\\t\"Name\" NVARCHAR(120),\\n\\tPRIMARY KEY (\"GenreId\")\\n)\\n\\n/*\\n3 rows from Genre table:\\nGenreId\\tName\\n1\\tRock\\n2\\tJazz\\n3\\tMetal\\n*/\\n\\n\\nCREATE TABLE \"Track\" (\\n\\t\"TrackId\" INTEGER NOT NULL,\\n\\t\"Name\" NVARCHAR(200) NOT NULL,\\n\\t\"AlbumId\" INTEGER,\\n\\t\"MediaTypeId\" INTEGER NOT NULL,\\n\\t\"GenreId\" INTEGER,\\n\\t\"Composer\" NVARCHAR(220),\\n\\t\"Milliseconds\" INTEGER NOT NULL,\\n\\t\"Bytes\" INTEGER,\\n\\t\"UnitPrice\" NUMERIC(10, 2) NOT NULL,\\n\\tPRIMARY KEY (\"TrackId\"),\\n\\tFOREIGN KEY(\"MediaTypeId\") REFERENCES \"MediaType\" (\"MediaTypeId\"),\\n\\tFOREIGN KEY(\"GenreId\") REFERENCES \"Genre\" (\"GenreId\"),\\n\\tFOREIGN KEY(\"AlbumId\") REFERENCES \"Album\" (\"AlbumId\")\\n)\\n\\n/*\\n3 rows from Track table:\\nTrackId\\tName\\tAlbumId\\tMediaTypeId\\tGenreId\\tComposer\\tMilliseconds\\tBytes\\tUnitPrice\\n1\\tFor Those About To Rock (We Salute You)\\t1\\t1\\t1\\tAngus Young, Malcolm Young, Brian Johnson\\t343719\\t11170334\\t0.99\\n2\\tBalls to the Wall\\t2\\t2\\t1\\tU. Dirkschneider, W. Hoffmann, H. Frank, P. Baltes, S. Kaufmann, G. Hoffmann\\t342562\\t5510424\\t0.99\\n3\\tFast As a Shark\\t3\\t2\\t1\\tF. Baltes, S. Kaufman, U. Dirkscneider & W. Hoffman\\t230619\\t3990994\\t0.99\\n*/\\n================================== Ai Message ==================================\\nTool Calls:\\n  sql_db_query (call_cb9ApLfZLSq7CWg6jd0im90b)\\n Call ID: call_cb9ApLfZLSq7CWg6jd0im90b\\n  Args:\\n    query: SELECT Genre.Name, AVG(Track.Milliseconds) AS AvgMilliseconds FROM Track JOIN Genre ON Track.GenreId = Genre.GenreId GROUP BY Genre.GenreId ORDER BY AvgMilliseconds DESC LIMIT 5;\\n================================== Ai Message ==================================\\nTool Calls:\\n  sql_db_query (call_DMVALfnQ4kJsuF3Yl6jxbeAU)\\n Call ID: call_DMVALfnQ4kJsuF3Yl6jxbeAU\\n  Args:\\n    query: SELECT Genre.Name, AVG(Track.Milliseconds) AS AvgMilliseconds FROM Track JOIN Genre ON Track.GenreId = Genre.GenreId GROUP BY Genre.GenreId ORDER BY AvgMilliseconds DESC LIMIT 5;\\n================================= Tool Message =================================\\nName: sql_db_query\\n\\n[(\\'Sci Fi & Fantasy\\', 2911783.0384615385), (\\'Science Fiction\\', 2625549.076923077), (\\'Drama\\', 2575283.78125), (\\'TV Shows\\', 2145041.0215053763), (\\'Comedy\\', 1585263.705882353)]\\n================================== Ai Message ==================================\\n\\nThe genre with the longest tracks on average is \"Sci Fi & Fantasy,\" with an average track length of approximately 2,911,783 milliseconds. Other genres with relatively long tracks include \"Science Fiction,\" \"Drama,\" \"TV Shows,\" and \"Comedy.\"\\n\\nSee LangSmith trace for the above run.\\n\\u200b6. Implement human-in-the-loop review\\nIt can be prudent to check the agentâ€™s SQL queries before they are executed for any unintended actions or inefficiencies.\\nHere we leverage LangGraphâ€™s human-in-the-loop features to pause the run before executing a SQL query and wait for human review. Using LangGraphâ€™s persistence layer, we can pause the run indefinitely (or at least as long as the persistence layer is alive).\\nLetâ€™s wrap the sql_db_query tool in a node that receives human input. We can implement this using the interrupt function. Below, we allow for input to approve the tool call, edit its arguments, or provide user feedback.\\nCopyAsk AIfrom langchain_core.runnables import RunnableConfig\\nfrom langchain.tools import tool\\nfrom langgraph.types import interrupt\\n\\n@tool(\\n    run_query_tool.name,\\n    description=run_query_tool.description,\\n    args_schema=run_query_tool.args_schema\\n)\\ndef run_query_tool_with_interrupt(config: RunnableConfig, **tool_input):\\n    request = {\\n        \"action\": run_query_tool.name,\\n        \"args\": tool_input,\\n        \"description\": \"Please review the tool call\"\\n    }\\n    response = interrupt([request]) \\n    # approve the tool call\\n    if response[\"type\"] == \"accept\":\\n        tool_response = run_query_tool.invoke(tool_input, config)\\n    # update tool call args\\n    elif response[\"type\"] == \"edit\":\\n        tool_input = response[\"args\"][\"args\"]\\n        tool_response = run_query_tool.invoke(tool_input, config)\\n    # respond to the LLM with user feedback\\n    elif response[\"type\"] == \"response\":\\n        user_feedback = response[\"args\"]\\n        tool_response = user_feedback\\n    else:\\n        raise ValueError(f\"Unsupported interrupt response type: {response[\\'type\\']}\")\\n\\n    return tool_response\\n\\nThe above implementation follows the tool interrupt example in the broader human-in-the-loop guide. Refer to that guide for details and alternatives.\\nLetâ€™s now re-assemble our graph. We will replace the programmatic check with human review. Note that we now include a checkpointer; this is required to pause and resume the run.\\nCopyAsk AIfrom langgraph.checkpoint.memory import InMemorySaver\\n\\ndef should_continue(state: MessagesState) -> Literal[END, \"run_query\"]:\\n    messages = state[\"messages\"]\\n    last_message = messages[-1]\\n    if not last_message.tool_calls:\\n        return END\\n    else:\\n        return \"run_query\"\\n\\nbuilder = StateGraph(MessagesState)\\nbuilder.add_node(list_tables)\\nbuilder.add_node(call_get_schema)\\nbuilder.add_node(get_schema_node, \"get_schema\")\\nbuilder.add_node(generate_query)\\nbuilder.add_node(run_query_node, \"run_query\")\\n\\nbuilder.add_edge(START, \"list_tables\")\\nbuilder.add_edge(\"list_tables\", \"call_get_schema\")\\nbuilder.add_edge(\"call_get_schema\", \"get_schema\")\\nbuilder.add_edge(\"get_schema\", \"generate_query\")\\nbuilder.add_conditional_edges(\\n    \"generate_query\",\\n    should_continue,\\n)\\nbuilder.add_edge(\"run_query\", \"generate_query\")\\n\\ncheckpointer = InMemorySaver() \\nagent = builder.compile(checkpointer=checkpointer) \\n\\nWe can invoke the graph as before. This time, execution is interrupted:\\nCopyAsk AIimport json\\n\\nconfig = {\"configurable\": {\"thread_id\": \"1\"}}\\n\\nquestion = \"Which genre on average has the longest tracks?\"\\n\\nfor step in agent.stream(\\n    {\"messages\": [{\"role\": \"user\", \"content\": question}]},\\n    config,\\n    stream_mode=\"values\",\\n):\\n    if \"messages\" in step:\\n        step[\"messages\"][-1].pretty_print()\\n    elif \"__interrupt__\" in step:\\n        action = step[\"__interrupt__\"][0]\\n        print(\"INTERRUPTED:\")\\n        for request in action.value:\\n            print(json.dumps(request, indent=2))\\n    else:\\n        pass\\n\\nCopyAsk AI...\\n\\nINTERRUPTED:\\n{\\n  \"action\": \"sql_db_query\",\\n  \"args\": {\\n    \"query\": \"SELECT Genre.Name, AVG(Track.Milliseconds) AS AvgLength FROM Track JOIN Genre ON Track.GenreId = Genre.GenreId GROUP BY Genre.Name ORDER BY AvgLength DESC LIMIT 5;\"\\n  },\\n  \"description\": \"Please review the tool call\"\\n}\\n\\nWe can accept or edit the tool call using Command:\\nCopyAsk AIfrom langgraph.types import Command\\n\\n\\nfor step in agent.stream(\\n    Command(resume={\"type\": \"accept\"}),\\n    # Command(resume={\"type\": \"edit\", \"args\": {\"query\": \"...\"}}),\\n    config,\\n    stream_mode=\"values\",\\n):\\n    if \"messages\" in step:\\n        step[\"messages\"][-1].pretty_print()\\n    elif \"__interrupt__\" in step:\\n        action = step[\"__interrupt__\"][0]\\n        print(\"INTERRUPTED:\")\\n        for request in action.value:\\n            print(json.dumps(request, indent=2))\\n    else:\\n        pass\\n\\nCopyAsk AI================================== Ai Message ==================================\\nTool Calls:\\n  sql_db_query (call_t4yXkD6shwdTPuelXEmY3sAY)\\n Call ID: call_t4yXkD6shwdTPuelXEmY3sAY\\n  Args:\\n    query: SELECT Genre.Name, AVG(Track.Milliseconds) AS AvgLength FROM Track JOIN Genre ON Track.GenreId = Genre.GenreId GROUP BY Genre.Name ORDER BY AvgLength DESC LIMIT 5;\\n================================= Tool Message =================================\\nName: sql_db_query\\n\\n[(\\'Sci Fi & Fantasy\\', 2911783.0384615385), (\\'Science Fiction\\', 2625549.076923077), (\\'Drama\\', 2575283.78125), (\\'TV Shows\\', 2145041.0215053763), (\\'Comedy\\', 1585263.705882353)]\\n================================== Ai Message ==================================\\n\\nThe genre with the longest average track length is \"Sci Fi & Fantasy\" with an average length of about 2,911,783 milliseconds. Other genres with long average track lengths include \"Science Fiction,\" \"Drama,\" \"TV Shows,\" and \"Comedy.\"\\n\\nRefer to the human-in-the-loop guide for details.\\n\\u200bNext steps\\nCheck out the Evaluate a graph guide for evaluating LangGraph applications, including SQL agents like this one, using LangSmith.\\n\\nEdit the source of this page on GitHub.\\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.Was this page helpful?YesNoBuild a custom RAG agentPreviousMemory overviewNextâŒ˜IDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify')]]"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "langchain_urls = [\n",
    "    \"https://docs.langchain.com/oss/python/langchain/rag\",\n",
    "    \"https://docs.langchain.com/oss/python/langchain/knowledge-base\",\n",
    "    \"https://docs.langchain.com/oss/python/langgraph/sql-agent\"\n",
    "]\n",
    "\n",
    "\n",
    "docs = [WebBaseLoader(url).load() for url in langchain_urls]\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "0e38aea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text Splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500, \n",
    "    chunk_overlap=50,\n",
    "    separators=[\".\", \",\"],\n",
    "    keep_separator=True\n",
    ")\n",
    "docs_list = [doc for sublist in docs for doc in sublist]\n",
    "split_docs = text_splitter.split_documents(docs_list)\n",
    "\n",
    "# Embeddings\n",
    "embeddings = HuggingFaceEmbeddings(model=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Vectorstore\n",
    "vectorstore_langchain = FAISS.from_documents(documents=split_docs, embedding=embeddings)\n",
    "\n",
    "# Retriever\n",
    "retriever_langchain = vectorstore.as_retriever(search_kwargs={'k': 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "a6f2d4b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructuredTool(name='Langchain_Retriever_Tool', description='Use this tool to retrieve relevant documents', args_schema=<class 'langchain_core.utils.pydantic.Langchain_Retriever_Tool'>, func=<function create_retriever_tool at 0x7327f1abee80>)"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Retriever to retriever tools\n",
    "from typing import TypedDict, List\n",
    "\n",
    "class RetrievedDoc(TypedDict):\n",
    "    content: str\n",
    "    source: str\n",
    "    score: float | None\n",
    "\n",
    "\n",
    "def create_langchain_retriever_tool(query:str) -> List[RetrievedDoc]:\n",
    "    \"\"\"Use this tool to retrieve relevant documents\"\"\"\n",
    "    print(\"Calling retriever tool\")\n",
    "    result = retriever.invoke(query)\n",
    "    return [\n",
    "        {\n",
    "            \"content\": d.page_content,\n",
    "            \"source\": d.metadata.get(\"source\"),\n",
    "            \"score\": d.metadata.get(\"score\")\n",
    "        } \n",
    "        for d in result\n",
    "    ]\n",
    "\n",
    "\n",
    "langchain_retriever_tool = StructuredTool.from_function(\n",
    "    name=\"Langchain_Retriever_Tool\",\n",
    "    description=\"Use this tool to retrieve relevant documents\",\n",
    "    func=create_retriever_tool\n",
    ")\n",
    "\n",
    "langchain_retriever_tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "807fabd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [ retriever_tool, langchain_retriever_tool]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0215ff6",
   "metadata": {},
   "source": [
    "### LangGraph Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "bcec1cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, Sequence, TypedDict, Literal\n",
    "from langgraph.graph.message import add_messages\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_classic import hub\n",
    "from langchain_core.messages import BaseMessage, HumanMessage\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    # The add_messages function defines how an update should be processed\n",
    "    # Default is to replace. add_messages says, \"append\"\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "36bb689e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "llm = ChatGroq(model=\"qwen/qwen3-32b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "4c2dca72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    Invokes the agent model to generate a response based on the current state. Given\n",
    "    the question, it will decide to retrieve using the retreiver tool, or simply end.\n",
    "    \n",
    "    Args:\n",
    "        state(messages): The current state\n",
    "    \n",
    "    Returns:\n",
    "        dict: The updated state with the agent response appended to messages\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"-- CALL AGENT ---\")\n",
    "    messages = state[\"messages\"]\n",
    "    model = ChatGroq(model=\"qwen/qwen3-32b\")\n",
    "    llm_with_tools = model.bind_tools(tools)\n",
    "    response = llm_with_tools.invoke(messages)\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "8f9b0290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edge\n",
    "def grade_documents(state: AgentState) -> Literal[\"generate\", \"rewrite\"]:\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question.\n",
    "    \n",
    "    Args:\n",
    "        state (messages): The current state\n",
    "        \n",
    "    Returns:\n",
    "        str: A decision for wheter the documents are relevant or not\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"--- CHECK RELEVANCE ---\")\n",
    "    \n",
    "    # Data Model\n",
    "    class grade(BaseModel):\n",
    "        \"\"\"Binary score for relevance check\"\"\"\n",
    "        binary_score: str = Field(description=\"Relevance score 'yes' or 'no'\")\n",
    "    \n",
    "    # LLM \n",
    "    model = ChatGroq(model=\"qwen/qwen3-32b\")\n",
    "    \n",
    "    # LLM with tools\n",
    "    llm_with_tool = model.with_structured_output(grade)\n",
    "    \n",
    "    # Prompt\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"\n",
    "            You are a grader assessing relevance of a retrieved document to a user question.\\n\n",
    "            Here is the retrieved document: \\n\\n {context} \\n\\n\n",
    "            Here is the user question: {question} \\n\n",
    "            If the document containes keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n",
    "            Give a binary score 'yes' or 'no' score to indicate wheter the document is relevant tto the question.\n",
    "        \"\"\",\n",
    "        input_variables=[\"context\", \"question\"]\n",
    "    )\n",
    "    \n",
    "    # Chain\n",
    "    chain = prompt | llm_with_tool\n",
    "    \n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    \n",
    "    # First message -> Question\n",
    "    question = messages[0].content\n",
    "    \n",
    "    # Last message -> retrieved document\n",
    "    docs = last_message\n",
    "    \n",
    "    scored_result = chain.invoke({'question': question, \"context\": docs})\n",
    "    score = scored_result.binary_score\n",
    "    \n",
    "    if score == \"yes\":\n",
    "        print(\"---DECISION: DOCS RELEVANT TO QUESTION---\")\n",
    "        return \"generate\"\n",
    "    else:\n",
    "        print(\"---DECISION: DOCS NOT RELEVANT---\")\n",
    "        print(score)\n",
    "        return \"rewrite\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "b08d70d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(state: AgentState) -> AgentState:\n",
    "    \"\"\" \n",
    "    Generate Answer\n",
    "    \n",
    "    Args:\n",
    "        state (messages): The current state\n",
    "        \n",
    "    Returns:\n",
    "        state (messages): The updated state\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"---GENERATE ANSWER---\")\n",
    "    messages = state[\"messages\"]\n",
    "    question = messages[0].content\n",
    "    last_message = messages[-1]\n",
    "    \n",
    "    docs = last_message.content\n",
    "    \n",
    "    # Prompt for answer generation\n",
    "    prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "    \n",
    "    # LLM\n",
    "    llm = ChatGroq(model=\"qwen/qwen3-32b\")\n",
    "    \n",
    "    # Post processing\n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "    \n",
    "    # Chain\n",
    "    rag_chain = prompt | llm | StrOutputParser()\n",
    "    \n",
    "    # Run\n",
    "    response = rag_chain.invoke({\"context\": docs, \"question\": question})\n",
    "    \n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "574dcdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite(state: AgentState) -> AgentState:\n",
    "    \"\"\" \n",
    "    Transform the query to produce a better question.\n",
    "    \n",
    "    Args:\n",
    "        state (messages): The current state\n",
    "\n",
    "    Returns:\n",
    "       messages (list): The updateddicst: The updated state with re-phrased question\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"---TRANSFORM QUERY---\")\n",
    "    messages = state['messages']\n",
    "    question = messages[0].content\n",
    "    \n",
    "    msg = [\n",
    "        HumanMessage(content=f\"\"\"\n",
    "                Look a the input and try to reason about the underlying semantic intent / meaning.\\n\n",
    "                Here is the initial question:\n",
    "                \\n-----\\n \n",
    "                {question}\n",
    "                \\n-----\\n \n",
    "                Formulate an improved question: \n",
    "            \"\"\"\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # Grader\n",
    "    llm = ChatGroq(model=\"qwen/qwen3-32b\")\n",
    "    response = llm.invoke(msg)\n",
    "    return {\"messages\": [response]}    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "ef9b3a07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from IPython.display import display, Image\n",
    "\n",
    "# define a new graph\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# Define the nodes we will cycle between\n",
    "workflow.add_node(\"agent\", agent)\n",
    "retrieve = ToolNode([langchain_retriever_tool, retriever_tool])\n",
    "workflow.add_node(\"retrieve\", retrieve)\n",
    "workflow.add_node(\"rewrite\", rewrite)\n",
    "workflow.add_node(\"generate\", generate) \n",
    "\n",
    "# Edges\n",
    "workflow.add_edge(START, \"agent\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    tools_condition,\n",
    "    {\n",
    "        # Translate the condition outputs to nodes in our graph\n",
    "        \"tools\": \"retrieve\",\n",
    "        END: END\n",
    "    }        \n",
    ")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"retrieve\",\n",
    "    grade_documents,    \n",
    ")\n",
    "\n",
    "workflow.add_edge(\"generate\", END)\n",
    "workflow.add_edge(\"rewrite\", \"agent\")\n",
    "\n",
    "graph = workflow.compile()\n",
    "\n",
    "display(Image(graph.get_graph(xray=True).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "37b0b560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- CALL AGENT ---\n",
      "Calling retriever tool\n",
      "--- CHECK RELEVANCE ---\n",
      "---DECISION: DOCS RELEVANT TO QUESTION---\n",
      "---GENERATE ANSWER---\n",
      "<think>\n",
      "Okay, the user is asking \"What ist langgraph?\" which seems like a typo for \"What is LangGraph?\" First, I need to check the provided context.\n",
      "\n",
      "Looking through the context, there's a mention that LangGraph is a low-level orchestration framework and runtime for building, managing, and deploying long-running, stateful agents. It's part of LangChain and is used for agent orchestration. Also, it's inspired by Pregel and Apache Beam, and it has built-in visualization tools. The context also notes that it's used by companies like Klarna and Replit.\n",
      "\n",
      "I need to make sure I capture the main points: it's a framework for agents, part of LangChain, low-level, and has some inspirations from other systems. The user might be looking for a concise definition and key features. Since the answer should be three sentences max, I'll summarize the purpose, key features, and maybe the inspiration or usage.\n",
      "</think>\n",
      "\n",
      "LangGraph is a low-level orchestration framework and runtime for building, managing, and deploying long-running, stateful agents, part of the LangChain ecosystem. It focuses on agent orchestration, offering tools for visualization and integration with models and tools. Inspired by systems like Pregel and Apache Beam, it enables complex agent workflows without requiring LangChain itself.\n"
     ]
    }
   ],
   "source": [
    "result = graph.invoke({\"messages\":[HumanMessage(content=\"What ist langgraph?\")]})\n",
    "print(result['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83aa2036",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
