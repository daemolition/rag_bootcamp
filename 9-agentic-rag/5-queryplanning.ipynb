{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccdf025d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader, WebBaseLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_core.tools import Tool, tool, StructuredTool\n",
    "from langchain_core.documents import Document\n",
    "from langchain.agents import create_agent\n",
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "from langgraph.graph import StateGraph, END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e3ee128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document Preprocessing\n",
    "\n",
    "urls = [\n",
    "    'https://lilianweng.github.io/posts/2023-06-23-agent/',\n",
    "    'https://lilianweng.github.io/posts/2024-04-12-diffusion-video/'\n",
    "]\n",
    "\n",
    "loaders = [WebBaseLoader(url) for url in urls]\n",
    "\n",
    "docs = []\n",
    "\n",
    "for loader in loaders:\n",
    "    docs.extend(loader.load())\n",
    "    \n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "chunks = splitter.split_documents(docs)\n",
    "\n",
    "embedding = HuggingFaceEmbeddings(model=\"all-MiniLM-L6-v2\")\n",
    "vectorstore = FAISS.from_documents(chunks, embedding)\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "llm = ChatOllama(model=\"qwen3:4b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cb87aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGState(BaseModel):\n",
    "    question: str\n",
    "    sub_questions: List[str] = []\n",
    "    retrieved_docs: List[Document] = []\n",
    "    answer: str = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "784ef335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query Planner\n",
    "def plan_query(state: RAGState) -> RAGState:\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "        Break te foillowing complex question into 2-3 sub-questions:\n",
    "        \n",
    "        Question: {state.question}\n",
    "        \n",
    "        Sub-questions:\n",
    "    \"\"\"\n",
    "    \n",
    "    result = llm.invoke(prompt)\n",
    "    sub_question = [line.strip(\"- \").strip() for line in result.content.strip().split(\"\\n\") if line.strip()]\n",
    "    return RAGState(\n",
    "        question=state.question,\n",
    "        sub_questions=sub_question\n",
    "    )\n",
    "    \n",
    "# Retrieve Document for each sub question\n",
    "def retrieve_for_each(state: RAGState) -> RAGState:\n",
    "    all_docs = []\n",
    "    \n",
    "    for sub in state.sub_questions:\n",
    "        docs = retriever.invoke(sub)\n",
    "        all_docs.extend(docs)\n",
    "        \n",
    "    return RAGState(\n",
    "        question=state.question,\n",
    "        sub_questions=state.sub_questions,\n",
    "        retrieved_docs=all_docs\n",
    "    )\n",
    "    \n",
    "# Generate final answer\n",
    "def generate_final_answer(state: RAGState) -> RAGState:\n",
    "    context = \"\\n\\n\".join([docs.page_content for docs in state.retrieved_docs])\n",
    "    prompt = f\"\"\" \n",
    "        Use the context below to answer the question.\n",
    "        \n",
    "        Context:\n",
    "        {context}\n",
    "        \n",
    "        Question:\n",
    "        {state.question}        \n",
    "    \"\"\"\n",
    "    \n",
    "    answer = llm.invoke(prompt).content\n",
    "    \n",
    "    return RAGState(\n",
    "        question=state.question,\n",
    "        sub_questions=state.sub_questions,\n",
    "        retrieved_docs=state.retrieved_docs,\n",
    "        answer=answer\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e90c6db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Graph\n",
    "builder = StateGraph(RAGState)\n",
    "\n",
    "builder.add_node(\"planner\", plan_query)\n",
    "builder.add_node(\"retriever\", retrieve_for_each)\n",
    "builder.add_node(\"answer\", generate_final_answer)\n",
    "\n",
    "builder.set_entry_point(\"planner\")\n",
    "builder.add_edge(\"planner\", \"retriever\")\n",
    "builder.add_edge(\"retriever\", \"answer\")\n",
    "builder.add_edge(\"answer\", END)\n",
    "\n",
    "graph = builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "946a426b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'Explain how agent loops work and what are the challenges in diffusion video generation?', 'sub_questions': ['Here\\'s a precise breakdown of your complex question into **3 targeted sub-questions** that maintain technical accuracy while resolving potential terminology confusion (note: \"agent loops\" is **not standard terminology** in diffusion video generation‚Äîthis is addressed upfront to avoid misunderstanding):', '', '### üîç Sub-Questions:', '1. **Clarify terminology**: *What specific concept does \"agent loops\" refer to in the context of diffusion video generation?*', '*(Why? \"Agent loops\" isn\\'t a standard term in diffusion models. This sub-question resolves confusion‚Äîe.g., it likely means \"training loops,\" \"temporal conditioning loops,\" or \"agent-based control loops\" in generative AI systems.)*', '2. **Core mechanism**: *How do diffusion models generate video sequences step-by-step (e.g., from latent space to frames)?*', '*(Why? This directly answers \"how\" for video diffusion‚Äîcovering temporal dynamics, noise scheduling, and frame prediction without conflating unrelated concepts.)*', '3. **Key challenges**: *What are the most critical technical hurdles in training/stable deployment of diffusion video models (e.g., computational cost, temporal coherence)?*', '*(Why? Focuses on *real-world* video-specific challenges like long sequences, motion artifacts, and scalability‚Äîavoiding generic diffusion challenges.)*', '', '### üí° Why this works:', '| Original Question Part          | Why this sub-question is better                                                                 |', '|---------------------------------|----------------------------------------------------------------------------------------------|', '| \"Explain how agent loops work\" | Starts by **fixing terminology** (critical‚Äîdiffusion video uses *no* \"agent loops\" as a standard concept). Prevents misalignment. |', '| \"What are the challenges...\"   | Targets **video-specific** challenges (e.g., temporal consistency vs. image challenges). Avoids tangents like \"text-to-video\" or \"audio\" issues. |', '| **Overall**                     | Ensures sub-questions are **answerable**, **non-overlapping**, and **practically useful** for research/implementation. |', '> ‚úÖ **Key insight**: In diffusion video generation (e.g., models like **Stable Diffusion for video**, **Video Diffusion Models**), the term \"agent loops\" is almost certainly a misstatement. The field uses **temporal diffusion loops** (for frame-by-frame generation) or **training loops** (for optimization). This breakdown avoids spreading misinformation while staying technically grounded.', \"This structure helps you systematically address the question without ambiguity‚Äîperfect for academic, engineering, or self-study contexts. Let me know if you'd like deeper dives into any sub-question!\"], 'retrieved_docs': [Document(id='04d048c8-4c22-47a5-a8cc-b3fcffb604ff', metadata={'source': 'https://lilianweng.github.io/posts/2024-04-12-diffusion-video/', 'title': \"Diffusion Models for Video Generation | Lil'Log\", 'description': 'Diffusion models have demonstrated strong results on image synthesis in past years. Now the research community has started working on a harder task‚Äîusing it for video generation. The task itself is a superset of the image case, since an image is a video of 1 frame, and it is much more challenging because:\\n\\nIt has extra requirements on temporal consistency across frames in time, which naturally demands more world knowledge to be encoded into the model.\\nIn comparison to text or images, it is more difficult to collect large amounts of high-quality, high-dimensional video data, let along text-video pairs.\\n\\n\\n\\nü•ë Required Pre-read: Please make sure you have read the previous blog on ‚ÄúWhat are Diffusion Models?‚Äù for image generation before continue here.\\n', 'language': 'en'}, page_content='The overview of ControlVideo. (Image source: Zhang et al. 2023)\\n\\nCitation#\\nCited as:\\n\\nWeng, Lilian. (Apr 2024). Diffusion Models Video Generation. Lil‚ÄôLog. https://lilianweng.github.io/posts/2024-04-12-diffusion-video/.'), Document(id='f4c8d96c-aa73-40bd-9bcf-53d3dd820856', metadata={'source': 'https://lilianweng.github.io/posts/2024-04-12-diffusion-video/', 'title': \"Diffusion Models for Video Generation | Lil'Log\", 'description': 'Diffusion models have demonstrated strong results on image synthesis in past years. Now the research community has started working on a harder task‚Äîusing it for video generation. The task itself is a superset of the image case, since an image is a video of 1 frame, and it is much more challenging because:\\n\\nIt has extra requirements on temporal consistency across frames in time, which naturally demands more world knowledge to be encoded into the model.\\nIn comparison to text or images, it is more difficult to collect large amounts of high-quality, high-dimensional video data, let along text-video pairs.\\n\\n\\n\\nü•ë Required Pre-read: Please make sure you have read the previous blog on ‚ÄúWhat are Diffusion Models?‚Äù for image generation before continue here.\\n', 'language': 'en'}, page_content='[12] Esser et al. 2023 ‚ÄúStructure and Content-Guided Video Synthesis with Diffusion Models.‚Äù\\n[13] Bar-Tal et al. 2024 ‚ÄúLumiere: A Space-Time Diffusion Model for Video Generation.‚Äù'), Document(id='f5a277ca-50bd-4c85-bc6d-7f7fdc6fab08', metadata={'source': 'https://lilianweng.github.io/posts/2024-04-12-diffusion-video/', 'title': \"Diffusion Models for Video Generation | Lil'Log\", 'description': 'Diffusion models have demonstrated strong results on image synthesis in past years. Now the research community has started working on a harder task‚Äîusing it for video generation. The task itself is a superset of the image case, since an image is a video of 1 frame, and it is much more challenging because:\\n\\nIt has extra requirements on temporal consistency across frames in time, which naturally demands more world knowledge to be encoded into the model.\\nIn comparison to text or images, it is more difficult to collect large amounts of high-quality, high-dimensional video data, let along text-video pairs.\\n\\n\\n\\nü•ë Required Pre-read: Please make sure you have read the previous blog on ‚ÄúWhat are Diffusion Models?‚Äù for image generation before continue here.\\n', 'language': 'en'}, page_content=\"Diffusion Models for Video Generation | Lil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n|\\n\\n\\n\\n\\n\\n\\nPosts\\n\\n\\n\\n\\nArchive\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\nTags\\n\\n\\n\\n\\nFAQ\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      Diffusion Models for Video Generation\\n    \\nDate: April 12, 2024  |  Estimated Reading Time: 20 min  |  Author: Lilian Weng\\n\\n\\n \\n\\n\\nTable of Contents\\n\\n\\n\\nVideo Generation Modeling from Scratch\\n\\nParameterization & Sampling Basics\\n\\nModel Architecture: 3D U-Net & DiT\\n\\n\\nAdapting Image Models to Generate Videos\"), Document(id='110465b2-8c36-4609-8875-bebab7015a00', metadata={'source': 'https://lilianweng.github.io/posts/2024-04-12-diffusion-video/', 'title': \"Diffusion Models for Video Generation | Lil'Log\", 'description': 'Diffusion models have demonstrated strong results on image synthesis in past years. Now the research community has started working on a harder task‚Äîusing it for video generation. The task itself is a superset of the image case, since an image is a video of 1 frame, and it is much more challenging because:\\n\\nIt has extra requirements on temporal consistency across frames in time, which naturally demands more world knowledge to be encoded into the model.\\nIn comparison to text or images, it is more difficult to collect large amounts of high-quality, high-dimensional video data, let along text-video pairs.\\n\\n\\n\\nü•ë Required Pre-read: Please make sure you have read the previous blog on ‚ÄúWhat are Diffusion Models?‚Äù for image generation before continue here.\\n', 'language': 'en'}, page_content='Imagen Video (Ho, et al. 2022) is constructed on a cascade of diffusion models to enhance the video generation quality and upgrades to output 1280x768 videos at 24 fps. The Imagen Video architecture consists of the following components, counting 7 diffusion models in total.'), Document(id='285d208b-8036-4522-9cea-64d6714f511f', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent‚Äôs brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\t\\n\\tOverview of a LLM-powered autonomous agent system.\\n\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en'}, page_content='Thought: ...\\nAction: ...\\nObservation: ...\\n... (Repeated many times)'), Document(id='cb0f736e-f03d-4913-b961-f6368ae43fd5', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent‚Äôs brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\t\\n\\tOverview of a LLM-powered autonomous agent system.\\n\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en'}, page_content='Prompt LM with 100 most recent observations and to generate 3 most salient high-level questions given a set of observations/statements. Then ask LM to answer those questions.\\n\\n\\nPlanning & Reacting: translate the reflections and the environment information into actions'), Document(id='12a9beaf-e517-41b6-9c88-2a7551e41ea2', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent‚Äôs brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\t\\n\\tOverview of a LLM-powered autonomous agent system.\\n\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en'}, page_content='14. Get Improved Code: \"improve_code\", args: \"suggestions\": \"<list_of_suggestions>\", \"code\": \"<full_code_string>\"\\n15. Write Tests: \"write_tests\", args: \"code\": \"<full_code_string>\", \"focus\": \"<list_of_focus_areas>\"\\n16. Execute Python File: \"execute_python_file\", args: \"file\": \"<file>\"\\n17. Generate Image: \"generate_image\", args: \"prompt\": \"<prompt>\"\\n18. Send Tweet: \"send_tweet\", args: \"text\": \"<text>\"\\n19. Do Nothing: \"do_nothing\", args:'), Document(id='d8e79297-e530-4b75-917e-1bb6dba9d938', metadata={'source': 'https://lilianweng.github.io/posts/2024-04-12-diffusion-video/', 'title': \"Diffusion Models for Video Generation | Lil'Log\", 'description': 'Diffusion models have demonstrated strong results on image synthesis in past years. Now the research community has started working on a harder task‚Äîusing it for video generation. The task itself is a superset of the image case, since an image is a video of 1 frame, and it is much more challenging because:\\n\\nIt has extra requirements on temporal consistency across frames in time, which naturally demands more world knowledge to be encoded into the model.\\nIn comparison to text or images, it is more difficult to collect large amounts of high-quality, high-dimensional video data, let along text-video pairs.\\n\\n\\n\\nü•ë Required Pre-read: Please make sure you have read the previous blog on ‚ÄúWhat are Diffusion Models?‚Äù for image generation before continue here.\\n', 'language': 'en'}, page_content='STUNet inflates a pretrained text-to-image U-net to be able to downsample and upsample videos at both time and space dimensions. Convo-based blocks consist of pre-trained text-to-image layers, followed by a factorized space-time convolution. And attention-based blocks at the coarsest U-Net level contains the pre-trained text-to-image, followed by temporal attention. Further training only happens with the newly added layers.'), Document(id='d792d255-9bec-4312-a71d-3d3357260f41', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent‚Äôs brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\t\\n\\tOverview of a LLM-powered autonomous agent system.\\n\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en'}, page_content='\"content\": \"{{There are 10 levels in total. The main character is a plumber named Mario, who can walk and jump. It is a classical platform game just like Super Mario. The main character moves from left to right, trying to get to the destination, where there are many obstacles and attacks from enemies in the process.}}\\\\n\\\\nIs anything else unclear? If yes, only answer in the form:\\\\n{remaining unclear areas} remaining questions.\\\\n{Next question}\\\\nIf everything is sufficiently clear, only'), Document(id='cb0f736e-f03d-4913-b961-f6368ae43fd5', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent‚Äôs brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\t\\n\\tOverview of a LLM-powered autonomous agent system.\\n\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en'}, page_content='Prompt LM with 100 most recent observations and to generate 3 most salient high-level questions given a set of observations/statements. Then ask LM to answer those questions.\\n\\n\\nPlanning & Reacting: translate the reflections and the environment information into actions'), Document(id='339bf365-ffef-41c9-bd01-d0c06d168241', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent‚Äôs brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\t\\n\\tOverview of a LLM-powered autonomous agent system.\\n\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en'}, page_content='Level-1 evaluates the ability to call the API. Given an API‚Äôs description, the model needs to determine whether to call a given API, call it correctly, and respond properly to API returns.\\nLevel-2 examines the ability to retrieve the API. The model needs to search for possible APIs that may solve the user‚Äôs requirement and learn how to use them by reading documentation.'), Document(id='348c2005-966d-482d-b1dd-b8d2e85a5f19', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent‚Äôs brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\t\\n\\tOverview of a LLM-powered autonomous agent system.\\n\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en'}, page_content='}\\n]\\nChallenges#\\nAfter going through key ideas and demos of building LLM-centered agents, I start to see a couple common limitations:'), Document(id='04d048c8-4c22-47a5-a8cc-b3fcffb604ff', metadata={'source': 'https://lilianweng.github.io/posts/2024-04-12-diffusion-video/', 'title': \"Diffusion Models for Video Generation | Lil'Log\", 'description': 'Diffusion models have demonstrated strong results on image synthesis in past years. Now the research community has started working on a harder task‚Äîusing it for video generation. The task itself is a superset of the image case, since an image is a video of 1 frame, and it is much more challenging because:\\n\\nIt has extra requirements on temporal consistency across frames in time, which naturally demands more world knowledge to be encoded into the model.\\nIn comparison to text or images, it is more difficult to collect large amounts of high-quality, high-dimensional video data, let along text-video pairs.\\n\\n\\n\\nü•ë Required Pre-read: Please make sure you have read the previous blog on ‚ÄúWhat are Diffusion Models?‚Äù for image generation before continue here.\\n', 'language': 'en'}, page_content='The overview of ControlVideo. (Image source: Zhang et al. 2023)\\n\\nCitation#\\nCited as:\\n\\nWeng, Lilian. (Apr 2024). Diffusion Models Video Generation. Lil‚ÄôLog. https://lilianweng.github.io/posts/2024-04-12-diffusion-video/.'), Document(id='f4c8d96c-aa73-40bd-9bcf-53d3dd820856', metadata={'source': 'https://lilianweng.github.io/posts/2024-04-12-diffusion-video/', 'title': \"Diffusion Models for Video Generation | Lil'Log\", 'description': 'Diffusion models have demonstrated strong results on image synthesis in past years. Now the research community has started working on a harder task‚Äîusing it for video generation. The task itself is a superset of the image case, since an image is a video of 1 frame, and it is much more challenging because:\\n\\nIt has extra requirements on temporal consistency across frames in time, which naturally demands more world knowledge to be encoded into the model.\\nIn comparison to text or images, it is more difficult to collect large amounts of high-quality, high-dimensional video data, let along text-video pairs.\\n\\n\\n\\nü•ë Required Pre-read: Please make sure you have read the previous blog on ‚ÄúWhat are Diffusion Models?‚Äù for image generation before continue here.\\n', 'language': 'en'}, page_content='[12] Esser et al. 2023 ‚ÄúStructure and Content-Guided Video Synthesis with Diffusion Models.‚Äù\\n[13] Bar-Tal et al. 2024 ‚ÄúLumiere: A Space-Time Diffusion Model for Video Generation.‚Äù'), Document(id='54bd4d74-b66f-406a-b578-4b040fb73a41', metadata={'source': 'https://lilianweng.github.io/posts/2024-04-12-diffusion-video/', 'title': \"Diffusion Models for Video Generation | Lil'Log\", 'description': 'Diffusion models have demonstrated strong results on image synthesis in past years. Now the research community has started working on a harder task‚Äîusing it for video generation. The task itself is a superset of the image case, since an image is a video of 1 frame, and it is much more challenging because:\\n\\nIt has extra requirements on temporal consistency across frames in time, which naturally demands more world knowledge to be encoded into the model.\\nIn comparison to text or images, it is more difficult to collect large amounts of high-quality, high-dimensional video data, let along text-video pairs.\\n\\n\\n\\nü•ë Required Pre-read: Please make sure you have read the previous blog on ‚ÄúWhat are Diffusion Models?‚Äù for image generation before continue here.\\n', 'language': 'en'}, page_content='[3] Bar-Tal et al. 2024 ‚ÄúLumiere: A Space-Time Diffusion Model for Video Generation.‚Äù\\n[4] Brooks et al. ‚ÄúVideo generation models as world simulators.‚Äù OpenAI Blog, 2024.\\n[5] Zhang et al. 2023 ‚ÄúControlVideo: Training-free Controllable Text-to-Video Generation.‚Äù\\n[6] Khachatryan et al. 2023 ‚ÄúText2Video-Zero: Text-to-image diffusion models are zero-shot video generators.‚Äù\\n[7] Ho, et al. 2022 ‚ÄúImagen Video: High Definition Video Generation with Diffusion Models.‚Äù'), Document(id='f5a277ca-50bd-4c85-bc6d-7f7fdc6fab08', metadata={'source': 'https://lilianweng.github.io/posts/2024-04-12-diffusion-video/', 'title': \"Diffusion Models for Video Generation | Lil'Log\", 'description': 'Diffusion models have demonstrated strong results on image synthesis in past years. Now the research community has started working on a harder task‚Äîusing it for video generation. The task itself is a superset of the image case, since an image is a video of 1 frame, and it is much more challenging because:\\n\\nIt has extra requirements on temporal consistency across frames in time, which naturally demands more world knowledge to be encoded into the model.\\nIn comparison to text or images, it is more difficult to collect large amounts of high-quality, high-dimensional video data, let along text-video pairs.\\n\\n\\n\\nü•ë Required Pre-read: Please make sure you have read the previous blog on ‚ÄúWhat are Diffusion Models?‚Äù for image generation before continue here.\\n', 'language': 'en'}, page_content=\"Diffusion Models for Video Generation | Lil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n|\\n\\n\\n\\n\\n\\n\\nPosts\\n\\n\\n\\n\\nArchive\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\nTags\\n\\n\\n\\n\\nFAQ\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      Diffusion Models for Video Generation\\n    \\nDate: April 12, 2024  |  Estimated Reading Time: 20 min  |  Author: Lilian Weng\\n\\n\\n \\n\\n\\nTable of Contents\\n\\n\\n\\nVideo Generation Modeling from Scratch\\n\\nParameterization & Sampling Basics\\n\\nModel Architecture: 3D U-Net & DiT\\n\\n\\nAdapting Image Models to Generate Videos\"), Document(id='6a5819cf-629d-4451-8bc3-d7632df91f43', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent‚Äôs brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\t\\n\\tOverview of a LLM-powered autonomous agent system.\\n\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en'}, page_content='The idea of CoH is to present a history of sequentially improved outputs  in context and train the model to take on the trend to produce better outputs. Algorithm Distillation (AD; Laskin et al. 2023) applies the same idea to cross-episode trajectories in reinforcement learning tasks, where an algorithm is encapsulated in a long history-conditioned policy. Considering that an agent interacts with the environment many times and in each episode the agent gets a little better, AD concatenates this'), Document(id='8de7fff4-e278-484e-832a-f7cd8aba4417', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent‚Äôs brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\t\\n\\tOverview of a LLM-powered autonomous agent system.\\n\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en'}, page_content='agent gets a little better, AD concatenates this learning history and feeds that into the model. Hence we should expect the next predicted action to lead to better performance than previous trials. The goal is to learn the process of RL instead of training a task-specific policy itself.'), Document(id='e025ee85-a95e-447c-8361-866618cd361f', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent‚Äôs brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\t\\n\\tOverview of a LLM-powered autonomous agent system.\\n\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en'}, page_content=\"Planning is essentially in order to optimize believability at the moment vs in time.\\nPrompt template: {Intro of an agent X}. Here is X's plan today in broad strokes: 1)\\nRelationships between agents and observations of one agent by another are all taken into consideration for planning and reacting.\\nEnvironment information is present in a tree structure.\\n\\n\\n\\n\\n\\nThe generative agent architecture. (Image source: Park et al. 2023)\"), Document(id='75e6c761-311d-4e89-9115-63d356b8eae8', metadata={'source': 'https://lilianweng.github.io/posts/2024-04-12-diffusion-video/', 'title': \"Diffusion Models for Video Generation | Lil'Log\", 'description': 'Diffusion models have demonstrated strong results on image synthesis in past years. Now the research community has started working on a harder task‚Äîusing it for video generation. The task itself is a superset of the image case, since an image is a video of 1 frame, and it is much more challenging because:\\n\\nIt has extra requirements on temporal consistency across frames in time, which naturally demands more world knowledge to be encoded into the model.\\nIn comparison to text or images, it is more difficult to collect large amounts of high-quality, high-dimensional video data, let along text-video pairs.\\n\\n\\n\\nü•ë Required Pre-read: Please make sure you have read the previous blog on ‚ÄúWhat are Diffusion Models?‚Äù for image generation before continue here.\\n', 'language': 'en'}, page_content='The training pipeline of autoencoder in video latent diffusion models. The decoder is fine-tuned to have temporal coherency with a new across-frame discriminator while the encoder stays frozen. (Image source: Blattmann et al. 2023)'), Document(id='f4c8d96c-aa73-40bd-9bcf-53d3dd820856', metadata={'source': 'https://lilianweng.github.io/posts/2024-04-12-diffusion-video/', 'title': \"Diffusion Models for Video Generation | Lil'Log\", 'description': 'Diffusion models have demonstrated strong results on image synthesis in past years. Now the research community has started working on a harder task‚Äîusing it for video generation. The task itself is a superset of the image case, since an image is a video of 1 frame, and it is much more challenging because:\\n\\nIt has extra requirements on temporal consistency across frames in time, which naturally demands more world knowledge to be encoded into the model.\\nIn comparison to text or images, it is more difficult to collect large amounts of high-quality, high-dimensional video data, let along text-video pairs.\\n\\n\\n\\nü•ë Required Pre-read: Please make sure you have read the previous blog on ‚ÄúWhat are Diffusion Models?‚Äù for image generation before continue here.\\n', 'language': 'en'}, page_content='[12] Esser et al. 2023 ‚ÄúStructure and Content-Guided Video Synthesis with Diffusion Models.‚Äù\\n[13] Bar-Tal et al. 2024 ‚ÄúLumiere: A Space-Time Diffusion Model for Video Generation.‚Äù'), Document(id='04d048c8-4c22-47a5-a8cc-b3fcffb604ff', metadata={'source': 'https://lilianweng.github.io/posts/2024-04-12-diffusion-video/', 'title': \"Diffusion Models for Video Generation | Lil'Log\", 'description': 'Diffusion models have demonstrated strong results on image synthesis in past years. Now the research community has started working on a harder task‚Äîusing it for video generation. The task itself is a superset of the image case, since an image is a video of 1 frame, and it is much more challenging because:\\n\\nIt has extra requirements on temporal consistency across frames in time, which naturally demands more world knowledge to be encoded into the model.\\nIn comparison to text or images, it is more difficult to collect large amounts of high-quality, high-dimensional video data, let along text-video pairs.\\n\\n\\n\\nü•ë Required Pre-read: Please make sure you have read the previous blog on ‚ÄúWhat are Diffusion Models?‚Äù for image generation before continue here.\\n', 'language': 'en'}, page_content='The overview of ControlVideo. (Image source: Zhang et al. 2023)\\n\\nCitation#\\nCited as:\\n\\nWeng, Lilian. (Apr 2024). Diffusion Models Video Generation. Lil‚ÄôLog. https://lilianweng.github.io/posts/2024-04-12-diffusion-video/.'), Document(id='6526597a-bce7-447a-a83b-e9d6a61b0ea1', metadata={'source': 'https://lilianweng.github.io/posts/2024-04-12-diffusion-video/', 'title': \"Diffusion Models for Video Generation | Lil'Log\", 'description': 'Diffusion models have demonstrated strong results on image synthesis in past years. Now the research community has started working on a harder task‚Äîusing it for video generation. The task itself is a superset of the image case, since an image is a video of 1 frame, and it is much more challenging because:\\n\\nIt has extra requirements on temporal consistency across frames in time, which naturally demands more world knowledge to be encoded into the model.\\nIn comparison to text or images, it is more difficult to collect large amounts of high-quality, high-dimensional video data, let along text-video pairs.\\n\\n\\n\\nü•ë Required Pre-read: Please make sure you have read the previous blog on ‚ÄúWhat are Diffusion Models?‚Äù for image generation before continue here.\\n', 'language': 'en'}, page_content='It has extra requirements on temporal consistency across frames in time, which naturally demands more world knowledge to be encoded into the model.\\nIn comparison to text or images, it is more difficult to collect large amounts of high-quality, high-dimensional video data, let along text-video pairs.\\n\\n\\n\\nü•ë Required Pre-read: Please make sure you have read the previous blog on ‚ÄúWhat are Diffusion Models?‚Äù for image generation before continue here.'), Document(id='f5a277ca-50bd-4c85-bc6d-7f7fdc6fab08', metadata={'source': 'https://lilianweng.github.io/posts/2024-04-12-diffusion-video/', 'title': \"Diffusion Models for Video Generation | Lil'Log\", 'description': 'Diffusion models have demonstrated strong results on image synthesis in past years. Now the research community has started working on a harder task‚Äîusing it for video generation. The task itself is a superset of the image case, since an image is a video of 1 frame, and it is much more challenging because:\\n\\nIt has extra requirements on temporal consistency across frames in time, which naturally demands more world knowledge to be encoded into the model.\\nIn comparison to text or images, it is more difficult to collect large amounts of high-quality, high-dimensional video data, let along text-video pairs.\\n\\n\\n\\nü•ë Required Pre-read: Please make sure you have read the previous blog on ‚ÄúWhat are Diffusion Models?‚Äù for image generation before continue here.\\n', 'language': 'en'}, page_content=\"Diffusion Models for Video Generation | Lil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n|\\n\\n\\n\\n\\n\\n\\nPosts\\n\\n\\n\\n\\nArchive\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\nTags\\n\\n\\n\\n\\nFAQ\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      Diffusion Models for Video Generation\\n    \\nDate: April 12, 2024  |  Estimated Reading Time: 20 min  |  Author: Lilian Weng\\n\\n\\n \\n\\n\\nTable of Contents\\n\\n\\n\\nVideo Generation Modeling from Scratch\\n\\nParameterization & Sampling Basics\\n\\nModel Architecture: 3D U-Net & DiT\\n\\n\\nAdapting Image Models to Generate Videos\"), Document(id='f4c8d96c-aa73-40bd-9bcf-53d3dd820856', metadata={'source': 'https://lilianweng.github.io/posts/2024-04-12-diffusion-video/', 'title': \"Diffusion Models for Video Generation | Lil'Log\", 'description': 'Diffusion models have demonstrated strong results on image synthesis in past years. Now the research community has started working on a harder task‚Äîusing it for video generation. The task itself is a superset of the image case, since an image is a video of 1 frame, and it is much more challenging because:\\n\\nIt has extra requirements on temporal consistency across frames in time, which naturally demands more world knowledge to be encoded into the model.\\nIn comparison to text or images, it is more difficult to collect large amounts of high-quality, high-dimensional video data, let along text-video pairs.\\n\\n\\n\\nü•ë Required Pre-read: Please make sure you have read the previous blog on ‚ÄúWhat are Diffusion Models?‚Äù for image generation before continue here.\\n', 'language': 'en'}, page_content='[12] Esser et al. 2023 ‚ÄúStructure and Content-Guided Video Synthesis with Diffusion Models.‚Äù\\n[13] Bar-Tal et al. 2024 ‚ÄúLumiere: A Space-Time Diffusion Model for Video Generation.‚Äù'), Document(id='824f02c8-0088-44cb-8b05-db21e93f53ff', metadata={'source': 'https://lilianweng.github.io/posts/2024-04-12-diffusion-video/', 'title': \"Diffusion Models for Video Generation | Lil'Log\", 'description': 'Diffusion models have demonstrated strong results on image synthesis in past years. Now the research community has started working on a harder task‚Äîusing it for video generation. The task itself is a superset of the image case, since an image is a video of 1 frame, and it is much more challenging because:\\n\\nIt has extra requirements on temporal consistency across frames in time, which naturally demands more world knowledge to be encoded into the model.\\nIn comparison to text or images, it is more difficult to collect large amounts of high-quality, high-dimensional video data, let along text-video pairs.\\n\\n\\n\\nü•ë Required Pre-read: Please make sure you have read the previous blog on ‚ÄúWhat are Diffusion Models?‚Äù for image generation before continue here.\\n', 'language': 'en'}, page_content='Fine-tuning on Video Data#\\nMake-A-Video (Singer et al. 2022) extends a pre-trained diffusion image model with a temporal dimension, consisting of three key components:'), Document(id='04d048c8-4c22-47a5-a8cc-b3fcffb604ff', metadata={'source': 'https://lilianweng.github.io/posts/2024-04-12-diffusion-video/', 'title': \"Diffusion Models for Video Generation | Lil'Log\", 'description': 'Diffusion models have demonstrated strong results on image synthesis in past years. Now the research community has started working on a harder task‚Äîusing it for video generation. The task itself is a superset of the image case, since an image is a video of 1 frame, and it is much more challenging because:\\n\\nIt has extra requirements on temporal consistency across frames in time, which naturally demands more world knowledge to be encoded into the model.\\nIn comparison to text or images, it is more difficult to collect large amounts of high-quality, high-dimensional video data, let along text-video pairs.\\n\\n\\n\\nü•ë Required Pre-read: Please make sure you have read the previous blog on ‚ÄúWhat are Diffusion Models?‚Äù for image generation before continue here.\\n', 'language': 'en'}, page_content='The overview of ControlVideo. (Image source: Zhang et al. 2023)\\n\\nCitation#\\nCited as:\\n\\nWeng, Lilian. (Apr 2024). Diffusion Models Video Generation. Lil‚ÄôLog. https://lilianweng.github.io/posts/2024-04-12-diffusion-video/.'), Document(id='6526597a-bce7-447a-a83b-e9d6a61b0ea1', metadata={'source': 'https://lilianweng.github.io/posts/2024-04-12-diffusion-video/', 'title': \"Diffusion Models for Video Generation | Lil'Log\", 'description': 'Diffusion models have demonstrated strong results on image synthesis in past years. Now the research community has started working on a harder task‚Äîusing it for video generation. The task itself is a superset of the image case, since an image is a video of 1 frame, and it is much more challenging because:\\n\\nIt has extra requirements on temporal consistency across frames in time, which naturally demands more world knowledge to be encoded into the model.\\nIn comparison to text or images, it is more difficult to collect large amounts of high-quality, high-dimensional video data, let along text-video pairs.\\n\\n\\n\\nü•ë Required Pre-read: Please make sure you have read the previous blog on ‚ÄúWhat are Diffusion Models?‚Äù for image generation before continue here.\\n', 'language': 'en'}, page_content='It has extra requirements on temporal consistency across frames in time, which naturally demands more world knowledge to be encoded into the model.\\nIn comparison to text or images, it is more difficult to collect large amounts of high-quality, high-dimensional video data, let along text-video pairs.\\n\\n\\n\\nü•ë Required Pre-read: Please make sure you have read the previous blog on ‚ÄúWhat are Diffusion Models?‚Äù for image generation before continue here.'), Document(id='5b4a1f82-a34b-4a0b-8cf0-73ea2cd8ef4e', metadata={'source': 'https://lilianweng.github.io/posts/2024-04-12-diffusion-video/', 'title': \"Diffusion Models for Video Generation | Lil'Log\", 'description': 'Diffusion models have demonstrated strong results on image synthesis in past years. Now the research community has started working on a harder task‚Äîusing it for video generation. The task itself is a superset of the image case, since an image is a video of 1 frame, and it is much more challenging because:\\n\\nIt has extra requirements on temporal consistency across frames in time, which naturally demands more world knowledge to be encoded into the model.\\nIn comparison to text or images, it is more difficult to collect large amounts of high-quality, high-dimensional video data, let along text-video pairs.\\n\\n\\n\\nü•ë Required Pre-read: Please make sure you have read the previous blog on ‚ÄúWhat are Diffusion Models?‚Äù for image generation before continue here.\\n', 'language': 'en'}, page_content='Video Generation Modeling from Scratch#\\nFirst let‚Äôs review approaches for designing and training diffusion video models from scratch, meaning that we do not rely on pre-trained image generators.\\nParameterization & Sampling Basics#'), Document(id='824f02c8-0088-44cb-8b05-db21e93f53ff', metadata={'source': 'https://lilianweng.github.io/posts/2024-04-12-diffusion-video/', 'title': \"Diffusion Models for Video Generation | Lil'Log\", 'description': 'Diffusion models have demonstrated strong results on image synthesis in past years. Now the research community has started working on a harder task‚Äîusing it for video generation. The task itself is a superset of the image case, since an image is a video of 1 frame, and it is much more challenging because:\\n\\nIt has extra requirements on temporal consistency across frames in time, which naturally demands more world knowledge to be encoded into the model.\\nIn comparison to text or images, it is more difficult to collect large amounts of high-quality, high-dimensional video data, let along text-video pairs.\\n\\n\\n\\nü•ë Required Pre-read: Please make sure you have read the previous blog on ‚ÄúWhat are Diffusion Models?‚Äù for image generation before continue here.\\n', 'language': 'en'}, page_content='Fine-tuning on Video Data#\\nMake-A-Video (Singer et al. 2022) extends a pre-trained diffusion image model with a temporal dimension, consisting of three key components:'), Document(id='f5a277ca-50bd-4c85-bc6d-7f7fdc6fab08', metadata={'source': 'https://lilianweng.github.io/posts/2024-04-12-diffusion-video/', 'title': \"Diffusion Models for Video Generation | Lil'Log\", 'description': 'Diffusion models have demonstrated strong results on image synthesis in past years. Now the research community has started working on a harder task‚Äîusing it for video generation. The task itself is a superset of the image case, since an image is a video of 1 frame, and it is much more challenging because:\\n\\nIt has extra requirements on temporal consistency across frames in time, which naturally demands more world knowledge to be encoded into the model.\\nIn comparison to text or images, it is more difficult to collect large amounts of high-quality, high-dimensional video data, let along text-video pairs.\\n\\n\\n\\nü•ë Required Pre-read: Please make sure you have read the previous blog on ‚ÄúWhat are Diffusion Models?‚Äù for image generation before continue here.\\n', 'language': 'en'}, page_content=\"Diffusion Models for Video Generation | Lil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n|\\n\\n\\n\\n\\n\\n\\nPosts\\n\\n\\n\\n\\nArchive\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\nTags\\n\\n\\n\\n\\nFAQ\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      Diffusion Models for Video Generation\\n    \\nDate: April 12, 2024  |  Estimated Reading Time: 20 min  |  Author: Lilian Weng\\n\\n\\n \\n\\n\\nTable of Contents\\n\\n\\n\\nVideo Generation Modeling from Scratch\\n\\nParameterization & Sampling Basics\\n\\nModel Architecture: 3D U-Net & DiT\\n\\n\\nAdapting Image Models to Generate Videos\"), Document(id='75e6c761-311d-4e89-9115-63d356b8eae8', metadata={'source': 'https://lilianweng.github.io/posts/2024-04-12-diffusion-video/', 'title': \"Diffusion Models for Video Generation | Lil'Log\", 'description': 'Diffusion models have demonstrated strong results on image synthesis in past years. Now the research community has started working on a harder task‚Äîusing it for video generation. The task itself is a superset of the image case, since an image is a video of 1 frame, and it is much more challenging because:\\n\\nIt has extra requirements on temporal consistency across frames in time, which naturally demands more world knowledge to be encoded into the model.\\nIn comparison to text or images, it is more difficult to collect large amounts of high-quality, high-dimensional video data, let along text-video pairs.\\n\\n\\n\\nü•ë Required Pre-read: Please make sure you have read the previous blog on ‚ÄúWhat are Diffusion Models?‚Äù for image generation before continue here.\\n', 'language': 'en'}, page_content='The training pipeline of autoencoder in video latent diffusion models. The decoder is fine-tuned to have temporal coherency with a new across-frame discriminator while the encoder stays frozen. (Image source: Blattmann et al. 2023)'), Document(id='6526597a-bce7-447a-a83b-e9d6a61b0ea1', metadata={'source': 'https://lilianweng.github.io/posts/2024-04-12-diffusion-video/', 'title': \"Diffusion Models for Video Generation | Lil'Log\", 'description': 'Diffusion models have demonstrated strong results on image synthesis in past years. Now the research community has started working on a harder task‚Äîusing it for video generation. The task itself is a superset of the image case, since an image is a video of 1 frame, and it is much more challenging because:\\n\\nIt has extra requirements on temporal consistency across frames in time, which naturally demands more world knowledge to be encoded into the model.\\nIn comparison to text or images, it is more difficult to collect large amounts of high-quality, high-dimensional video data, let along text-video pairs.\\n\\n\\n\\nü•ë Required Pre-read: Please make sure you have read the previous blog on ‚ÄúWhat are Diffusion Models?‚Äù for image generation before continue here.\\n', 'language': 'en'}, page_content='It has extra requirements on temporal consistency across frames in time, which naturally demands more world knowledge to be encoded into the model.\\nIn comparison to text or images, it is more difficult to collect large amounts of high-quality, high-dimensional video data, let along text-video pairs.\\n\\n\\n\\nü•ë Required Pre-read: Please make sure you have read the previous blog on ‚ÄúWhat are Diffusion Models?‚Äù for image generation before continue here.'), Document(id='f4c8d96c-aa73-40bd-9bcf-53d3dd820856', metadata={'source': 'https://lilianweng.github.io/posts/2024-04-12-diffusion-video/', 'title': \"Diffusion Models for Video Generation | Lil'Log\", 'description': 'Diffusion models have demonstrated strong results on image synthesis in past years. Now the research community has started working on a harder task‚Äîusing it for video generation. The task itself is a superset of the image case, since an image is a video of 1 frame, and it is much more challenging because:\\n\\nIt has extra requirements on temporal consistency across frames in time, which naturally demands more world knowledge to be encoded into the model.\\nIn comparison to text or images, it is more difficult to collect large amounts of high-quality, high-dimensional video data, let along text-video pairs.\\n\\n\\n\\nü•ë Required Pre-read: Please make sure you have read the previous blog on ‚ÄúWhat are Diffusion Models?‚Äù for image generation before continue here.\\n', 'language': 'en'}, page_content='[12] Esser et al. 2023 ‚ÄúStructure and Content-Guided Video Synthesis with Diffusion Models.‚Äù\\n[13] Bar-Tal et al. 2024 ‚ÄúLumiere: A Space-Time Diffusion Model for Video Generation.‚Äù'), Document(id='8fdf0520-e74f-4bdd-a758-5a708cf3f44d', metadata={'source': 'https://lilianweng.github.io/posts/2024-04-12-diffusion-video/', 'title': \"Diffusion Models for Video Generation | Lil'Log\", 'description': 'Diffusion models have demonstrated strong results on image synthesis in past years. Now the research community has started working on a harder task‚Äîusing it for video generation. The task itself is a superset of the image case, since an image is a video of 1 frame, and it is much more challenging because:\\n\\nIt has extra requirements on temporal consistency across frames in time, which naturally demands more world knowledge to be encoded into the model.\\nIn comparison to text or images, it is more difficult to collect large amounts of high-quality, high-dimensional video data, let along text-video pairs.\\n\\n\\n\\nü•ë Required Pre-read: Please make sure you have read the previous blog on ‚ÄúWhat are Diffusion Models?‚Äù for image generation before continue here.\\n', 'language': 'en'}, page_content='[8] Singer et al. ‚ÄúMake-A-Video: Text-to-Video Generation without Text-Video Data.‚Äù 2022.\\n[9] Wu et al. ‚ÄúTune-A-Video: One-Shot Tuning of Image Diffusion Models for Text-to-Video Generation.‚Äù ICCV 2023.\\n[10] Blattmann et al. 2023 ‚ÄúAlign your Latents: High-Resolution Video Synthesis with Latent Diffusion Models.‚Äù\\n[11] Blattmann et al. 2023 ‚ÄúStable Video Diffusion: Scaling Latent Video Diffusion Models to Large Datasets.‚Äù'), Document(id='f5a277ca-50bd-4c85-bc6d-7f7fdc6fab08', metadata={'source': 'https://lilianweng.github.io/posts/2024-04-12-diffusion-video/', 'title': \"Diffusion Models for Video Generation | Lil'Log\", 'description': 'Diffusion models have demonstrated strong results on image synthesis in past years. Now the research community has started working on a harder task‚Äîusing it for video generation. The task itself is a superset of the image case, since an image is a video of 1 frame, and it is much more challenging because:\\n\\nIt has extra requirements on temporal consistency across frames in time, which naturally demands more world knowledge to be encoded into the model.\\nIn comparison to text or images, it is more difficult to collect large amounts of high-quality, high-dimensional video data, let along text-video pairs.\\n\\n\\n\\nü•ë Required Pre-read: Please make sure you have read the previous blog on ‚ÄúWhat are Diffusion Models?‚Äù for image generation before continue here.\\n', 'language': 'en'}, page_content=\"Diffusion Models for Video Generation | Lil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n|\\n\\n\\n\\n\\n\\n\\nPosts\\n\\n\\n\\n\\nArchive\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\nTags\\n\\n\\n\\n\\nFAQ\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      Diffusion Models for Video Generation\\n    \\nDate: April 12, 2024  |  Estimated Reading Time: 20 min  |  Author: Lilian Weng\\n\\n\\n \\n\\n\\nTable of Contents\\n\\n\\n\\nVideo Generation Modeling from Scratch\\n\\nParameterization & Sampling Basics\\n\\nModel Architecture: 3D U-Net & DiT\\n\\n\\nAdapting Image Models to Generate Videos\"), Document(id='285d208b-8036-4522-9cea-64d6714f511f', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent‚Äôs brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\t\\n\\tOverview of a LLM-powered autonomous agent system.\\n\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en'}, page_content='Thought: ...\\nAction: ...\\nObservation: ...\\n... (Repeated many times)'), Document(id='cb0f736e-f03d-4913-b961-f6368ae43fd5', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent‚Äôs brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\t\\n\\tOverview of a LLM-powered autonomous agent system.\\n\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en'}, page_content='Prompt LM with 100 most recent observations and to generate 3 most salient high-level questions given a set of observations/statements. Then ask LM to answer those questions.\\n\\n\\nPlanning & Reacting: translate the reflections and the environment information into actions'), Document(id='12a9beaf-e517-41b6-9c88-2a7551e41ea2', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent‚Äôs brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\t\\n\\tOverview of a LLM-powered autonomous agent system.\\n\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en'}, page_content='14. Get Improved Code: \"improve_code\", args: \"suggestions\": \"<list_of_suggestions>\", \"code\": \"<full_code_string>\"\\n15. Write Tests: \"write_tests\", args: \"code\": \"<full_code_string>\", \"focus\": \"<list_of_focus_areas>\"\\n16. Execute Python File: \"execute_python_file\", args: \"file\": \"<file>\"\\n17. Generate Image: \"generate_image\", args: \"prompt\": \"<prompt>\"\\n18. Send Tweet: \"send_tweet\", args: \"text\": \"<text>\"\\n19. Do Nothing: \"do_nothing\", args:'), Document(id='d8e79297-e530-4b75-917e-1bb6dba9d938', metadata={'source': 'https://lilianweng.github.io/posts/2024-04-12-diffusion-video/', 'title': \"Diffusion Models for Video Generation | Lil'Log\", 'description': 'Diffusion models have demonstrated strong results on image synthesis in past years. Now the research community has started working on a harder task‚Äîusing it for video generation. The task itself is a superset of the image case, since an image is a video of 1 frame, and it is much more challenging because:\\n\\nIt has extra requirements on temporal consistency across frames in time, which naturally demands more world knowledge to be encoded into the model.\\nIn comparison to text or images, it is more difficult to collect large amounts of high-quality, high-dimensional video data, let along text-video pairs.\\n\\n\\n\\nü•ë Required Pre-read: Please make sure you have read the previous blog on ‚ÄúWhat are Diffusion Models?‚Äù for image generation before continue here.\\n', 'language': 'en'}, page_content='STUNet inflates a pretrained text-to-image U-net to be able to downsample and upsample videos at both time and space dimensions. Convo-based blocks consist of pre-trained text-to-image layers, followed by a factorized space-time convolution. And attention-based blocks at the coarsest U-Net level contains the pre-trained text-to-image, followed by temporal attention. Further training only happens with the newly added layers.'), Document(id='12a9beaf-e517-41b6-9c88-2a7551e41ea2', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent‚Äôs brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\t\\n\\tOverview of a LLM-powered autonomous agent system.\\n\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en'}, page_content='14. Get Improved Code: \"improve_code\", args: \"suggestions\": \"<list_of_suggestions>\", \"code\": \"<full_code_string>\"\\n15. Write Tests: \"write_tests\", args: \"code\": \"<full_code_string>\", \"focus\": \"<list_of_focus_areas>\"\\n16. Execute Python File: \"execute_python_file\", args: \"file\": \"<file>\"\\n17. Generate Image: \"generate_image\", args: \"prompt\": \"<prompt>\"\\n18. Send Tweet: \"send_tweet\", args: \"text\": \"<text>\"\\n19. Do Nothing: \"do_nothing\", args:'), Document(id='4ddbf464-629c-45cd-a2c9-69ed69780603', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent‚Äôs brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\t\\n\\tOverview of a LLM-powered autonomous agent system.\\n\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en'}, page_content='19. Do Nothing: \"do_nothing\", args:\\n20. Task Complete (Shutdown): \"task_complete\", args: \"reason\": \"<reason>\"'), Document(id='348c2005-966d-482d-b1dd-b8d2e85a5f19', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent‚Äôs brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\t\\n\\tOverview of a LLM-powered autonomous agent system.\\n\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en'}, page_content='}\\n]\\nChallenges#\\nAfter going through key ideas and demos of building LLM-centered agents, I start to see a couple common limitations:'), Document(id='f6b419cb-fecf-48c4-9fb7-addd8b0310ef', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent‚Äôs brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\t\\n\\tOverview of a LLM-powered autonomous agent system.\\n\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en'}, page_content='Case Studies#\\nScientific Discovery Agent#\\nChemCrow (Bran et al. 2023) is a domain-specific example in which LLM is augmented with 13 expert-designed tools to accomplish tasks across organic synthesis, drug discovery, and materials design. The workflow, implemented in LangChain, reflects what was previously described in the ReAct and MRKLs and combines CoT reasoning with tools relevant to the tasks:'), Document(id='7b20b8a7-4a3c-409c-ac4e-f14a1b039c05', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent‚Äôs brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\t\\n\\tOverview of a LLM-powered autonomous agent system.\\n\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en'}, page_content='everything is sufficiently clear, only answer \\\\\"Nothing more to clarify.\\\\\".\"'), Document(id='d792d255-9bec-4312-a71d-3d3357260f41', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent‚Äôs brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\t\\n\\tOverview of a LLM-powered autonomous agent system.\\n\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en'}, page_content='\"content\": \"{{There are 10 levels in total. The main character is a plumber named Mario, who can walk and jump. It is a classical platform game just like Super Mario. The main character moves from left to right, trying to get to the destination, where there are many obstacles and attacks from enemies in the process.}}\\\\n\\\\nIs anything else unclear? If yes, only answer in the form:\\\\n{remaining unclear areas} remaining questions.\\\\n{Next question}\\\\nIf everything is sufficiently clear, only'), Document(id='285d208b-8036-4522-9cea-64d6714f511f', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent‚Äôs brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\t\\n\\tOverview of a LLM-powered autonomous agent system.\\n\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en'}, page_content='Thought: ...\\nAction: ...\\nObservation: ...\\n... (Repeated many times)'), Document(id='c46e73fa-a35a-4d17-877d-f06a6d471e36', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent‚Äôs brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\t\\n\\tOverview of a LLM-powered autonomous agent system.\\n\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en'}, page_content='},\\n  {\\n    \"role\": \"assistant\",\\n    \"content\": \"Summary of areas that need clarification:\\\\n1. Specifics of the Super Mario game (e.g. level design, characters, gameplay mechanics)\\\\n2. Details about the MVC components (e.g. which components are in each file)\\\\n3. Keyboard control implementation (e.g. which keys to use, how to handle input)\\\\n\\\\nClarifying question:\\\\nCan you provide more details about the Super Mario game, such as level design, characters, and gameplay mechanics?\"\\n  },\\n  {'), Document(id='d792d255-9bec-4312-a71d-3d3357260f41', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent‚Äôs brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\t\\n\\tOverview of a LLM-powered autonomous agent system.\\n\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en'}, page_content='\"content\": \"{{There are 10 levels in total. The main character is a plumber named Mario, who can walk and jump. It is a classical platform game just like Super Mario. The main character moves from left to right, trying to get to the destination, where there are many obstacles and attacks from enemies in the process.}}\\\\n\\\\nIs anything else unclear? If yes, only answer in the form:\\\\n{remaining unclear areas} remaining questions.\\\\n{Next question}\\\\nIf everything is sufficiently clear, only'), Document(id='1ac31911-7b3f-4520-8142-93cd2e15bcbf', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent‚Äôs brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\t\\n\\tOverview of a LLM-powered autonomous agent system.\\n\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en'}, page_content='aspect of the keyboard control implementation you would like me to clarify?\"'), Document(id='c46e73fa-a35a-4d17-877d-f06a6d471e36', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent‚Äôs brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\t\\n\\tOverview of a LLM-powered autonomous agent system.\\n\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en'}, page_content='},\\n  {\\n    \"role\": \"assistant\",\\n    \"content\": \"Summary of areas that need clarification:\\\\n1. Specifics of the Super Mario game (e.g. level design, characters, gameplay mechanics)\\\\n2. Details about the MVC components (e.g. which components are in each file)\\\\n3. Keyboard control implementation (e.g. which keys to use, how to handle input)\\\\n\\\\nClarifying question:\\\\nCan you provide more details about the Super Mario game, such as level design, characters, and gameplay mechanics?\"\\n  },\\n  {'), Document(id='7b20b8a7-4a3c-409c-ac4e-f14a1b039c05', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent‚Äôs brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\t\\n\\tOverview of a LLM-powered autonomous agent system.\\n\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en'}, page_content='everything is sufficiently clear, only answer \\\\\"Nothing more to clarify.\\\\\".\"'), Document(id='6a5819cf-629d-4451-8bc3-d7632df91f43', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent‚Äôs brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\t\\n\\tOverview of a LLM-powered autonomous agent system.\\n\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en'}, page_content='The idea of CoH is to present a history of sequentially improved outputs  in context and train the model to take on the trend to produce better outputs. Algorithm Distillation (AD; Laskin et al. 2023) applies the same idea to cross-episode trajectories in reinforcement learning tasks, where an algorithm is encapsulated in a long history-conditioned policy. Considering that an agent interacts with the environment many times and in each episode the agent gets a little better, AD concatenates this'), Document(id='04d048c8-4c22-47a5-a8cc-b3fcffb604ff', metadata={'source': 'https://lilianweng.github.io/posts/2024-04-12-diffusion-video/', 'title': \"Diffusion Models for Video Generation | Lil'Log\", 'description': 'Diffusion models have demonstrated strong results on image synthesis in past years. Now the research community has started working on a harder task‚Äîusing it for video generation. The task itself is a superset of the image case, since an image is a video of 1 frame, and it is much more challenging because:\\n\\nIt has extra requirements on temporal consistency across frames in time, which naturally demands more world knowledge to be encoded into the model.\\nIn comparison to text or images, it is more difficult to collect large amounts of high-quality, high-dimensional video data, let along text-video pairs.\\n\\n\\n\\nü•ë Required Pre-read: Please make sure you have read the previous blog on ‚ÄúWhat are Diffusion Models?‚Äù for image generation before continue here.\\n', 'language': 'en'}, page_content='The overview of ControlVideo. (Image source: Zhang et al. 2023)\\n\\nCitation#\\nCited as:\\n\\nWeng, Lilian. (Apr 2024). Diffusion Models Video Generation. Lil‚ÄôLog. https://lilianweng.github.io/posts/2024-04-12-diffusion-video/.'), Document(id='e025ee85-a95e-447c-8361-866618cd361f', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent‚Äôs brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\t\\n\\tOverview of a LLM-powered autonomous agent system.\\n\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en'}, page_content=\"Planning is essentially in order to optimize believability at the moment vs in time.\\nPrompt template: {Intro of an agent X}. Here is X's plan today in broad strokes: 1)\\nRelationships between agents and observations of one agent by another are all taken into consideration for planning and reacting.\\nEnvironment information is present in a tree structure.\\n\\n\\n\\n\\n\\nThe generative agent architecture. (Image source: Park et al. 2023)\"), Document(id='f5a277ca-50bd-4c85-bc6d-7f7fdc6fab08', metadata={'source': 'https://lilianweng.github.io/posts/2024-04-12-diffusion-video/', 'title': \"Diffusion Models for Video Generation | Lil'Log\", 'description': 'Diffusion models have demonstrated strong results on image synthesis in past years. Now the research community has started working on a harder task‚Äîusing it for video generation. The task itself is a superset of the image case, since an image is a video of 1 frame, and it is much more challenging because:\\n\\nIt has extra requirements on temporal consistency across frames in time, which naturally demands more world knowledge to be encoded into the model.\\nIn comparison to text or images, it is more difficult to collect large amounts of high-quality, high-dimensional video data, let along text-video pairs.\\n\\n\\n\\nü•ë Required Pre-read: Please make sure you have read the previous blog on ‚ÄúWhat are Diffusion Models?‚Äù for image generation before continue here.\\n', 'language': 'en'}, page_content=\"Diffusion Models for Video Generation | Lil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n|\\n\\n\\n\\n\\n\\n\\nPosts\\n\\n\\n\\n\\nArchive\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\nTags\\n\\n\\n\\n\\nFAQ\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      Diffusion Models for Video Generation\\n    \\nDate: April 12, 2024  |  Estimated Reading Time: 20 min  |  Author: Lilian Weng\\n\\n\\n \\n\\n\\nTable of Contents\\n\\n\\n\\nVideo Generation Modeling from Scratch\\n\\nParameterization & Sampling Basics\\n\\nModel Architecture: 3D U-Net & DiT\\n\\n\\nAdapting Image Models to Generate Videos\"), Document(id='1b4ee1fe-fe26-475a-9835-02b961e5deda', metadata={'source': 'https://lilianweng.github.io/posts/2024-04-12-diffusion-video/', 'title': \"Diffusion Models for Video Generation | Lil'Log\", 'description': 'Diffusion models have demonstrated strong results on image synthesis in past years. Now the research community has started working on a harder task‚Äîusing it for video generation. The task itself is a superset of the image case, since an image is a video of 1 frame, and it is much more challenging because:\\n\\nIt has extra requirements on temporal consistency across frames in time, which naturally demands more world knowledge to be encoded into the model.\\nIn comparison to text or images, it is more difficult to collect large amounts of high-quality, high-dimensional video data, let along text-video pairs.\\n\\n\\n\\nü•ë Required Pre-read: Please make sure you have read the previous blog on ‚ÄúWhat are Diffusion Models?‚Äù for image generation before continue here.\\n', 'language': 'en'}, page_content='Content $c$ refers to appearance and semantics of the video, that is sampled from the text for conditional editing. CLIP embedding of the frame is a good representation of content, and stays largely orthogonal to structure traits.'), Document(id='348c2005-966d-482d-b1dd-b8d2e85a5f19', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent‚Äôs brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\t\\n\\tOverview of a LLM-powered autonomous agent system.\\n\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en'}, page_content='}\\n]\\nChallenges#\\nAfter going through key ideas and demos of building LLM-centered agents, I start to see a couple common limitations:'), Document(id='240fb3e8-a74c-4f09-94f6-0f58051b7a1a', metadata={'source': 'https://lilianweng.github.io/posts/2024-04-12-diffusion-video/', 'title': \"Diffusion Models for Video Generation | Lil'Log\", 'description': 'Diffusion models have demonstrated strong results on image synthesis in past years. Now the research community has started working on a harder task‚Äîusing it for video generation. The task itself is a superset of the image case, since an image is a video of 1 frame, and it is much more challenging because:\\n\\nIt has extra requirements on temporal consistency across frames in time, which naturally demands more world knowledge to be encoded into the model.\\nIn comparison to text or images, it is more difficult to collect large amounts of high-quality, high-dimensional video data, let along text-video pairs.\\n\\n\\n\\nü•ë Required Pre-read: Please make sure you have read the previous blog on ‚ÄúWhat are Diffusion Models?‚Äù for image generation before continue here.\\n', 'language': 'en'}, page_content='Tune-A-Video (Wu et al. 2023) inflates a pre-trained image diffusion model to enable one-shot video tuning: Given a video containing $m$ frames, $\\\\mathcal{V} = \\\\{v_i \\\\mid i = 1, \\\\dots, m\\\\}$, paired with a descriptive prompt $\\\\tau$, the task is to generate a new video $\\\\mathcal{V}^*$ based on a slightly edited & related text prompt $\\\\tau^*$. For example, $\\\\tau$ = \"A man is skiing\" can be extended to $\\\\tau^*$=\"Spiderman is skiing on the beach\". Tune-A-Video is meant to be used for object editing,'), Document(id='5ea5e2ba-f460-40c9-a8b7-1488d226c716', metadata={'source': 'https://lilianweng.github.io/posts/2024-04-12-diffusion-video/', 'title': \"Diffusion Models for Video Generation | Lil'Log\", 'description': 'Diffusion models have demonstrated strong results on image synthesis in past years. Now the research community has started working on a harder task‚Äîusing it for video generation. The task itself is a superset of the image case, since an image is a video of 1 frame, and it is much more challenging because:\\n\\nIt has extra requirements on temporal consistency across frames in time, which naturally demands more world knowledge to be encoded into the model.\\nIn comparison to text or images, it is more difficult to collect large amounts of high-quality, high-dimensional video data, let along text-video pairs.\\n\\n\\n\\nü•ë Required Pre-read: Please make sure you have read the previous blog on ‚ÄúWhat are Diffusion Models?‚Äù for image generation before continue here.\\n', 'language': 'en'}, page_content='ControlVideo (Zhang et al. 2023) aims to generate videos conditioned on text prompt $\\\\tau$ and a motion sequence (e.g., depth or edge maps), $\\\\mathbf{c} = \\\\{c^i\\\\}_{i=0}^{N-1}$. It is adapted from ControlNet with three new mechanisms added:'), Document(id='6d62fe79-0d13-4093-8d8e-2d50e9f6fc3d', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent‚Äôs brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\t\\n\\tOverview of a LLM-powered autonomous agent system.\\n\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en'}, page_content='then aggregates the results. The idea is quite related to KD tree but a lot more scalable.'), Document(id='d792d255-9bec-4312-a71d-3d3357260f41', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent‚Äôs brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\t\\n\\tOverview of a LLM-powered autonomous agent system.\\n\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en'}, page_content='\"content\": \"{{There are 10 levels in total. The main character is a plumber named Mario, who can walk and jump. It is a classical platform game just like Super Mario. The main character moves from left to right, trying to get to the destination, where there are many obstacles and attacks from enemies in the process.}}\\\\n\\\\nIs anything else unclear? If yes, only answer in the form:\\\\n{remaining unclear areas} remaining questions.\\\\n{Next question}\\\\nIf everything is sufficiently clear, only'), Document(id='339bf365-ffef-41c9-bd01-d0c06d168241', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent‚Äôs brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\t\\n\\tOverview of a LLM-powered autonomous agent system.\\n\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en'}, page_content='Level-1 evaluates the ability to call the API. Given an API‚Äôs description, the model needs to determine whether to call a given API, call it correctly, and respond properly to API returns.\\nLevel-2 examines the ability to retrieve the API. The model needs to search for possible APIs that may solve the user‚Äôs requirement and learn how to use them by reading documentation.'), Document(id='c46e73fa-a35a-4d17-877d-f06a6d471e36', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent‚Äôs brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\t\\n\\tOverview of a LLM-powered autonomous agent system.\\n\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en'}, page_content='},\\n  {\\n    \"role\": \"assistant\",\\n    \"content\": \"Summary of areas that need clarification:\\\\n1. Specifics of the Super Mario game (e.g. level design, characters, gameplay mechanics)\\\\n2. Details about the MVC components (e.g. which components are in each file)\\\\n3. Keyboard control implementation (e.g. which keys to use, how to handle input)\\\\n\\\\nClarifying question:\\\\nCan you provide more details about the Super Mario game, such as level design, characters, and gameplay mechanics?\"\\n  },\\n  {'), Document(id='04d048c8-4c22-47a5-a8cc-b3fcffb604ff', metadata={'source': 'https://lilianweng.github.io/posts/2024-04-12-diffusion-video/', 'title': \"Diffusion Models for Video Generation | Lil'Log\", 'description': 'Diffusion models have demonstrated strong results on image synthesis in past years. Now the research community has started working on a harder task‚Äîusing it for video generation. The task itself is a superset of the image case, since an image is a video of 1 frame, and it is much more challenging because:\\n\\nIt has extra requirements on temporal consistency across frames in time, which naturally demands more world knowledge to be encoded into the model.\\nIn comparison to text or images, it is more difficult to collect large amounts of high-quality, high-dimensional video data, let along text-video pairs.\\n\\n\\n\\nü•ë Required Pre-read: Please make sure you have read the previous blog on ‚ÄúWhat are Diffusion Models?‚Äù for image generation before continue here.\\n', 'language': 'en'}, page_content='The overview of ControlVideo. (Image source: Zhang et al. 2023)\\n\\nCitation#\\nCited as:\\n\\nWeng, Lilian. (Apr 2024). Diffusion Models Video Generation. Lil‚ÄôLog. https://lilianweng.github.io/posts/2024-04-12-diffusion-video/.'), Document(id='54bd4d74-b66f-406a-b578-4b040fb73a41', metadata={'source': 'https://lilianweng.github.io/posts/2024-04-12-diffusion-video/', 'title': \"Diffusion Models for Video Generation | Lil'Log\", 'description': 'Diffusion models have demonstrated strong results on image synthesis in past years. Now the research community has started working on a harder task‚Äîusing it for video generation. The task itself is a superset of the image case, since an image is a video of 1 frame, and it is much more challenging because:\\n\\nIt has extra requirements on temporal consistency across frames in time, which naturally demands more world knowledge to be encoded into the model.\\nIn comparison to text or images, it is more difficult to collect large amounts of high-quality, high-dimensional video data, let along text-video pairs.\\n\\n\\n\\nü•ë Required Pre-read: Please make sure you have read the previous blog on ‚ÄúWhat are Diffusion Models?‚Äù for image generation before continue here.\\n', 'language': 'en'}, page_content='[3] Bar-Tal et al. 2024 ‚ÄúLumiere: A Space-Time Diffusion Model for Video Generation.‚Äù\\n[4] Brooks et al. ‚ÄúVideo generation models as world simulators.‚Äù OpenAI Blog, 2024.\\n[5] Zhang et al. 2023 ‚ÄúControlVideo: Training-free Controllable Text-to-Video Generation.‚Äù\\n[6] Khachatryan et al. 2023 ‚ÄúText2Video-Zero: Text-to-image diffusion models are zero-shot video generators.‚Äù\\n[7] Ho, et al. 2022 ‚ÄúImagen Video: High Definition Video Generation with Diffusion Models.‚Äù'), Document(id='6a5819cf-629d-4451-8bc3-d7632df91f43', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent‚Äôs brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\t\\n\\tOverview of a LLM-powered autonomous agent system.\\n\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en'}, page_content='The idea of CoH is to present a history of sequentially improved outputs  in context and train the model to take on the trend to produce better outputs. Algorithm Distillation (AD; Laskin et al. 2023) applies the same idea to cross-episode trajectories in reinforcement learning tasks, where an algorithm is encapsulated in a long history-conditioned policy. Considering that an agent interacts with the environment many times and in each episode the agent gets a little better, AD concatenates this'), Document(id='f4c8d96c-aa73-40bd-9bcf-53d3dd820856', metadata={'source': 'https://lilianweng.github.io/posts/2024-04-12-diffusion-video/', 'title': \"Diffusion Models for Video Generation | Lil'Log\", 'description': 'Diffusion models have demonstrated strong results on image synthesis in past years. Now the research community has started working on a harder task‚Äîusing it for video generation. The task itself is a superset of the image case, since an image is a video of 1 frame, and it is much more challenging because:\\n\\nIt has extra requirements on temporal consistency across frames in time, which naturally demands more world knowledge to be encoded into the model.\\nIn comparison to text or images, it is more difficult to collect large amounts of high-quality, high-dimensional video data, let along text-video pairs.\\n\\n\\n\\nü•ë Required Pre-read: Please make sure you have read the previous blog on ‚ÄúWhat are Diffusion Models?‚Äù for image generation before continue here.\\n', 'language': 'en'}, page_content='[12] Esser et al. 2023 ‚ÄúStructure and Content-Guided Video Synthesis with Diffusion Models.‚Äù\\n[13] Bar-Tal et al. 2024 ‚ÄúLumiere: A Space-Time Diffusion Model for Video Generation.‚Äù'), Document(id='e446154e-a66f-4612-9224-8218cd464f88', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent‚Äôs brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\t\\n\\tOverview of a LLM-powered autonomous agent system.\\n\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en'}, page_content='Illustration of the Reflexion framework. (Image source: Shinn & Labash, 2023)'), Document(id='fd12b93c-9d8d-436d-bef0-230ad74e23e4', metadata={'source': 'https://lilianweng.github.io/posts/2024-04-12-diffusion-video/', 'title': \"Diffusion Models for Video Generation | Lil'Log\", 'description': 'Diffusion models have demonstrated strong results on image synthesis in past years. Now the research community has started working on a harder task‚Äîusing it for video generation. The task itself is a superset of the image case, since an image is a video of 1 frame, and it is much more challenging because:\\n\\nIt has extra requirements on temporal consistency across frames in time, which naturally demands more world knowledge to be encoded into the model.\\nIn comparison to text or images, it is more difficult to collect large amounts of high-quality, high-dimensional video data, let along text-video pairs.\\n\\n\\n\\nü•ë Required Pre-read: Please make sure you have read the previous blog on ‚ÄúWhat are Diffusion Models?‚Äù for image generation before continue here.\\n', 'language': 'en'}, page_content='Structure $s$ depicts greometry and dynamics, including shapes, locations, temporal changes of objects, and $s$ is sampled from the input video. Depth estimation or other task-specific side information (e.g. human body pose or face landmarks for human video synthesis) can be used.'), Document(id='2961195f-9851-44b4-b768-033753a476f6', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent‚Äôs brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\t\\n\\tOverview of a LLM-powered autonomous agent system.\\n\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en'}, page_content='\"content\": \"You will get instructions for code to write.\\\\nYou will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their'), Document(id='339bf365-ffef-41c9-bd01-d0c06d168241', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent‚Äôs brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\t\\n\\tOverview of a LLM-powered autonomous agent system.\\n\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en'}, page_content='Level-1 evaluates the ability to call the API. Given an API‚Äôs description, the model needs to determine whether to call a given API, call it correctly, and respond properly to API returns.\\nLevel-2 examines the ability to retrieve the API. The model needs to search for possible APIs that may solve the user‚Äôs requirement and learn how to use them by reading documentation.')], 'answer': '# Agent Loops and Challenges in Diffusion Video Generation\\n\\n## How Agent Loops Work\\n\\nBased on the information provided in your text, agent loops refer to the cyclical process where AI agents interact with their environment and each other through a continuous feedback mechanism. The key characteristics of these loops include:\\n\\n1. **Multi-agent relationships**: \"Relationships between agents and observations of one agent by another are all taken into consideration for planning and reacting.\" This means that in a system with multiple agents, each agent\\'s actions and observations directly influence other agents\\' behavior.\\n\\n2. **Environment structure**: \"Environment information is present in a tree structure.\" This suggests that the system maintains a hierarchical representation of the environment where information flows from observations to decisions.\\n\\n3. **Planning and reacting**: The loop involves agents planning actions based on their observations, then executing those actions, and reacting to the resulting environmental changes.\\n\\n4. **Temporal dimension**: Unlike static systems, agent loops operate over time with the ability to observe the environment at different points in time and adjust behavior accordingly.\\n\\nThis creates a self-improving system where each cycle of observation ‚Üí planning ‚Üí action ‚Üí feedback leads to better decision-making over time.\\n\\n## Challenges in Diffusion Video Generation\\n\\nFrom the context of your text, diffusion video generation faces several key challenges:\\n\\n1. **Temporal consistency**: Video generation requires maintaining consistent visual content across frames while still allowing for meaningful motion. As noted in your text, the \"structure $s$ depicts geometry and dynamics, including shapes, locations, temporal changes of objects,\" which creates complexity in maintaining temporal coherence.\\n\\n2. **Model scalability**: Video generation is significantly more complex than image generation. The text mentions that models like ControlVideo (Zhang et al. 2023) and Tune-A-Video (Wu et al. 2023) have been developed to address these challenges, showing that the problem requires specialized approaches.\\n\\n3. **Conditional editing**: As mentioned in your text, \"Content $c$ refers to appearance and semantics of the video, that is sampled from the text for conditional editing.\" This creates challenges in precisely controlling which aspects of the video are modified while maintaining overall coherence.\\n\\n4. **Motion representation**: Video generation requires modeling motion trajectories, which is more complex than static image generation. The text references models like Lumiere (Bar-Tal et al. 2024) that specifically address this challenge.\\n\\n5. **Real-time processing**: The text mentions that \"Tune-A-Video is meant to be used for object editing,\" indicating that practical applications require efficient processing that can handle video generation without excessive computational overhead.\\n\\nThe challenges in diffusion video generation are particularly significant because video is inherently a temporal medium with complex dynamics that require specialized modeling approaches beyond what works well for static images. Models like ControlVideo and Lumiere represent important efforts to address these challenges through specialized architectures that can handle both the visual content and the temporal aspects of video generation.\\n\\nThis is why the field has seen rapid development with models that specifically address video generation challenges, building upon the more established image generation techniques while adding the necessary temporal components.'}\n",
      "\n",
      " Sub-question:\n",
      "- Here's a precise breakdown of your complex question into **3 targeted sub-questions** that maintain technical accuracy while resolving potential terminology confusion (note: \"agent loops\" is **not standard terminology** in diffusion video generation‚Äîthis is addressed upfront to avoid misunderstanding):\n",
      "- \n",
      "- ### üîç Sub-Questions:\n",
      "- 1. **Clarify terminology**: *What specific concept does \"agent loops\" refer to in the context of diffusion video generation?*\n",
      "- *(Why? \"Agent loops\" isn't a standard term in diffusion models. This sub-question resolves confusion‚Äîe.g., it likely means \"training loops,\" \"temporal conditioning loops,\" or \"agent-based control loops\" in generative AI systems.)*\n",
      "- 2. **Core mechanism**: *How do diffusion models generate video sequences step-by-step (e.g., from latent space to frames)?*\n",
      "- *(Why? This directly answers \"how\" for video diffusion‚Äîcovering temporal dynamics, noise scheduling, and frame prediction without conflating unrelated concepts.)*\n",
      "- 3. **Key challenges**: *What are the most critical technical hurdles in training/stable deployment of diffusion video models (e.g., computational cost, temporal coherence)?*\n",
      "- *(Why? Focuses on *real-world* video-specific challenges like long sequences, motion artifacts, and scalability‚Äîavoiding generic diffusion challenges.)*\n",
      "- \n",
      "- ### üí° Why this works:\n",
      "- | Original Question Part          | Why this sub-question is better                                                                 |\n",
      "- |---------------------------------|----------------------------------------------------------------------------------------------|\n",
      "- | \"Explain how agent loops work\" | Starts by **fixing terminology** (critical‚Äîdiffusion video uses *no* \"agent loops\" as a standard concept). Prevents misalignment. |\n",
      "- | \"What are the challenges...\"   | Targets **video-specific** challenges (e.g., temporal consistency vs. image challenges). Avoids tangents like \"text-to-video\" or \"audio\" issues. |\n",
      "- | **Overall**                     | Ensures sub-questions are **answerable**, **non-overlapping**, and **practically useful** for research/implementation. |\n",
      "- > ‚úÖ **Key insight**: In diffusion video generation (e.g., models like **Stable Diffusion for video**, **Video Diffusion Models**), the term \"agent loops\" is almost certainly a misstatement. The field uses **temporal diffusion loops** (for frame-by-frame generation) or **training loops** (for optimization). This breakdown avoids spreading misinformation while staying technically grounded.\n",
      "- This structure helps you systematically address the question without ambiguity‚Äîperfect for academic, engineering, or self-study contexts. Let me know if you'd like deeper dives into any sub-question!\n",
      "\n",
      " Final Answer:\n",
      " # Agent Loops and Challenges in Diffusion Video Generation\n",
      "\n",
      "## How Agent Loops Work\n",
      "\n",
      "Based on the information provided in your text, agent loops refer to the cyclical process where AI agents interact with their environment and each other through a continuous feedback mechanism. The key characteristics of these loops include:\n",
      "\n",
      "1. **Multi-agent relationships**: \"Relationships between agents and observations of one agent by another are all taken into consideration for planning and reacting.\" This means that in a system with multiple agents, each agent's actions and observations directly influence other agents' behavior.\n",
      "\n",
      "2. **Environment structure**: \"Environment information is present in a tree structure.\" This suggests that the system maintains a hierarchical representation of the environment where information flows from observations to decisions.\n",
      "\n",
      "3. **Planning and reacting**: The loop involves agents planning actions based on their observations, then executing those actions, and reacting to the resulting environmental changes.\n",
      "\n",
      "4. **Temporal dimension**: Unlike static systems, agent loops operate over time with the ability to observe the environment at different points in time and adjust behavior accordingly.\n",
      "\n",
      "This creates a self-improving system where each cycle of observation ‚Üí planning ‚Üí action ‚Üí feedback leads to better decision-making over time.\n",
      "\n",
      "## Challenges in Diffusion Video Generation\n",
      "\n",
      "From the context of your text, diffusion video generation faces several key challenges:\n",
      "\n",
      "1. **Temporal consistency**: Video generation requires maintaining consistent visual content across frames while still allowing for meaningful motion. As noted in your text, the \"structure $s$ depicts geometry and dynamics, including shapes, locations, temporal changes of objects,\" which creates complexity in maintaining temporal coherence.\n",
      "\n",
      "2. **Model scalability**: Video generation is significantly more complex than image generation. The text mentions that models like ControlVideo (Zhang et al. 2023) and Tune-A-Video (Wu et al. 2023) have been developed to address these challenges, showing that the problem requires specialized approaches.\n",
      "\n",
      "3. **Conditional editing**: As mentioned in your text, \"Content $c$ refers to appearance and semantics of the video, that is sampled from the text for conditional editing.\" This creates challenges in precisely controlling which aspects of the video are modified while maintaining overall coherence.\n",
      "\n",
      "4. **Motion representation**: Video generation requires modeling motion trajectories, which is more complex than static image generation. The text references models like Lumiere (Bar-Tal et al. 2024) that specifically address this challenge.\n",
      "\n",
      "5. **Real-time processing**: The text mentions that \"Tune-A-Video is meant to be used for object editing,\" indicating that practical applications require efficient processing that can handle video generation without excessive computational overhead.\n",
      "\n",
      "The challenges in diffusion video generation are particularly significant because video is inherently a temporal medium with complex dynamics that require specialized modeling approaches beyond what works well for static images. Models like ControlVideo and Lumiere represent important efforts to address these challenges through specialized architectures that can handle both the visual content and the temporal aspects of video generation.\n",
      "\n",
      "This is why the field has seen rapid development with models that specifically address video generation challenges, building upon the more established image generation techniques while adding the necessary temporal components.\n"
     ]
    }
   ],
   "source": [
    "user_query = \"Explain how agent loops work and what are the challenges in diffusion video generation?\"\n",
    "initial_state = RAGState(question=user_query)\n",
    "final_state = graph.invoke(initial_state)\n",
    "print(final_state)\n",
    "\n",
    "print(\"\\n Sub-question:\")\n",
    "for q in final_state['sub_questions']:\n",
    "    print(\"-\", q)\n",
    "    \n",
    "print(\"\\n Final Answer:\\n\", final_state['answer'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a1986a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
